{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#https://deepseekpro.org/guide/building-an-image-similarity-search-engine-with-faiss-and-clip/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install \"rectools[implicit]\" catboost \"git+https://github.com/openai/CLIP.git\" \\\n    torch torchvision pillow scikit-learn tqdm\n\n\n## 1. Генерация синтетического датасета + картинок\n\nimport os\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\nimport random\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageDraw\nfrom tqdm.auto import tqdm\n\nfrom rectools import Columns\nfrom rectools.dataset import Dataset\nfrom rectools.model_selection import LastNSplitter\n\n# =========================\n# 1. Генерация данных\n# =========================\n\nRND_SEED = 42\nrandom.seed(RND_SEED)\nnp.random.seed(RND_SEED)\n\nN_USERS = 1000\nN_ITEMS = 500\nMIN_INTER_PER_USER = 20\nMAX_INTER_PER_USER = 80\n\nimages_root = Path(\"item_images\")\nimages_root.mkdir(exist_ok=True)\n\ndef generate_item_images(n_items: int, images_per_item: int = 2,\n                         size=(224, 224)) -> pd.DataFrame:\n    rows = []\n    for item_id in tqdm(range(n_items), desc=\"Generate images\"):\n        for img_idx in range(images_per_item):\n            img = Image.new(\"RGB\", size, color=(\n                random.randint(0, 255),\n                random.randint(0, 255),\n                random.randint(0, 255),\n            ))\n            draw = ImageDraw.Draw(img)\n            # немного рандомного \"паттерна\"\n            for _ in range(5):\n                x0, y0 = random.randint(0, size[0]//2), random.randint(0, size[1]//2)\n                x1, y1 = random.randint(x0, size[0]), random.randint(y0, size[1])\n                draw.rectangle([x0, y0, x1, y1],\n                               outline=\"black\", width=2)\n\n            fname = images_root / f\"item_{item_id}_img_{img_idx}.png\"\n            img.save(fname)\n            rows.append({\n                \"item_id\": item_id,\n                \"image_path\": str(fname),\n            })\n    return pd.DataFrame(rows)\n\nitems_images = generate_item_images(N_ITEMS, images_per_item=2)\n\ndef generate_interactions(n_users, n_items) -> pd.DataFrame:\n    start_time = datetime(2024, 1, 1)\n    rows = []\n    for u in range(n_users):\n        n_inter = random.randint(MIN_INTER_PER_USER, MAX_INTER_PER_USER)\n        # пользователь склонен к своему «кластеру» айтемов\n        base = random.randint(0, n_items - 1)\n        item_ids = np.random.normal(loc=base,\n                                    scale=max(5, n_items / 20),\n                                    size=n_inter).round().astype(int)\n        item_ids = np.clip(item_ids, 0, n_items - 1)\n\n        t = start_time\n        for it in item_ids:\n            t += timedelta(minutes=random.randint(1, 60))\n            rows.append({\n                Columns.User: u,\n                Columns.Item: it,\n                Columns.Weight: 1.0,\n                Columns.Datetime: t,\n            })\n    df = pd.DataFrame(rows)\n    df[Columns.Datetime] = pd.to_datetime(df[Columns.Datetime])\n    return df\n\ninteractions = generate_interactions(N_USERS, N_ITEMS)\n\nprint(interactions.head())\nprint(items_images.head())\n\n## 2. CLIP-эмбеддинги картинок и усреднение по айтему\n\nimport torch\nimport clip\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)  # базовая CLIP-модель\n\ndef compute_item_clip_embeddings(items_images: pd.DataFrame,\n                                 batch_size: int = 64):\n    # ожидаем, что у каждого item_id может быть несколько картинок\n    grouped = items_images.groupby(\"item_id\")[\"image_path\"].apply(list)\n\n    item_ids = []\n    embs = []\n\n    with torch.no_grad():\n        for item_id, paths in tqdm(grouped.items(), desc=\"CLIP embeddings\"):\n            imgs = []\n            for p in paths:\n                img = Image.open(p).convert(\"RGB\")\n                imgs.append(preprocess(img))\n            imgs = torch.stack(imgs).to(device)\n\n            feats = model.encode_image(imgs)\n            feats = feats / feats.norm(dim=-1, keepdim=True)  # нормируем\n            mean_emb = feats.mean(dim=0).cpu().numpy()\n\n            item_ids.append(item_id)\n            embs.append(mean_emb)\n\n    emb_matrix = np.vstack(embs)  # shape: (n_items, d)\n    item_ids = np.array(item_ids)\n    return item_ids, emb_matrix\n\nitem_ids_clip, item_embs = compute_item_clip_embeddings(items_images)\n\n# нормируем ещё раз \"на всякий\"\nitem_embs = item_embs / np.linalg.norm(item_embs, axis=1, keepdims=True)\n\n# маппинг id -> индекс в матрице эмбеддингов\nitem_id_to_idx = {int(i): idx for idx, i in enumerate(item_ids_clip)}\n\n## 3. Dataset в RecTools и сплит по времени\n\ndataset = Dataset.construct(interactions_df=interactions)\n\n# LastNSplitter: для каждого юзера последняя интеракция в тест\nsplitter = LastNSplitter(\n    n=1,\n    n_splits=1,\n    filter_cold_users=False,\n    filter_cold_items=False,\n    filter_already_seen=True,\n)\n\ntrain_idx, test_idx, info = next(splitter.split(dataset.interactions))\nprint(\"Split info:\", info)\n\nds_train = dataset.filter_interactions(train_idx)\nds_test = dataset.filter_interactions(test_idx)\n\ntrain_raw = ds_train.get_raw_interactions()\ntest_raw = ds_test.get_raw_interactions()\n\nprint(\"Train interactions:\", train_raw.shape)\nprint(\"Test interactions:\", test_raw.shape)\n\n\n## 4. Кандидаты: ALS, быстрый itemKNN (TFIDF) и item-item по CLIP","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T23:00:23.932642Z","iopub.execute_input":"2025-11-18T23:00:23.933488Z","iopub.status.idle":"2025-11-18T23:00:43.866630Z","shell.execute_reply.started":"2025-11-18T23:00:23.933459Z","shell.execute_reply":"2025-11-18T23:00:43.865473Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### 4.1 ALS (ImplicitALSWrapperModel)\n\nfrom implicit.als import AlternatingLeastSquares\nfrom rectools.models import ImplicitALSWrapperModel\n\nals_base = AlternatingLeastSquares(\n    factors=64,\n    regularization=0.01,\n    alpha=1.0,\n    random_state=RND_SEED,\n    use_gpu=False,\n    iterations=15,\n)\n\nals_model = ImplicitALSWrapperModel(als_base)\nals_model.fit(ds_train)\n\n### 4.2 Быстрый itemKNN по TF-IDF\n\nfrom implicit.nearest_neighbours import TFIDFRecommender\nfrom rectools.models import ImplicitItemKNNWrapperModel\n\nknn_base = TFIDFRecommender(K=50, num_threads=4)\nknn_model = ImplicitItemKNNWrapperModel(knn_base)\nknn_model.fit(ds_train)\n\n### 4.3 item-item по CLIP (косинусное сходство)\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nall_items = np.array(sorted(train_raw[Columns.Item].unique()))\n# матрица эмбеддингов в том же порядке\nemb_order_idx = np.array([item_id_to_idx[i] for i in all_items])\nemb_order = item_embs[emb_order_idx]\n\ndef clip_candidates_for_user(user_id, top_k=50):\n    hist_items = train_raw.loc[\n        train_raw[Columns.User] == user_id, Columns.Item\n    ].unique()\n    hist_items = [i for i in hist_items if i in item_id_to_idx]\n    if not hist_items:\n        return np.array([], dtype=int), np.array([], dtype=float)\n\n    hist_idx = [item_id_to_idx[i] for i in hist_items]\n    hist_embs = item_embs[hist_idx]\n\n    # cos(user_hist_items, all_items)\n    sims = cosine_similarity(hist_embs, emb_order)  # shape: len_hist x n_items\n    max_sims = sims.max(axis=0)\n\n    # обнуляем уже просмотренные айтемы\n    mask_hist = np.isin(all_items, hist_items)\n    max_sims[mask_hist] = -1.0\n\n    if top_k >= len(max_sims):\n        top_idx = np.argsort(-max_sims)\n    else:\n        top_idx = np.argpartition(-max_sims, top_k)[:top_k]\n        top_idx = top_idx[np.argsort(-max_sims[top_idx])]\n\n    cand_items = all_items[top_idx]\n    cand_scores = max_sims[top_idx]\n    return cand_items, cand_scores\n\n### 4.4 Объединяем кандидатов из трёх источников\n\nusers_train = train_raw[Columns.User].unique()\n\n# ALS\nals_reco = als_model.recommend(\n    users=users_train,\n    dataset=ds_train,\n    k=50,\n    filter_viewed=True,\n)\nals_reco = als_reco.rename(columns={\n    Columns.Rank: \"als_rank\",\n    Columns.Score: \"als_score\",\n})\nals_reco[\"source_als\"] = 1\n\n# TFIDF itemKNN\nknn_reco = knn_model.recommend(\n    users=users_train,\n    dataset=ds_train,\n    k=50,\n    filter_viewed=True,\n)\nknn_reco = knn_reco.rename(columns={\n    Columns.Rank: \"knn_rank\",\n    Columns.Score: \"knn_score\",\n})\nknn_reco[\"source_knn\"] = 1\n\n# CLIP item-item\nrows = []\nfor u in tqdm(users_train, desc=\"CLIP i2i candidates\"):\n    items, scores = clip_candidates_for_user(u, top_k=50)\n    for it, s in zip(items, scores):\n        rows.append({\n            Columns.User: u,\n            Columns.Item: it,\n            \"clip_score\": float(s),\n            \"source_clip\": 1,\n        })\nclip_reco = pd.DataFrame(rows)\n\n# outer join трёх таблиц\ncandidates = als_reco.merge(\n    knn_reco[[Columns.User, Columns.Item, \"knn_rank\", \"knn_score\", \"source_knn\"]],\n    on=[Columns.User, Columns.Item],\n    how=\"outer\",\n).merge(\n    clip_reco,\n    on=[Columns.User, Columns.Item],\n    how=\"outer\",\n)\n\nfor col in [\"als_rank\", \"als_score\", \"knn_rank\", \"knn_score\",\n            \"clip_score\", \"source_als\", \"source_knn\", \"source_clip\"]:\n    if col not in candidates:\n        candidates[col] = 0.0\ncandidates[[\"source_als\", \"source_knn\", \"source_clip\"]] = \\\n    candidates[[\"source_als\", \"source_knn\", \"source_clip\"]].fillna(0).astype(int)\n\nprint(\"Candidates shape:\", candidates.shape)\n\n## 5. Разметка и фичи для ранжирования\n\n# разметка\ntest_pairs = test_raw[[Columns.User, Columns.Item]].copy()\ntest_pairs[\"label\"] = 1\n\ntrain_pairs = candidates.merge(\n    test_pairs,\n    on=[Columns.User, Columns.Item],\n    how=\"left\",\n)\ntrain_pairs[\"label\"] = train_pairs[\"label\"].fillna(0).astype(int)\n\n# оставим только юзеров, у которых есть хотя бы один позитив\nusers_with_pos = train_pairs.loc[train_pairs[\"label\"] == 1, Columns.User].unique()\ntrain_pairs = train_pairs[train_pairs[Columns.User].isin(users_with_pos)]\n\n# базовые статистики\nuser_activity = train_raw.groupby(Columns.User)[Columns.Item].size()\nitem_popularity = train_raw.groupby(Columns.Item)[Columns.User].size()\n\ntrain_pairs[\"user_activity\"] = train_pairs[Columns.User].map(user_activity)\ntrain_pairs[\"item_popularity\"] = train_pairs[Columns.Item].map(item_popularity)\n\n# история юзеров по времени\ntrain_sorted = train_raw.sort_values(Columns.Datetime)\nuser_history_items = train_sorted.groupby(Columns.User)[Columns.Item].apply(list)\n\nuser_last_item = {u: items[-1] for u, items in user_history_items.items()\n                  if len(items) > 0}\n\n# средний CLIP-вектор истории юзера\nuser_hist_mean_vec = {}\nfor u, items in user_history_items.items():\n    idx = [item_id_to_idx[i] for i in items if i in item_id_to_idx]\n    if not idx:\n        continue\n    mean_vec = item_embs[idx].mean(axis=0)\n    mean_vec /= np.linalg.norm(mean_vec) + 1e-12\n    user_hist_mean_vec[u] = mean_vec\n\ndef add_clip_sim_features(df):\n    sim_last = []\n    sim_mean = []\n    for u, it in zip(df[Columns.User].values, df[Columns.Item].values):\n        idx = item_id_to_idx.get(int(it))\n        if idx is None:\n            sim_last.append(0.0)\n            sim_mean.append(0.0)\n            continue\n        v = item_embs[idx]\n\n        last_item_id = user_last_item.get(int(u))\n        if last_item_id is not None and last_item_id in item_id_to_idx:\n            v_last = item_embs[item_id_to_idx[last_item_id]]\n            sim_last.append(float(np.dot(v, v_last)))\n        else:\n            sim_last.append(0.0)\n\n        mean_vec = user_hist_mean_vec.get(int(u))\n        if mean_vec is not None:\n            sim_mean.append(float(np.dot(v, mean_vec)))\n        else:\n            sim_mean.append(0.0)\n\n    df[\"sim_last_item\"] = sim_last\n    df[\"sim_hist_mean\"] = sim_mean\n\nadd_clip_sim_features(train_pairs)\n\n# финальный набор фич\nfeature_cols = [\n    Columns.User, Columns.Item,\n    \"user_activity\", \"item_popularity\",\n    \"als_rank\", \"als_score\",\n    \"knn_rank\", \"knn_score\",\n    \"clip_score\",\n    \"source_als\", \"source_knn\", \"source_clip\",\n    \"sim_last_item\", \"sim_hist_mean\",\n]\n\ntrain_pairs = train_pairs.sort_values(Columns.User).reset_index(drop=True)\nX_train = train_pairs[feature_cols]\ny_train = train_pairs[\"label\"].values\n\ncat_feature_names = [Columns.User, Columns.Item]\ncat_feature_indices = [feature_cols.index(c) for c in cat_feature_names]\n\n## 6. CatBoostRanker как основной ранкер\n\nfrom catboost import CatBoostRanker, Pool\n\ngroup_id = train_pairs[Columns.User].values  # один пользователь = одна группа\n\ntrain_pool = Pool(\n    data=X_train,\n    label=y_train,\n    group_id=group_id,\n    cat_features=cat_feature_indices,\n)\n\nranker = CatBoostRanker(\n    loss_function=\"YetiRank\",   # можно пробовать YetiRank, PairLogit, QuerySoftMax\n    depth=6,\n    learning_rate=0.05,\n    iterations=300,\n    random_seed=RND_SEED,\n    verbose=50,\n)\n\nranker.fit(train_pool)\n\n# подготовим пары \"user, candidate\" и фичи для теста\n# здесь для простоты используем тех же кандидатов, но можно пересчитать только на train-участке\ntest_candidates = candidates[candidates[Columns.User].isin(test_raw[Columns.User].unique())].copy()\n\ntest_candidates = test_candidates.sort_values(Columns.User).reset_index(drop=True)\n\n# добавляем статистики и CLIP-фичи\ntest_candidates[\"user_activity\"] = test_candidates[Columns.User].map(user_activity).fillna(0)\ntest_candidates[\"item_popularity\"] = test_candidates[Columns.Item].map(item_popularity).fillna(0)\nadd_clip_sim_features(test_candidates)\n\nX_test = test_candidates[feature_cols]\ngroup_test = test_candidates[Columns.User].values\ntest_pool = Pool(\n    data=X_test,\n    group_id=group_test,\n    cat_features=cat_feature_indices,\n)\n\ntest_candidates[\"ranker_score\"] = ranker.predict(test_pool)\n\n# финальный ранжированный список (пример: top-10 на юзера)\ntest_candidates[\"rank\"] = test_candidates.groupby(Columns.User)[\"ranker_score\"] \\\n    .rank(method=\"first\", ascending=False)\n\nfinal_reco = test_candidates[test_candidates[\"rank\"] <= 10][\n    [Columns.User, Columns.Item, \"rank\", \"ranker_score\"]\n].sort_values([Columns.User, \"rank\"])\n\nprint(final_reco.head(20))\n\n## 7. CatBoostClassifier как запасной вариант\n\nfrom catboost import CatBoostClassifier\n\nclf = CatBoostClassifier(\n    loss_function=\"Logloss\",\n    depth=6,\n    learning_rate=0.05,\n    iterations=300,\n    random_seed=RND_SEED,\n    verbose=50,\n)\n\nclf.fit(\n    X_train,\n    y_train,\n    cat_features=cat_feature_indices,\n)\n\n# предсказания на тест-кандидатах\ntest_candidates[\"clf_proba\"] = clf.predict_proba(\n    test_candidates[feature_cols]\n)[:, 1]\n\ntest_candidates[\"rank_clf\"] = test_candidates.groupby(Columns.User)[\"clf_proba\"] \\\n    .rank(method=\"first\", ascending=False)\n\nfinal_reco_clf = test_candidates[test_candidates[\"rank_clf\"] <= 10][\n    [Columns.User, Columns.Item, \"rank_clf\", \"clf_proba\"]\n].sort_values([Columns.User, \"rank_clf\"])\n\nprint(final_reco_clf.head(20))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T23:01:10.651762Z","iopub.execute_input":"2025-11-18T23:01:10.652530Z","iopub.status.idle":"2025-11-18T23:01:56.774652Z","shell.execute_reply.started":"2025-11-18T23:01:10.652499Z","shell.execute_reply":"2025-11-18T23:01:56.773848Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"CLIP i2i candidates:   0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"632075fd77a145c2be6146bd15df0723"}},"metadata":{}},{"name":"stdout","text":"Candidates shape: (106152, 10)\nGroupwise loss function. OneHotMaxSize set to 10\n0:\ttotal: 101ms\tremaining: 30.1s\n50:\ttotal: 4.91s\tremaining: 23.9s\n100:\ttotal: 9.82s\tremaining: 19.3s\n150:\ttotal: 14.6s\tremaining: 14.4s\n200:\ttotal: 19.4s\tremaining: 9.57s\n250:\ttotal: 24.2s\tremaining: 4.73s\n299:\ttotal: 29s\tremaining: 0us\n     user_id  item_id  rank  ranker_score\n50         0       75   1.0      1.971580\n48         0       73   2.0      1.659163\n49         0       74   3.0      1.554273\n63         0       96   4.0      1.484229\n4          0       51   5.0      1.466151\n43         0       67   6.0      1.464499\n47         0       72   7.0      1.447821\n54         0       81   8.0      1.422408\n8          0       55   9.0      1.406828\n44         0       68  10.0      1.398755\n171        1      195   1.0      2.729786\n154        1      198   2.0      2.650116\n140        1      227   3.0      2.609582\n160        1      204   4.0      2.468981\n139        1      226   5.0      2.467147\n147        1      209   6.0      2.461532\n156        1      200   7.0      2.417927\n205        1      240   8.0      2.405498\n153        1      197   9.0      2.401197\n150        1      215  10.0      2.389703\n0:\tlearn: 0.5577545\ttotal: 35.8ms\tremaining: 10.7s\n50:\tlearn: 0.0460363\ttotal: 1.64s\tremaining: 8.01s\n100:\tlearn: 0.0442519\ttotal: 3.21s\tremaining: 6.32s\n150:\tlearn: 0.0430089\ttotal: 4.81s\tremaining: 4.75s\n200:\tlearn: 0.0418093\ttotal: 6.42s\tremaining: 3.16s\n250:\tlearn: 0.0407235\ttotal: 8.08s\tremaining: 1.58s\n299:\tlearn: 0.0397184\ttotal: 9.63s\tremaining: 0us\n     user_id  item_id  rank_clf  clf_proba\n47         0       72       1.0   0.046868\n104        0       56       2.0   0.037590\n9          0       63       3.0   0.035746\n59         0       89       4.0   0.034382\n50         0       75       5.0   0.029880\n12         0       59       6.0   0.029167\n89         0       35       7.0   0.027034\n0          0       45       8.0   0.024764\n51         0       76       9.0   0.022074\n105        0       57      10.0   0.020438\n151        1      216       1.0   0.055029\n171        1      195       2.0   0.035436\n154        1      198       3.0   0.034602\n153        1      197       4.0   0.033965\n157        1      201       5.0   0.032465\n156        1      200       6.0   0.032059\n160        1      204       7.0   0.031993\n147        1      209       8.0   0.031547\n138        1      221       9.0   0.030580\n145        1      207      10.0   0.028992\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# =========================\n# 8. Валидация данных и пайплайна\n# =========================\nimport os\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom collections import defaultdict\n\ndef validate_interactions_schema(df: pd.DataFrame, name=\"interactions\"):\n    required = [Columns.User, Columns.Item, Columns.Weight, Columns.Datetime]\n    missing = [c for c in required if c not in df.columns]\n    assert not missing, f\"{name}: нет колонок {missing}\"\n\n    # Типы\n    issues = []\n    if not pd.api.types.is_integer_dtype(df[Columns.User]):\n        issues.append(f\"{name}: {Columns.User} не integer (dtype={df[Columns.User].dtype})\")\n    if not pd.api.types.is_integer_dtype(df[Columns.Item]):\n        issues.append(f\"{name}: {Columns.Item} не integer (dtype={df[Columns.Item].dtype})\")\n    if not pd.api.types.is_numeric_dtype(df[Columns.Weight]):\n        issues.append(f\"{name}: {Columns.Weight} не numeric (dtype={df[Columns.Weight].dtype})\")\n    if not np.issubdtype(df[Columns.Datetime].dtype, np.datetime64):\n        issues.append(f\"{name}: {Columns.Datetime} не datetime64 (dtype={df[Columns.Datetime].dtype})\")\n\n    if issues:\n        warnings.warn(\"\\n\".join(issues))\n\n    # Пропуски и дубли\n    n_na = int(df[required].isna().sum().sum())\n    n_dups = int(df.duplicated(subset=[Columns.User, Columns.Item, Columns.Datetime]).sum())\n    if n_na > 0:\n        warnings.warn(f\"{name}: есть пропуски в ключевых колонках: {n_na}\")\n    if n_dups > 0:\n        warnings.warn(f\"{name}: найдено дубликатов взаимодействий: {n_dups}\")\n\n    # Базовые рамки\n    neg_w = int((df[Columns.Weight] < 0).sum())\n    if neg_w > 0:\n        warnings.warn(f\"{name}: есть отрицательные веса: {neg_w}\")\n\n    print(f\"[OK] {name}: rows={len(df)}, users={df[Columns.User].nunique()}, items={df[Columns.Item].nunique()}\")\n\ndef validate_images_table(items_images: pd.DataFrame, n_items_expected: int = None, sample_check: int = 50):\n    assert {\"item_id\", \"image_path\"}.issubset(items_images.columns), \"items_images: нет колонок item_id / image_path\"\n    # проверим, что у каждого айтема есть >=1 картинка\n    per_item = items_images.groupby(\"item_id\")[\"image_path\"].count()\n    n_items_w_images = (per_item > 0).sum()\n    if n_items_expected is not None and n_items_w_images < n_items_expected:\n        warnings.warn(f\"Картинки есть не у всех айтемов: {n_items_w_images}/{n_items_expected}\")\n\n    # файлы существуют?\n    sample_paths = items_images[\"image_path\"].sample(min(sample_check, len(items_images)), random_state=RND_SEED)\n    missing = [p for p in sample_paths if not os.path.exists(p)]\n    if missing:\n        warnings.warn(f\"Отсутствуют файлы картинок (примеров): {len(missing)}. Первый: {missing[0]}\")\n    print(f\"[OK] images: items_with_images={n_items_w_images}, rows={len(items_images)}\")\n\ndef validate_temporal_split(train_df: pd.DataFrame, test_df: pd.DataFrame):\n    train_users = set(train_df[Columns.User].unique())\n    test_users = set(test_df[Columns.User].unique())\n    cold_users = test_users - train_users\n    frac_cold_users = 0.0 if len(test_users) == 0 else len(cold_users) / len(test_users)\n\n    train_items = set(train_df[Columns.Item].unique())\n    test_items = set(test_df[Columns.Item].unique())\n    cold_items = test_items - train_items\n    frac_cold_items = 0.0 if len(test_items) == 0 else len(cold_items) / len(test_items)\n\n    # Для LastNSplitter проверим, что у каждого пользователя тест-время >= любого train-времени этого пользователя\n    ok_temporal = True\n    # у нас по определению test — одна последняя интеракция на юзера, но всё же проверим\n    last_train = train_df.groupby(Columns.User)[Columns.Datetime].max()\n    test_time = test_df.groupby(Columns.User)[Columns.Datetime].max()\n    inter = pd.concat([last_train.rename(\"t_train\"), test_time.rename(\"t_test\")], axis=1).dropna()\n    if not (inter[\"t_test\"] >= inter[\"t_train\"]).all():\n        ok_temporal = False\n        bad = inter[~(inter[\"t_test\"] >= inter[\"t_train\"])].head()\n        warnings.warn(f\"Временной порядок нарушен для некоторых пользователей. Примеры:\\n{bad}\")\n\n    print(f\"[OK] split: users_train={len(train_users)}, users_test={len(test_users)}, cold_users={len(cold_users)} ({frac_cold_users:.2%})\")\n    print(f\"[OK] split: items_train={len(train_items)}, items_test={len(test_items)}, cold_items={len(cold_items)} ({frac_cold_items:.2%})\")\n    if not ok_temporal:\n        warnings.warn(\"Проверь стратегию сплита: test должен быть позже train для каждого пользователя.\")\n\n    # Теоретический максимум Recall@K при рекомендациях только из train-каталога:\n    # доля тестовых пар, где item ∈ train_items\n    test_total = len(test_df)\n    test_in_train_cat = int((test_df[Columns.Item].isin(train_items)).sum())\n    ub_recall = test_in_train_cat / max(1, test_total)\n    print(f\"[Info] Upper bound Recall (из-за cold items): {ub_recall:.2%}\")\n\ndef validate_candidates_coverage(candidates: pd.DataFrame, test_df: pd.DataFrame, k=10):\n    # Покрытие тест-правды кандидатами (upper bound для ре-ранкера)\n    test_truth = test_df[[Columns.User, Columns.Item]].drop_duplicates()\n    cand_users = candidates[Columns.User].unique()\n    test_truth = test_truth[test_truth[Columns.User].isin(cand_users)].copy()\n\n    in_cands = candidates.merge(test_truth, on=[Columns.User, Columns.Item], how=\"inner\")\n    users = test_truth[Columns.User].nunique()\n    hits = in_cands[Columns.User].nunique()  # у скольких юзеров их тестовый item вообще попал в candidate union\n    coverage = 0.0 if users == 0 else hits / users\n\n    # Если у тебя в candidates уже есть столбец ранга по источнику — можно оценить hit@k\n    if \"rank\" in candidates.columns:\n        topk = candidates[candidates[\"rank\"] <= k]\n        in_topk = topk.merge(test_truth, on=[Columns.User, Columns.Item], how=\"inner\")\n        hits_topk = in_topk[Columns.User].nunique()\n        coverage_topk = 0.0 if users == 0 else hits_topk / users\n        print(f\"[OK] candidates: users={users}, candidate_coverage={coverage:.2%}, candidate_coverage@{k}={coverage_topk:.2%}\")\n    else:\n        print(f\"[OK] candidates: users={users}, candidate_coverage={coverage:.2%}\")\n\ndef recall_at_k(recs_df, truth_df, k=10, rank_col=\"rank\"):\n    pred = recs_df[recs_df[rank_col] <= k].groupby(Columns.User)[Columns.Item].apply(list)\n    truth = truth_df.groupby(Columns.User)[Columns.Item].apply(set)\n    users = np.intersect1d(pred.index.values, truth.index.values)\n    hits, total = 0, 0\n    for u in users:\n        p = set(pred.loc[u]); t = truth.loc[u]\n        hits += len(p & t); total += len(t)\n    return hits / max(1, total)\n\ndef ndcg_at_k(recs_df, truth_df, k=10, rank_col=\"rank\"):\n    truth = truth_df.groupby(Columns.User)[Columns.Item].apply(set).to_dict()\n    ndcgs = []\n    for u, g in recs_df.groupby(Columns.User):\n        relset = truth.get(u, set())\n        cand = g.sort_values(rank_col).head(k)[Columns.Item].tolist()\n        dcg = 0.0\n        for i, it in enumerate(cand, 1):\n            dcg += (1.0 if it in relset else 0.0) / np.log2(i + 1)\n        idcg = sum(1.0 / np.log2(i + 1) for i in range(1, min(k, len(relset)) + 1))\n        ndcgs.append(dcg / idcg if idcg > 0 else 0.0)\n    return float(np.mean(ndcgs)) if ndcgs else 0.0\n\ndef validate_final_recos(final_reco: pd.DataFrame, final_reco_clf: pd.DataFrame, test_df: pd.DataFrame, k=10):\n    # Дубликаты в топ-K\n    for name, recs, rank_col in [\n        (\"ranker\", final_reco, \"rank\"),\n        (\"classifier\", final_reco_clf, \"rank_clf\"),\n    ]:\n        if recs is None or len(recs) == 0:\n            print(f\"[Warn] {name}: пустые рекомендации\")\n            continue\n        # проверка дубликатов айтемов в топ-K\n        top = recs[recs[rank_col] <= k]\n        dups = top.duplicated(subset=[Columns.User, Columns.Item]).sum()\n        if dups > 0:\n            warnings.warn(f\"{name}: найдено {dups} дубликатов (user,item) в топ-{k}\")\n\n        # монотонность рангов\n        bad_monot = []\n        for u, g in top.groupby(Columns.User):\n            if not g[rank_col].is_monotonic_increasing:\n                bad_monot.append(u)\n                if len(bad_monot) >= 3:\n                    break\n        if bad_monot:\n            warnings.warn(f\"{name}: нарушена монотонность рангов у пользователей (пример): {bad_monot[:3]}\")\n\n        # метрики\n        r = recall_at_k(recs.rename(columns={rank_col: \"rank\"}), test_df, k=k, rank_col=\"rank\")\n        n = ndcg_at_k(recs.rename(columns={rank_col: \"rank\"}), test_df, k=k, rank_col=\"rank\")\n        print(f\"[OK] {name}: Recall@{k}={r:.4f}, nDCG@{k}={n:.4f}\")\n\n# Вызов всех проверок\nprint(\"\\n========== ВАЛИДАЦИЯ ДАННЫХ ==========\")\nvalidate_interactions_schema(interactions, name=\"interactions(full)\")\nvalidate_images_table(items_images, n_items_expected=N_ITEMS, sample_check=50)\nvalidate_interactions_schema(train_raw, name=\"train_raw\")\nvalidate_interactions_schema(test_raw, name=\"test_raw\")\nvalidate_temporal_split(train_raw, test_raw)\n\n# Покрытие кандидатами — используем кандидатов по тестовым пользователям\ncand_for_test = candidates[candidates[Columns.User].isin(test_raw[Columns.User].unique())].copy()\nvalidate_candidates_coverage(cand_for_test, test_raw, k=10)\n\n# Валидация финальных списков (top-10)\nvalidate_final_recos(final_reco, final_reco_clf, test_raw, k=10)\nprint(\"=========== ВАЛИДАЦИЯ ЗАВЕРШЕНА ===========\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T23:02:13.797532Z","iopub.execute_input":"2025-11-18T23:02:13.797819Z","iopub.status.idle":"2025-11-18T23:02:14.267152Z","shell.execute_reply.started":"2025-11-18T23:02:13.797800Z","shell.execute_reply":"2025-11-18T23:02:14.266097Z"}},"outputs":[{"name":"stdout","text":"\n========== ВАЛИДАЦИЯ ДАННЫХ ==========\n[OK] interactions(full): rows=50210, users=1000, items=500\n[OK] images: items_with_images=500, rows=1000\n[OK] train_raw: rows=49210, users=1000, items=500\n[OK] test_raw: rows=561, users=561, items=349\n[OK] split: users_train=1000, users_test=561, cold_users=0 (0.00%)\n[OK] split: items_train=500, items_test=349, cold_items=0 (0.00%)\n[Info] Upper bound Recall (из-за cold items): 100.00%\n[OK] candidates: users=561, candidate_coverage=88.95%\n[OK] ranker: Recall@10=0.2531, nDCG@10=0.1395\n[OK] classifier: Recall@10=0.3066, nDCG@10=0.1944\n=========== ВАЛИДАЦИЯ ЗАВЕРШЕНА ===========\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Что дальше можно докрутить под соревнования\n\n'''* Поменять стратегию сплита на более реалистичную (`TimeRangeSplitter` / несколько фолдов). ([rectools.readthedocs.io][3])\n* Добавить больше фич: time-based (частота, свежесть), текстовые (title / description + эмбеддинги), категории.\n* Разнести кандидатов на несколько моделей, посчитать метрики `MAP@k`, `NDCG@k` через `rectools.metrics`. ([rectools.readthedocs.io][4])\n* Поиграть с ранкер-лоссами CatBoost (YetiRank, QuerySoftMax) и параметрами.\n\nЕсли хочешь, дальше могу:\n\n* добавить блок с вычислением `MAP@K` / `NDCG@K` по `rectools.metrics`;\n* или переписать это под формат конкретного соревнования (например, Yandex / Kion style submission).\n\n[1]: https://rectools.readthedocs.io/en/latest/features.html?utm_source=chatgpt.com \"Components - RecTools documentation - Read the Docs\"\n[2]: https://habr.com/ru/articles/773126/?utm_source=chatgpt.com \"RecTools – OpenSource библиотека для ...\"\n[3]: https://rectools.readthedocs.io/en/v0.9.0/_modules/rectools/model_selection/last_n_split.html \"rectools.model_selection.last_n_split — RecTools  documentation\"\n[4]: https://rectools.readthedocs.io/en/latest/api/rectools.metrics.ranking.MAP.html?utm_source=chatgpt.com \"MAP - RecTools documentation - Read the Docs\"\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#from __future__ import annotations\n\nimport json\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom typing import Dict, List, Sequence, Tuple\n\nimport clip\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom PIL import Image, ImageDraw\nfrom catboost import CatBoostClassifier, CatBoostRanker, Pool\nfrom implicit.als import AlternatingLeastSquares\nfrom rectools import Columns\nfrom rectools.dataset import Dataset\nfrom rectools.metrics import MAP, NDCG, Precision, Recall, calc_metrics\nfrom rectools.model_selection import TimeRangeSplitter\nfrom rectools.models import ImplicitALSWrapperModel, PopularModel\n\n\n@dataclass(slots=True)\nclass SyntheticConfig:\n    \"\"\"Configuration block for the synthetic dataset.\"\"\"\n\n    num_users: int = 200\n    num_items: int = 180\n    interactions_per_user: Tuple[int, int] = (40, 90)\n    images_per_item: int = 3\n    start_date: datetime = datetime(2024, 1, 1)\n    time_span_days: int = 90\n    seed: int = 42\n\n\n@dataclass(slots=True)\nclass PipelineConfig:\n    data_dir: Path\n    synthetic: SyntheticConfig = field(default_factory=SyntheticConfig)\n    clip_model: str = \"ViT-B/32\"\n    candidate_k: int = 50\n    final_k: int = 20\n    history_window: int = 15\n\n\n@dataclass(slots=True)\nclass GeneratedData:\n    interactions: pd.DataFrame\n    users: pd.DataFrame\n    items: pd.DataFrame\n    image_manifest: pd.DataFrame\n\n\n@dataclass(slots=True)\nclass CandidateSet:\n    df: pd.DataFrame\n    name: str\n\n\nclass SyntheticDataGenerator:\n    def __init__(self, config: SyntheticConfig, data_dir: Path) -> None:\n        self.config = config\n        self.data_dir = data_dir\n        self.image_dir = data_dir / \"images\"\n        self.image_dir.mkdir(parents=True, exist_ok=True)\n        self.rng = np.random.default_rng(config.seed)\n        self.categories = [\"tech\", \"books\", \"fashion\", \"home\", \"outdoor\", \"beauty\"]\n        self.styles = [\"minimal\", \"sport\", \"classic\", \"retro\", \"eco\"]\n        self.regions = [\"Moscow\", \"Saint-P\", \"Siberia\", \"South\", \"Volga\"]\n        self.age_groups = [\"18-25\", \"26-35\", \"36-45\", \"46+\"]\n\n    def generate(self) -> GeneratedData:\n        users = self._generate_users()\n        items = self._generate_items()\n        image_manifest = self._generate_item_images(items)\n        interactions = self._generate_interactions(users, items)\n        return GeneratedData(interactions, users, items, image_manifest)\n\n    def _generate_users(self) -> pd.DataFrame:\n        records = []\n        for user_id in range(1, self.config.num_users + 1):\n            region = self.rng.choice(self.regions)\n            age_group = self.rng.choice(self.age_groups, p=[0.25, 0.35, 0.25, 0.15])\n            primary_category = self.rng.choice(self.categories)\n            secondary_category = self.rng.choice([c for c in self.categories if c != primary_category])\n            style_preference = self.rng.choice(self.styles)\n            records.append(\n                {\n                    \"user_id\": user_id,\n                    \"region\": region,\n                    \"age_group\": age_group,\n                    \"primary_category\": primary_category,\n                    \"secondary_category\": secondary_category,\n                    \"style_preference\": style_preference,\n                }\n            )\n        return pd.DataFrame(records)\n\n    def _generate_items(self) -> pd.DataFrame:\n        palette = [\n            \"#0A9396\",\n            \"#94D2BD\",\n            \"#EE9B00\",\n            \"#CA6702\",\n            \"#BB3E03\",\n            \"#9B2226\",\n            \"#005F73\",\n            \"#AE2012\",\n        ]\n        records = []\n        for item_id in range(1, self.config.num_items + 1):\n            category = self.rng.choice(self.categories)\n            style = self.rng.choice(self.styles)\n            base_color = self.rng.choice(palette)\n            price = float(self.rng.integers(400, 6000))\n            trendiness = float(np.round(self.rng.uniform(0.1, 1.0), 3))\n            novelty = float(np.round(self.rng.uniform(0.2, 0.9), 3))\n            records.append(\n                {\n                    \"item_id\": item_id,\n                    \"category\": category,\n                    \"style\": style,\n                    \"base_color\": base_color,\n                    \"price\": price,\n                    \"trendiness\": trendiness,\n                    \"novelty\": novelty,\n                }\n            )\n        return pd.DataFrame(records)\n\n    def _generate_item_images(self, items: pd.DataFrame) -> pd.DataFrame:\n        manifest_records: List[dict] = []\n        for _, row in items.iterrows():\n            for idx in range(self.config.images_per_item):\n                path = self.image_dir / f\"item_{row.item_id}_{idx}.png\"\n                self._draw_item_image(\n                    path,\n                    background=row.base_color,\n                    accent=self._shift_color(row.base_color, idx),\n                    text=str(row.item_id),\n                )\n                manifest_records.append({\"item_id\": row.item_id, \"image_path\": str(path)})\n        return pd.DataFrame(manifest_records)\n\n    def _draw_item_image(self, path: Path, background: str, accent: str, text: str) -> None:\n        size = (224, 224)\n        image = Image.new(\"RGB\", size, background)\n        draw = ImageDraw.Draw(image)\n        x0, y0 = self.rng.integers(10, 80, size=2)\n        x1, y1 = self.rng.integers(120, 214, size=2)\n        draw.rectangle((x0, y0, x1, y1), fill=accent)\n        draw.text((size[0] // 3, size[1] // 3), text, fill=\"#FFFFFF\")\n        image.save(path)\n\n    def _shift_color(self, hex_color: str, idx: int) -> str:\n        base = int(hex_color.lstrip(\"#\"), 16)\n        r = (base >> 16) & 0xFF\n        g = (base >> 8) & 0xFF\n        b = base & 0xFF\n        shift = (idx + 1) * 15\n        r = (r + shift) % 255\n        g = (g + shift * 2) % 255\n        b = (b + shift * 3) % 255\n        return f\"#{r:02X}{g:02X}{b:02X}\"\n\n    def _generate_interactions(self, users: pd.DataFrame, items: pd.DataFrame) -> pd.DataFrame:\n        interactions: List[dict] = []\n        catalog_by_cat = {cat: df for cat, df in items.groupby(\"category\")}\n        max_seconds = self.config.time_span_days * 24 * 3600\n        for _, user in users.iterrows():\n            n_interactions = int(self.rng.integers(*self.config.interactions_per_user))\n            timestamps = np.sort(self.rng.integers(0, max_seconds, size=n_interactions))\n            for ts in timestamps:\n                ts_dt = self.config.start_date + timedelta(seconds=int(ts))\n                if self.rng.random() < 0.65:\n                    category = user.primary_category\n                elif self.rng.random() < 0.75:\n                    category = user.secondary_category\n                else:\n                    category = self.rng.choice(self.categories)\n                pool = catalog_by_cat.get(category)\n                if pool is None or pool.empty:\n                    pool = items\n                weights = (pool.trendiness.values + pool.novelty.values) / 2\n                weights = weights / weights.sum()\n                item = pool.sample(n=1, weights=weights, random_state=int(self.rng.integers(0, 1_000_000)))\n                interactions.append(\n                    {\n                        Columns.User: user.user_id,\n                        Columns.Item: int(item.item_id.values[0]),\n                        Columns.Weight: 1.0,\n                        Columns.Datetime: ts_dt,\n                    }\n                )\n        interactions_df = pd.DataFrame(interactions)\n        interactions_df.sort_values(Columns.Datetime, inplace=True)\n        interactions_df.reset_index(drop=True, inplace=True)\n        return interactions_df\n\n\nclass ClipEmbedder:\n    def __init__(self, model_name: str, device: str | None = None) -> None:\n        if device is None:\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.device = device\n        self.model, self.preprocess = clip.load(model_name, device=self.device)\n\n    def embed_items(self, image_manifest: pd.DataFrame) -> Dict[int, np.ndarray]:\n        embeddings: Dict[int, np.ndarray] = {}\n        grouped = image_manifest.groupby(\"item_id\")\n        for item_id, group in grouped:\n            images = [self.preprocess(Image.open(path)).unsqueeze(0) for path in group.image_path]\n            batch = torch.cat(images).to(self.device)\n            with torch.no_grad():\n                features = self.model.encode_image(batch)\n                features = features / features.norm(dim=-1, keepdim=True)\n            emb = features.mean(dim=0)\n            emb = emb / emb.norm()\n            embeddings[int(item_id)] = emb.cpu().numpy()\n        return embeddings\n\n\nclass ClipSimilarCandidates:\n    def __init__(\n        self,\n        embeddings: Dict[int, np.ndarray],\n        train_interactions: pd.DataFrame,\n        history_window: int,\n    ) -> None:\n        self.embeddings = embeddings\n        self.history_window = history_window\n        self.item_ids = np.array(list(embeddings.keys()))\n        matrix = np.vstack([embeddings[i] for i in self.item_ids])\n        self.item_matrix = matrix / np.linalg.norm(matrix, axis=1, keepdims=True)\n        self.global_profile = self.item_matrix.mean(axis=0)\n        self.user_histories = self._build_histories(train_interactions)\n\n    def _build_histories(self, interactions: pd.DataFrame) -> Dict[int, List[int]]:\n        histories: Dict[int, List[int]] = {}\n        grouped = interactions.sort_values(Columns.Datetime).groupby(Columns.User)\n        for user_id, group in grouped:\n            histories[int(user_id)] = group[Columns.Item].tolist()\n        return histories\n\n    def _get_profile(self, user: int) -> np.ndarray:\n        history = self.user_histories.get(int(user), [])[-self.history_window :]\n        vectors = [self.embeddings[i] for i in history if i in self.embeddings]\n        if not vectors:\n            return self.global_profile\n        stack = np.vstack(vectors)\n        profile = stack.mean(axis=0)\n        norm = np.linalg.norm(profile)\n        if norm > 0:\n            profile = profile / norm\n        return profile\n\n    def recommend(self, users: Sequence[int], k: int, filter_viewed: bool = True) -> pd.DataFrame:\n        records: List[dict] = []\n        item_ids = self.item_ids\n        matrix = self.item_matrix\n        for user in users:\n            profile = self._get_profile(int(user))\n            scores = matrix @ profile\n            order = np.argsort(scores)[::-1]\n            seen = set(self.user_histories.get(int(user), [])) if filter_viewed else set()\n            collected = 0\n            for idx in order:\n                item_id = int(item_ids[idx])\n                if filter_viewed and item_id in seen:\n                    continue\n                records.append({Columns.User: int(user), Columns.Item: item_id, \"clip_score\": float(scores[idx])})\n                collected += 1\n                if collected >= k:\n                    break\n        return pd.DataFrame(records)\n\n\nclass FeatureBuilder:\n    def __init__(\n        self,\n        train_interactions: pd.DataFrame,\n        users: pd.DataFrame,\n        items: pd.DataFrame,\n        embeddings: Dict[int, np.ndarray],\n    ) -> None:\n        self.users = users\n        self.items = items\n        self.embeddings = embeddings\n        self.embedding_dim = len(next(iter(embeddings.values()))) if embeddings else 0\n        enriched = train_interactions.merge(items, left_on=Columns.Item, right_on=\"item_id\", how=\"left\")\n        self.user_stats = self._aggregate_user_stats(enriched)\n        self.item_stats = self._aggregate_item_stats(train_interactions)\n        self.user_profiles = self._make_user_profiles(train_interactions)\n        self.categorical_sources = [\n            \"region\",\n            \"age_group\",\n            \"style_preference\",\n            \"category\",\n            \"style\",\n            \"user_top_category\",\n        ]\n        self.cat_features = [f\"{col}_idx\" for col in self.categorical_sources]\n\n    def _aggregate_user_stats(self, enriched: pd.DataFrame) -> pd.DataFrame:\n        stats = (\n            enriched.groupby(Columns.User)\n            .agg(\n                user_interactions=(Columns.Item, \"count\"),\n                user_unique_items=(Columns.Item, \"nunique\"),\n                user_last_ts=(Columns.Datetime, \"max\"),\n                user_mean_price=(\"price\", \"mean\"),\n                user_price_std=(\"price\", \"std\"),\n            )\n            .reset_index()\n        )\n        top_category = (\n            enriched.groupby([Columns.User, \"category\"])[Columns.Item]\n            .count()\n            .reset_index()\n            .sort_values(by=Columns.Item, ascending=False)\n            .drop_duplicates(subset=Columns.User)\n            .rename(columns={\"category\": \"user_top_category\", Columns.Item: \"_tmp\"})\n            .drop(columns=[\"_tmp\"])\n        )\n        stats = stats.merge(top_category, on=Columns.User, how=\"left\")\n        stats[\"user_price_std\"] = stats[\"user_price_std\"].fillna(0.0)\n        return stats\n\n    def _aggregate_item_stats(self, interactions: pd.DataFrame) -> pd.DataFrame:\n        stats = (\n            interactions.groupby(Columns.Item)\n            .agg(item_interactions=(Columns.User, \"count\"))\n            .reset_index()\n        )\n        return stats\n\n    def _make_user_profiles(self, interactions: pd.DataFrame) -> Dict[int, np.ndarray]:\n        profiles: Dict[int, np.ndarray] = {}\n        grouped = interactions.groupby(Columns.User)\n        for user_id, group in grouped:\n            vectors = [self.embeddings[i] for i in group[Columns.Item] if i in self.embeddings]\n            if not vectors:\n                continue\n            stack = np.vstack(vectors)\n            vec = stack.mean(axis=0)\n            norm = np.linalg.norm(vec)\n            if norm > 0:\n                vec = vec / norm\n            profiles[int(user_id)] = vec\n        return profiles\n\n    def _merge_candidates(self, candidates: Sequence[CandidateSet]) -> pd.DataFrame:\n        renamed = []\n        for cand in candidates:\n            df = cand.df.copy()\n            df = df.rename(columns={Columns.Score: f\"{cand.name}_score\", Columns.Rank: f\"{cand.name}_rank\"})\n            renamed.append(df)\n        if not renamed:\n            raise ValueError(\"Candidate list must not be empty\")\n        combined = renamed[0]\n        for frame in renamed[1:]:\n            combined = combined.merge(frame, on=[Columns.User, Columns.Item], how=\"outer\")\n        return combined\n\n    def build_dataset(\n        self,\n        target_interactions: pd.DataFrame,\n        candidates: Sequence[CandidateSet],\n    ) -> Tuple[pd.DataFrame, List[str]]:\n        merged_candidates = self._merge_candidates(candidates)\n        positives = target_interactions[[Columns.User, Columns.Item]].drop_duplicates()\n        merged = merged_candidates.merge(positives, on=[Columns.User, Columns.Item], how=\"outer\")\n        merged = merged.fillna(0.0)\n        merged = merged.merge(positives.assign(target=1), on=[Columns.User, Columns.Item], how=\"left\")\n        merged[\"target\"] = merged[\"target\"].fillna(0)\n        merged = merged.merge(self.users, left_on=Columns.User, right_on=\"user_id\", how=\"left\")\n        merged = merged.merge(self.user_stats, on=Columns.User, how=\"left\")\n        merged = merged.merge(self.items, left_on=Columns.Item, right_on=\"item_id\", how=\"left\")\n        merged = merged.merge(self.item_stats, on=Columns.Item, how=\"left\")\n        merged = merged.copy()\n        merged[\"user_interactions\"] = merged[\"user_interactions\"].fillna(0)\n        merged[\"user_unique_items\"] = merged[\"user_unique_items\"].fillna(0)\n        merged[\"item_interactions\"] = merged[\"item_interactions\"].fillna(0)\n        merged[\"user_mean_price\"] = merged[\"user_mean_price\"].fillna(merged[\"price\"].mean())\n        merged[\"user_price_std\"] = merged[\"user_price_std\"].fillna(1.0)\n        merged[\"user_top_category\"] = merged[\"user_top_category\"].fillna(\"unknown\")\n        merged[\"region\"] = merged[\"region\"].fillna(\"unknown\")\n        merged[\"age_group\"] = merged[\"age_group\"].fillna(\"unknown\")\n        merged[\"style_preference\"] = merged[\"style_preference\"].fillna(\"unknown\")\n        merged[\"category\"] = merged[\"category\"].fillna(\"unknown\")\n        merged[\"style\"] = merged[\"style\"].fillna(\"unknown\")\n        merged[\"user_price_delta\"] = merged[\"price\"] - merged[\"user_mean_price\"]\n        merged[\"user_price_z\"] = merged[\"user_price_delta\"] / (merged[\"user_price_std\"] + 1e-3)\n        merged[\"category_match\"] = (merged[\"user_top_category\"] == merged[\"category\"]).astype(int)\n        merged[\"clip_similarity\"] = self._clip_similarity(merged)\n        merged[\"recency_days\"] = self._compute_recency(merged)\n        merged = self._encode_categorical(merged)\n        merged[Columns.User] = merged[Columns.User].astype(int)\n        merged[Columns.Item] = merged[Columns.Item].astype(int)\n        feature_cols = [col for col in merged.columns if col not in {\"target\"}]\n        return merged, self.cat_features\n\n    def _encode_categorical(self, merged: pd.DataFrame) -> pd.DataFrame:\n        for source, target in zip(self.categorical_sources, self.cat_features):\n            merged[target] = merged[source].astype(\"category\").cat.codes.astype(int)\n        merged = merged.drop(columns=self.categorical_sources)\n        object_cols = [col for col in merged.select_dtypes(include=\"object\").columns if col not in (Columns.User, Columns.Item)]\n        for col in object_cols:\n            code_col = f\"{col}_idx\"\n            merged[code_col] = merged[col].astype(\"category\").cat.codes.astype(int)\n            merged = merged.drop(columns=[col])\n            if code_col not in self.cat_features:\n                self.cat_features.append(code_col)\n        self.cat_features = list(dict.fromkeys(self.cat_features))\n        return merged\n\n    def _clip_similarity(self, merged: pd.DataFrame) -> np.ndarray:\n        similarities = []\n        for _, row in merged.iterrows():\n            user_vec = self.user_profiles.get(int(row[Columns.User]))\n            item_vec = self.embeddings.get(int(row[Columns.Item]))\n            if user_vec is None or item_vec is None:\n                similarities.append(0.0)\n            else:\n                similarities.append(float(np.dot(user_vec, item_vec)))\n        return np.array(similarities)\n\n    def _compute_recency(self, merged: pd.DataFrame) -> np.ndarray:\n        recencies = []\n        now = merged[\"user_last_ts\"].max()\n        for ts in merged[\"user_last_ts\"].fillna(now):\n            diff = now - ts if isinstance(ts, pd.Timestamp) else timedelta(days=0)\n            recencies.append(float(diff.days))\n        return np.array(recencies)\n\n\ndef build_dataset(config: PipelineConfig) -> Tuple[Dataset, GeneratedData, Dict[int, np.ndarray]]:\n    generator = SyntheticDataGenerator(config.synthetic, config.data_dir)\n    generated = generator.generate()\n    embedder = ClipEmbedder(config.clip_model)\n    embeddings = embedder.embed_items(generated.image_manifest)\n    generated.interactions.to_parquet(config.data_dir / \"interactions.parquet\", index=False)\n    generated.users.to_parquet(config.data_dir / \"users.parquet\", index=False)\n    generated.items.to_parquet(config.data_dir / \"items.parquet\", index=False)\n    interactions_df = generated.interactions.copy()\n    dataset = Dataset.construct(interactions_df)\n    return dataset, generated, embeddings\n\n\ndef split_dataset(dataset: Dataset) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    splitter = TimeRangeSplitter(\"10D\", n_splits=2, filter_cold_users=True, filter_cold_items=True, filter_already_seen=False)\n    splits = list(splitter.split(dataset.interactions))\n    (train_idx, val_idx, _), (train_plus_idx, test_idx, _) = splits\n    return train_idx, val_idx, train_plus_idx, test_idx\n\n\ndef get_dataframe_by_idx(df: pd.DataFrame, idx: np.ndarray) -> pd.DataFrame:\n    return df.iloc[idx].copy()\n\n\ndef make_candidate_set(name: str, df: pd.DataFrame) -> CandidateSet:\n    cols = [Columns.User, Columns.Item, Columns.Score, Columns.Rank]\n    df = df.copy()\n    missing = [c for c in cols if c not in df]\n    for c in missing:\n        if c == Columns.Score:\n            df[c] = 0.0\n        elif c == Columns.Rank:\n            df[c] = 0\n    return CandidateSet(df[cols], name)\n\n\ndef train_candidate_models(\n    dataset: Dataset,\n    train_idx: np.ndarray,\n    embeddings: Dict[int, np.ndarray],\n    config: PipelineConfig,\n    train_interactions_external: pd.DataFrame,\n) -> Tuple[ImplicitALSWrapperModel, PopularModel, ClipSimilarCandidates]:\n    train_dataset = dataset.filter_interactions(train_idx)\n    als_model = ImplicitALSWrapperModel(\n        AlternatingLeastSquares(\n            factors=64,\n            iterations=20,\n            regularization=0.05,\n            alpha=16,\n            num_threads=0,\n            random_state=42,\n        )\n    )\n    als_model.fit(train_dataset)\n    fast_model = PopularModel(popularity=\"n_users\")\n    fast_model.fit(train_dataset)\n    clip_candidates = ClipSimilarCandidates(\n        embeddings,\n        train_interactions_external[[Columns.User, Columns.Item, Columns.Datetime]],\n        history_window=config.history_window,\n    )\n    return als_model, fast_model, clip_candidates\n\n\ndef generate_candidates(\n    users: Sequence[int],\n    als_model: ImplicitALSWrapperModel,\n    fast_model: PopularModel,\n    clip_candidates: ClipSimilarCandidates,\n    dataset: Dataset,\n    k: int,\n) -> List[CandidateSet]:\n    als = als_model.recommend(users=users, dataset=dataset, k=k, filter_viewed=True)\n    fast = fast_model.recommend(users=users, dataset=dataset, k=k, filter_viewed=True)\n    clip_df = clip_candidates.recommend(users, k, filter_viewed=True)\n    clip_df[Columns.Score] = clip_df[\"clip_score\"]\n    clip_df[Columns.Rank] = clip_df.groupby(Columns.User)[\"clip_score\"].rank(ascending=False, method=\"first\").astype(int)\n    return [\n        make_candidate_set(\"als\", als),\n        make_candidate_set(\"popular\", fast),\n        make_candidate_set(\"clip\", clip_df),\n    ]\n\n\ndef prepare_features(\n    train_interactions: pd.DataFrame,\n    users: pd.DataFrame,\n    items: pd.DataFrame,\n    embeddings: Dict[int, np.ndarray],\n    target_interactions: pd.DataFrame,\n    candidates: Sequence[CandidateSet],\n) -> Tuple[pd.DataFrame, List[str]]:\n    builder = FeatureBuilder(train_interactions, users, items, embeddings)\n    dataset, cat_features = builder.build_dataset(target_interactions, candidates)\n    return dataset, cat_features\n\n\ndef train_catboost_models(\n    features: pd.DataFrame,\n    cat_features: List[str],\n) -> Tuple[CatBoostRanker, CatBoostClassifier, List[str], List[int]]:\n    feature_cols = [\n        col\n        for col in features.columns\n        if col\n        not in {\n            \"target\",\n            Columns.User,\n            Columns.Item,\n            \"user_id\",\n            \"item_id\",\n            \"image_path\",\n        }\n    ]\n    cat_feature_cols = [col for col in cat_features if col in feature_cols]\n    cat_feature_indices = [feature_cols.index(col) for col in cat_feature_cols]\n    print(f\"Using categorical features: {cat_feature_cols}\")\n    train_pool = Pool(\n        data=features[feature_cols],\n        label=features[\"target\"],\n        group_id=features[Columns.User],\n        cat_features=cat_feature_indices,\n    )\n    ranker = CatBoostRanker(\n        iterations=400,\n        depth=6,\n        learning_rate=0.05,\n        loss_function=\"YetiRank\",\n        random_seed=42,\n        verbose=False,\n    )\n    ranker.fit(train_pool)\n    classifier = CatBoostClassifier(\n        iterations=400,\n        depth=6,\n        learning_rate=0.05,\n        loss_function=\"Logloss\",\n        random_seed=52,\n        verbose=False,\n    )\n    classifier.fit(train_pool)\n    return ranker, classifier, feature_cols, cat_feature_indices\n\n\ndef evaluate_model(\n    model,\n    features: pd.DataFrame,\n    feature_cols: List[str],\n    cat_feature_indices: List[int],\n    target_interactions: pd.DataFrame,\n    k: int,\n    label: str,\n) -> Tuple[pd.DataFrame, Dict[str, float]]:\n    pool = Pool(\n        data=features[feature_cols],\n        group_id=features[Columns.User],\n        cat_features=cat_feature_indices,\n    )\n    if isinstance(model, CatBoostClassifier):\n        scores = model.predict_proba(pool)[:, 1]\n    else:\n        scores = model.predict(pool)\n    features = features.copy()\n    features[f\"{label}_score\"] = scores\n    ranked = (\n        features.sort_values([Columns.User, f\"{label}_score\"], ascending=[True, False])\n        .groupby(Columns.User)\n        .head(k)\n    )\n    reco = ranked[[Columns.User, Columns.Item]].copy()\n    reco[Columns.Rank] = reco.groupby(Columns.User).cumcount() + 1\n    reco.rename(columns={Columns.Item: Columns.Item}, inplace=True)\n    metrics = {\n        f\"map@{k}\": MAP(k=k),\n        f\"recall@{k}\": Recall(k=k),\n        f\"precision@{k}\": Precision(k=k),\n        f\"ndcg@{k}\": NDCG(k=k),\n    }\n    metric_values = calc_metrics(metrics, reco=reco, interactions=target_interactions)\n    return reco, metric_values\n\n\ndef main() -> None:\n    base_dir = Path('/kaggle/working/')\n    data_dir = base_dir / \"data\"\n    data_dir.mkdir(parents=True, exist_ok=True)\n    config = PipelineConfig(data_dir=data_dir)\n    dataset, generated, embeddings = build_dataset(config)\n    train_idx, val_idx, train_plus_idx, test_idx = split_dataset(dataset)\n\n    full_interactions = generated.interactions\n    train_interactions = get_dataframe_by_idx(full_interactions, train_idx)\n    val_interactions = get_dataframe_by_idx(full_interactions, val_idx)\n    test_interactions = get_dataframe_by_idx(full_interactions, test_idx)\n\n    als_model, fast_model, clip_candidates_model = train_candidate_models(\n        dataset, train_idx, embeddings, config, train_interactions\n    )\n    train_dataset = dataset.filter_interactions(train_idx)\n    val_users = sorted(val_interactions[Columns.User].unique())\n    candidates = generate_candidates(\n        val_users,\n        als_model,\n        fast_model,\n        clip_candidates_model,\n        train_dataset,\n        config.candidate_k,\n    )\n\n    train_features, cat_features = prepare_features(\n        train_interactions,\n        generated.users,\n        generated.items,\n        embeddings,\n        val_interactions,\n        candidates,\n    )\n\n    ranker, classifier, feature_cols, cat_feature_indices = train_catboost_models(train_features, cat_features)\n\n    train_plus_interactions = get_dataframe_by_idx(full_interactions, train_plus_idx)\n    final_als, final_fast, final_clip = train_candidate_models(\n        dataset, train_plus_idx, embeddings, config, train_plus_interactions\n    )\n    test_dataset = dataset.filter_interactions(train_plus_idx)\n    test_users = sorted(test_interactions[Columns.User].unique())\n    test_candidates = generate_candidates(\n        test_users,\n        final_als,\n        final_fast,\n        final_clip,\n        test_dataset,\n        config.candidate_k,\n    )\n    test_features, _ = prepare_features(\n        train_plus_interactions,\n        generated.users,\n        generated.items,\n        embeddings,\n        test_interactions,\n        test_candidates,\n    )\n    ranker_reco, ranker_metrics = evaluate_model(\n        ranker,\n        test_features,\n        feature_cols,\n        cat_feature_indices,\n        test_interactions,\n        config.final_k,\n        label=\"ranker\",\n    )\n    classifier_reco, classifier_metrics = evaluate_model(\n        classifier,\n        test_features,\n        feature_cols,\n        cat_feature_indices,\n        test_interactions,\n        config.final_k,\n        label=\"classifier\",\n    )\n    ranker_reco.to_parquet(data_dir / \"ranker_reco.parquet\", index=False)\n    classifier_reco.to_parquet(data_dir / \"classifier_reco.parquet\", index=False)\n    summary = {\"ranker\": ranker_metrics, \"classifier\": classifier_metrics}\n    (data_dir / \"metrics.json\").write_text(json.dumps(summary, indent=2, ensure_ascii=False))\n    print(\"CatBoostRanker metrics:\", ranker_metrics)\n    print(\"CatBoostClassifier metrics:\", classifier_metrics)\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T23:07:34.614713Z","iopub.execute_input":"2025-11-18T23:07:34.615045Z","iopub.status.idle":"2025-11-18T23:08:19.759697Z","shell.execute_reply.started":"2025-11-18T23:07:34.615022Z","shell.execute_reply":"2025-11-18T23:08:19.758730Z"}},"outputs":[{"name":"stdout","text":"Using categorical features: ['region_idx', 'age_group_idx', 'style_preference_idx', 'category_idx', 'style_idx', 'user_top_category_idx', 'primary_category_idx', 'secondary_category_idx', 'base_color_idx']\nCatBoostRanker metrics: {'recall@20': 0.8836249038510344, 'precision@20': 0.32160804020100503, 'ndcg@20': 0.4765650517347863, 'map@20': 0.7875528279675555}\nCatBoostClassifier metrics: {'recall@20': 0.9186664605634456, 'precision@20': 0.33266331658291465, 'ndcg@20': 0.48673425672239756, 'map@20': 0.8100006145223941}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# зависимости\n!pip install -U opencv-python-headless torch torchvision torchaudio -q\n\nimport os, sys, torch, cv2\nfrom transformers import CLIPModel, CLIPImageProcessor\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nCLIP_CKPT = \"openai/clip-vit-base-patch32\"\nhf_model = CLIPModel.from_pretrained(CLIP_CKPT).to(DEVICE).eval()\nhf_proc = CLIPImageProcessor.from_pretrained(CLIP_CKPT)\n\n@torch.no_grad()\ndef embed_batch(paths):\n    imgs_rgb = []\n    for p in paths:\n        bgr = cv2.imread(p, cv2.IMREAD_COLOR)\n        if bgr is None:\n            raise FileNotFoundError(p)\n        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n        imgs_rgb.append(rgb)\n    inputs = hf_proc(images=imgs_rgb, return_tensors=\"pt\").to(DEVICE)\n    feats = hf_model.get_image_features(**inputs)\n    feats = feats / feats.norm(dim=-1, keepdim=True)\n    return feats.detach().cpu().numpy().astype(\"float32\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T22:35:32.453418Z","iopub.execute_input":"2025-11-18T22:35:32.454154Z","iopub.status.idle":"2025-11-18T22:35:39.995683Z","shell.execute_reply.started":"2025-11-18T22:35:32.454120Z","shell.execute_reply":"2025-11-18T22:35:39.994460Z"}},"outputs":[{"name":"stdout","text":"^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/2575620432.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCLIPModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIPImageProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mDEVICE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2152\u001b[0m     \u001b[0mis_dummy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2154\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"_from_config\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_dummy\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mro\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"call\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2182\u001b[0m         \u001b[0mmodule_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2183\u001b[0m         \u001b[0mimport_structure\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIMPORT_STRUCTURE_T\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2184\u001b[0;31m         \u001b[0mmodule_spec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachinery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleSpec\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2185\u001b[0m         \u001b[0mextra_objects\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2186\u001b[0m         \u001b[0mexplicit_import_shortcut\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2180\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2181\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2182\u001b[0;31m         \u001b[0mmodule_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2183\u001b[0m         \u001b[0mimport_structure\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIMPORT_STRUCTURE_T\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2184\u001b[0m         \u001b[0mmodule_spec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachinery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleSpec\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mmodeling_layers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGradientCheckpointingLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mmodeling_outputs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseModelOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseModelOutputWithPooling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageClassifierOutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mmodeling_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mALL_ATTENTION_FUNCTIONS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_docstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcan_return_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_out_non_signature_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_int\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfiguration_clip\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCLIPConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIPTextConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIPVisionConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdynamic_module_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcustom_object_save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCompileConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenerationConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mintegrations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPeftAdapterMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepspeed_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_deepspeed_zero3_enabled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_fsdp_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mintegrations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind_tied_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_empty_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mintegrations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_load_state_dict_into_zero3_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'PeftAdapterMixin' from 'transformers.integrations' (/usr/local/lib/python3.11/dist-packages/transformers/integrations/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'PeftAdapterMixin' from 'transformers.integrations' (/usr/local/lib/python3.11/dist-packages/transformers/integrations/__init__.py)","output_type":"error"}],"execution_count":21},{"cell_type":"code","source":"!pip install -U timm opencv-python-headless torch torchvision torchaudio -q\nimport torch, cv2, numpy as np, timm\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = timm.create_model(\"resnet50\", pretrained=True, num_classes=0)  # фича-экстрактор\nmodel.eval().to(DEVICE)\n\n# нормализация как у ImageNet\nMEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\nSTD  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n\n@torch.no_grad()\ndef embed_batch(paths):\n    batch = []\n    for p in paths:\n        bgr = cv2.imread(p, cv2.IMREAD_COLOR)\n        if bgr is None:\n            raise FileNotFoundError(p)\n        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n        img = cv2.resize(rgb, (224, 224), interpolation=cv2.INTER_AREA).astype(np.float32) / 255.0\n        img = (img - MEAN) / STD\n        x = torch.from_numpy(img).permute(2, 0, 1)  # CHW\n        batch.append(x)\n    x = torch.stack(batch).to(DEVICE)\n    feats = model(x)                    # shape [B, 2048]\n    feats = feats / (feats.norm(dim=-1, keepdim=True) + 1e-12)\n    return feats.detach().cpu().numpy().astype(\"float32\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T22:32:48.339869Z","iopub.execute_input":"2025-11-18T22:32:48.340557Z","iopub.status.idle":"2025-11-18T22:32:54.213574Z","shell.execute_reply.started":"2025-11-18T22:32:48.340529Z","shell.execute_reply":"2025-11-18T22:32:54.212339Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/3503997242.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install -U timm opencv-python-headless torch torchvision torchaudio -q'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mDEVICE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resnet50\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# фича-экстрактор\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m from .layers import (\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mis_scriptable\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mis_scriptable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mis_exportable\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mis_exportable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mset_scriptable\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mset_scriptable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from ._fx import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mcreate_feature_extractor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mget_graph_node_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mregister_notrace_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mregister_notrace_module\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/layers/_fx.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# NOTE we wrap torchvision fns to use timm leaf / no trace definitions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_feature_extractor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_create_feature_extractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_graph_node_names\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_get_graph_node_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mhas_fx_feature_extraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# .extensions) before entering _meta_registrations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0malexnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconvnext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdensenet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mefficientnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgooglenet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/convnext.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2dNormActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPermute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstochastic_depth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStochasticDepth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_presets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/ops/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_register_onnx_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_register_custom_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m from .boxes import (\n\u001b[1;32m      3\u001b[0m     \u001b[0mbatched_nms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbox_area\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbox_convert\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/ops/_register_onnx_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msymbolic_opset11\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopset11\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_helper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/onnx/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_onnx_program\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mONNXProgram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m from ._internal.torchscript_exporter import (  # Deprecated members that are excluded from __all__\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/onnx/ops/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_symbolic_impl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/onnx/ops/_impl.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_dtype_mappings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/onnx/ops/_dtype_mappings.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;36m21\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# UINT4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;36m22\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# INT4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;36m23\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat4_e2m1fn_x2\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# FLOAT4E2M1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m }\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   2679\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2680\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2681\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2682\u001b[0m \u001b[0;31m# Enable CUDA Sanitizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2683\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"TORCH_CUDA_SANITIZER\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'float4_e2m1fn_x2'"],"ename":"AttributeError","evalue":"module 'torch' has no attribute 'float4_e2m1fn_x2'","output_type":"error"}],"execution_count":16}]}
