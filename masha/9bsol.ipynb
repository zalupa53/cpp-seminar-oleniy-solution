{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91496,"databundleVersionId":11802066,"sourceType":"competition"},{"sourceId":13423638,"sourceType":"datasetVersion","datasetId":8519945}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport math\nimport random\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport scipy.sparse as sp\n\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-18T14:10:12.820242Z","iopub.execute_input":"2025-10-18T14:10:12.820502Z","iopub.status.idle":"2025-10-18T14:10:18.077907Z","shell.execute_reply.started":"2025-10-18T14:10:12.820482Z","shell.execute_reply":"2025-10-18T14:10:18.077278Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom collections import defaultdict\nimport warnings\nfrom tqdm import tqdm\n\nwarnings.filterwarnings('ignore')\n\nclass TShoppingRecommender:\n    def __init__(self, recent_days_window=3):\n        self.user_recent_clicks = defaultdict(list)\n        self.global_popularity = defaultdict(int)\n        self.last_day = None\n        self.recent_days_window = recent_days_window\n        \n    def fit(self, train_data):\n        \"\"\"Обучение модели на исторических данных\"\"\"\n        print(\"Обработка тренировочных данных...\")\n        \n        # Находим последний день в данных\n        self.last_day = train_data['date'].max()\n        print(f\"Последний день в данных: {self.last_day}\")\n        print(f\"Используется окно из {self.recent_days_window} последних дней\")\n        \n        # Собираем клики пользователей за последние N дней\n        recent_days = [self.last_day - i for i in range(self.recent_days_window)]\n        recent_data = train_data[train_data['date'].isin(recent_days)]\n        \n        print(f\"Найдено взаимодействий за последние {self.recent_days_window} дней: {len(recent_data)}\")\n        \n        print(\"Сбор персональных предпочтений...\")\n        # Используем tqdm для прогресса\n        for _, row in tqdm(recent_data.iterrows(), total=len(recent_data), desc=\"Обработка кликов\"):\n            user_id = row['user_id']\n            item_id = row['item_id']\n            date = row['date']\n            \n            # Сохраняем клики с учетом даты (более поздние имеют больший вес)\n            self.user_recent_clicks[user_id].append((item_id, date))\n        \n        print(\"Сортировка кликов по дате...\")\n        # Сортируем клики каждого пользователя по дате (сначала последние)\n        for user_id in tqdm(self.user_recent_clicks.keys(), desc=\"Сортировка пользователей\"):\n            self.user_recent_clicks[user_id].sort(key=lambda x: x[1], reverse=True)\n        \n        # Собираем глобальную популярность товаров за последний день\n        last_day_data = train_data[train_data['date'] == self.last_day]\n        item_counts = last_day_data['item_id'].value_counts()\n        \n        print(\"Расчет глобальной популярности...\")\n        for item_id, count in tqdm(item_counts.items(), total=len(item_counts), desc=\"Популярные товары\"):\n            self.global_popularity[item_id] = count\n            \n        print(f\"Обработано пользователей: {len(self.user_recent_clicks)}\")\n        print(f\"Уникальных популярных товаров: {len(self.global_popularity)}\")\n    \n    def predict(self, user_ids, top_k=20):\n        \"\"\"Предсказание топ-K товаров для каждого пользователя\"\"\"\n        print(\"Генерация рекомендаций...\")\n        \n        # Создаем глобальный рейтинг популярности\n        global_ranking = list(self.global_popularity.keys())\n        \n        submissions = []\n        \n        # Используем tqdm для прогресса предсказания\n        for user_id in tqdm(user_ids, desc=\"Генерация рекомендаций\"):\n            user_recommendations = []\n            \n            # Берем уникальные товары из последних кликов пользователя\n            if user_id in self.user_recent_clicks:\n                recent_items = []\n                for item_id, date in self.user_recent_clicks[user_id]:\n                    if item_id not in recent_items:\n                        recent_items.append(item_id)\n                    if len(recent_items) >= top_k:\n                        break\n                \n                user_recommendations.extend(recent_items)\n            \n            # Если недостаточно персональных рекомендаций, добавляем глобально популярные\n            if len(user_recommendations) < top_k:\n                # Берем только те товары, которых еще нет в рекомендациях\n                additional_items = [item for item in global_ranking \n                                  if item not in user_recommendations]\n                \n                # Добавляем столько, сколько нужно до top_k\n                needed = top_k - len(user_recommendations)\n                user_recommendations.extend(additional_items[:needed])\n            \n            # Обеспечиваем, что рекомендаций ровно top_k\n            user_recommendations = user_recommendations[:top_k]\n            \n            # Добавляем в submission\n            for item_id in user_recommendations:\n                submissions.append({\n                    'user_id': user_id,\n                    'item_id': item_id\n                })\n        \n        return pd.DataFrame(submissions)\n\ndef main():\n    # Загрузка данных\n    print(\"Загрузка данных...\")\n    train_data = pd.read_parquet('/kaggle/input/stupidshit777/train_data.pq')\n    \n    # Загрузка sample submission для получения списка пользователей\n    sample_submission = pd.read_csv('/kaggle/input/stupidshit777/sample_submission (11).csv')\n    test_users = sample_submission['user_id'].unique()\n    \n    print(f\"Всего пользователей в тесте: {len(test_users)}\")\n    print(f\"Всего взаимодействий в тренировочных данных: {len(train_data)}\")\n    print(f\"Уникальных пользователей: {train_data['user_id'].nunique()}\")\n    print(f\"Уникальных товаров: {train_data['item_id'].nunique()}\")\n    \n    # Настройка параметров\n    RECENT_DAYS_WINDOW = 15  # Можно изменить на 3, 5, 10 и т.д.\n    \n    # Обучение модели\n    model = TShoppingRecommender(recent_days_window=RECENT_DAYS_WINDOW)\n    model.fit(train_data)\n    \n    # Предсказание\n    submission_df = model.predict(test_users, top_k=20)\n    \n    # Проверка формата\n    print(f\"\\nПроверка формата submission:\")\n    print(f\"Всего строк в submission: {len(submission_df)}\")\n    print(f\"Ожидалось: {len(test_users) * 20}\")\n    \n    # Сохранение результатов\n    submission_df.to_csv('submission.csv', index=False)\n    print(\"\\nSubmission сохранен в submission.csv\")\n    \n    # Пример рекомендаций для первых 5 пользователей\n    print(\"\\nПример рекомендаций для первых 5 пользователей:\")\n    for i, user_id in enumerate(test_users[:5]):\n        user_recs = submission_df[submission_df['user_id'] == user_id]['item_id'].tolist()\n        print(f\"Пользователь {user_id}: {user_recs[:5]}... (всего {len(user_recs)} рекомендаций)\")\n    \n    # Статистика по рекомендациям\n    print(f\"\\nСтатистика:\")\n    print(f\"Использовано дней для персональных предпочтений: {RECENT_DAYS_WINDOW}\")\n    print(f\"Размер глобального пула популярных товаров: {len(model.global_popularity)}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T17:23:29.332115Z","iopub.execute_input":"2025-10-18T17:23:29.332427Z","iopub.status.idle":"2025-10-18T17:53:59.972814Z","shell.execute_reply.started":"2025-10-18T17:23:29.332402Z","shell.execute_reply":"2025-10-18T17:53:59.971031Z"}},"outputs":[{"name":"stdout","text":"Загрузка данных...\nВсего пользователей в тесте: 293230\nВсего взаимодействий в тренировочных данных: 8777975\nУникальных пользователей: 2682603\nУникальных товаров: 740651\nОбработка тренировочных данных...\nПоследний день в данных: 46\nИспользуется окно из 15 последних дней\nНайдено взаимодействий за последние 15 дней: 3030646\nСбор персональных предпочтений...\n","output_type":"stream"},{"name":"stderr","text":"Обработка кликов: 100%|██████████| 3030646/3030646 [02:14<00:00, 22549.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Сортировка кликов по дате...\n","output_type":"stream"},{"name":"stderr","text":"Сортировка пользователей: 100%|██████████| 1157076/1157076 [00:01<00:00, 973740.50it/s] \n","output_type":"stream"},{"name":"stdout","text":"Расчет глобальной популярности...\n","output_type":"stream"},{"name":"stderr","text":"Популярные товары: 100%|██████████| 56659/56659 [00:00<00:00, 1708275.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Обработано пользователей: 1157076\nУникальных популярных товаров: 56659\nГенерация рекомендаций...\n","output_type":"stream"},{"name":"stderr","text":"Генерация рекомендаций: 100%|██████████| 293230/293230 [27:46<00:00, 175.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nПроверка формата submission:\nВсего строк в submission: 5864600\nОжидалось: 5864600\n\nSubmission сохранен в submission.csv\n\nПример рекомендаций для первых 5 пользователей:\nПользователь 247446: [1030, 658, 114, 302, 20]... (всего 20 рекомендаций)\nПользователь 352619: [658, 114, 302, 20, 34]... (всего 20 рекомендаций)\nПользователь 352620: [15093, 658, 114, 302, 20]... (всего 20 рекомендаций)\nПользователь 352622: [154849, 884, 106647, 437758, 68094]... (всего 20 рекомендаций)\nПользователь 3257: [171, 5049, 658, 57227, 2144]... (всего 20 рекомендаций)\n\nСтатистика:\nИспользовано дней для персональных предпочтений: 15\nРазмер глобального пула популярных товаров: 56659\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom collections import defaultdict\nimport warnings\nfrom tqdm import tqdm\n\nwarnings.filterwarnings('ignore')\n\nclass TShoppingRecommender:\n    def __init__(self, recent_days_window=3):\n        self.user_recent_clicks = defaultdict(list)\n        self.global_popularity = defaultdict(int)\n        self.last_day = None\n        self.recent_days_window = recent_days_window\n        \n    def fit(self, train_data):\n        \"\"\"Обучение модели на исторических данных\"\"\"\n        print(\"Обработка тренировочных данных...\")\n        \n        # Находим последний день в данных\n        self.last_day = train_data['date'].max()\n        print(f\"Последний день в данных: {self.last_day}\")\n        print(f\"Используется окно из {self.recent_days_window} последних дней\")\n        \n        # Собираем клики пользователей за последние N дней\n        recent_days = [self.last_day - i for i in range(self.recent_days_window)]\n        recent_data = train_data[train_data['date'].isin(recent_days)]\n        \n        print(f\"Найдено взаимодействий за последние {self.recent_days_window} дней: {len(recent_data)}\")\n        \n        print(\"Сбор персональных предпочтений...\")\n        # Используем tqdm для прогресса\n        for _, row in tqdm(recent_data.iterrows(), total=len(recent_data), desc=\"Обработка кликов\"):\n            user_id = row['user_id']\n            item_id = row['item_id']\n            date = row['date']\n            \n            # Сохраняем клики с учетом даты (более поздние имеют больший вес)\n            self.user_recent_clicks[user_id].append((item_id, date))\n        \n        print(\"Сортировка кликов по дате...\")\n        # Сортируем клики каждого пользователя по дате (сначала последние)\n        for user_id in tqdm(self.user_recent_clicks.keys(), desc=\"Сортировка пользователей\"):\n            self.user_recent_clicks[user_id].sort(key=lambda x: x[1], reverse=True)\n        \n        # Собираем глобальную популярность товаров за последний день\n        last_day_data = train_data[train_data['date'] == self.last_day]\n        item_counts = last_day_data['item_id'].value_counts()\n        \n        print(\"Расчет глобальной популярности...\")\n        for item_id, count in tqdm(item_counts.items(), total=len(item_counts), desc=\"Популярные товары\"):\n            self.global_popularity[item_id] = count\n            \n        print(f\"Обработано пользователей: {len(self.user_recent_clicks)}\")\n        print(f\"Уникальных популярных товаров: {len(self.global_popularity)}\")\n    \n    def predict(self, user_ids, top_k=20):\n        \"\"\"Предсказание топ-K товаров для каждого пользователя\"\"\"\n        print(\"Генерация рекомендаций...\")\n        \n        # Создаем глобальный рейтинг популярности\n        global_ranking = list(self.global_popularity.keys())\n        \n        submissions = []\n        \n        # Используем tqdm для прогресса предсказания\n        for user_id in tqdm(user_ids, desc=\"Генерация рекомендаций\"):\n            user_recommendations = []\n            \n            # Берем уникальные товары из последних кликов пользователя\n            if user_id in self.user_recent_clicks:\n                recent_items = []\n                for item_id, date in self.user_recent_clicks[user_id]:\n                    if item_id not in recent_items:\n                        recent_items.append(item_id)\n                    if len(recent_items) >= top_k:\n                        break\n                \n                user_recommendations.extend(recent_items)\n            \n            # Если недостаточно персональных рекомендаций, добавляем глобально популярные\n            if len(user_recommendations) < top_k:\n                # Берем только те товары, которых еще нет в рекомендациях\n                additional_items = [item for item in global_ranking \n                                  if item not in user_recommendations]\n                \n                # Добавляем столько, сколько нужно до top_k\n                needed = top_k - len(user_recommendations)\n                user_recommendations.extend(additional_items[:needed])\n            \n            # Обеспечиваем, что рекомендаций ровно top_k\n            user_recommendations = user_recommendations[:top_k]\n            \n            # Добавляем в submission\n            for item_id in user_recommendations:\n                submissions.append({\n                    'user_id': user_id,\n                    'item_id': item_id\n                })\n        \n        return pd.DataFrame(submissions)\n\ndef main():\n    # Загрузка данных\n    print(\"Загрузка данных...\")\n    train_data = pd.read_parquet('/kaggle/input/stupidshit777/train_data.pq')\n    \n    # Загрузка sample submission для получения списка пользователей\n    sample_submission = pd.read_csv('/kaggle/input/stupidshit777/sample_submission (11).csv')\n    test_users = sample_submission['user_id'].unique()\n    \n    print(f\"Всего пользователей в тесте: {len(test_users)}\")\n    print(f\"Всего взаимодействий в тренировочных данных: {len(train_data)}\")\n    print(f\"Уникальных пользователей: {train_data['user_id'].nunique()}\")\n    print(f\"Уникальных товаров: {train_data['item_id'].nunique()}\")\n    \n    # Настройка параметров\n    RECENT_DAYS_WINDOW = 15  # Можно изменить на 3, 5, 10 и т.д.\n    \n    # Обучение модели\n    model = TShoppingRecommender(recent_days_window=RECENT_DAYS_WINDOW)\n    model.fit(train_data)\n    \n    # Предсказание\n    submission_df = model.predict(test_users, top_k=20)\n    \n    # Проверка формата\n    print(f\"\\nПроверка формата submission:\")\n    print(f\"Всего строк в submission: {len(submission_df)}\")\n    print(f\"Ожидалось: {len(test_users) * 20}\")\n    \n    # Сохранение результатов\n    submission_df.to_csv('submission.csv', index=False)\n    print(\"\\nSubmission сохранен в submission.csv\")\n    \n    # Пример рекомендаций для первых 5 пользователей\n    print(\"\\nПример рекомендаций для первых 5 пользователей:\")\n    for i, user_id in enumerate(test_users[:5]):\n        user_recs = submission_df[submission_df['user_id'] == user_id]['item_id'].tolist()\n        print(f\"Пользователь {user_id}: {user_recs[:5]}... (всего {len(user_recs)} рекомендаций)\")\n    \n    # Статистика по рекомендациям\n    print(f\"\\nСтатистика:\")\n    print(f\"Использовано дней для персональных предпочтений: {RECENT_DAYS_WINDOW}\")\n    print(f\"Размер глобального пула популярных товаров: {len(model.global_popularity)}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T17:17:54.677335Z","iopub.execute_input":"2025-10-18T17:17:54.677720Z"}},"outputs":[{"name":"stdout","text":"Загрузка данных...\nВсего пользователей в тесте: 293230\nВсего взаимодействий в тренировочных данных: 8777975\nУникальных пользователей: 2682603\nУникальных товаров: 740651\nОбработка тренировочных данных...\nПоследний день в данных: 46\nИспользуется окно из 15 последних дней\nНайдено взаимодействий за последние 15 дней: 3030646\nСбор персональных предпочтений...\n","output_type":"stream"},{"name":"stderr","text":"Обработка кликов:  64%|██████▍   | 1938742/3030646 [01:26<00:47, 23029.43it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install polars pyarrow numpy scipy pandas tqdm torch scikit-learn\n!pip install implicit\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T12:30:33.896108Z","iopub.execute_input":"2025-10-18T12:30:33.896428Z","iopub.status.idle":"2025-10-18T12:31:57.766225Z","shell.execute_reply.started":"2025-10-18T12:30:33.896401Z","shell.execute_reply":"2025-10-18T12:31:57.765469Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: polars in /usr/local/lib/python3.11/dist-packages (1.25.0)\nRequirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (19.0.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.19.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.9.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nCollecting implicit\n  Downloading implicit-0.7.2-cp311-cp311-manylinux2014_x86_64.whl.metadata (6.1 kB)\nRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from implicit) (1.26.4)\nRequirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.11/dist-packages (from implicit) (1.15.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from implicit) (4.67.1)\nRequirement already satisfied: threadpoolctl in /usr/local/lib/python3.11/dist-packages (from implicit) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->implicit) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->implicit) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->implicit) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->implicit) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->implicit) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->implicit) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.0->implicit) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.0->implicit) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.0->implicit) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.0->implicit) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.0->implicit) (2024.2.0)\nDownloading implicit-0.7.2-cp311-cp311-manylinux2014_x86_64.whl (8.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: implicit\nSuccessfully installed implicit-0.7.2\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"pip install faiss-gpu-cu12","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T12:30:13.613941Z","iopub.execute_input":"2025-10-18T12:30:13.614213Z","iopub.status.idle":"2025-10-18T12:30:20.627023Z","shell.execute_reply.started":"2025-10-18T12:30:13.614192Z","shell.execute_reply":"2025-10-18T12:30:20.626257Z"}},"outputs":[{"name":"stdout","text":"Collecting faiss-gpu-cu12\n  Downloading faiss_gpu_cu12-1.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nRequirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from faiss-gpu-cu12) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-gpu-cu12) (25.0)\nRequirement already satisfied: nvidia-cuda-runtime-cu12>=12.1.105 in /usr/local/lib/python3.11/dist-packages (from faiss-gpu-cu12) (12.5.82)\nRequirement already satisfied: nvidia-cublas-cu12>=12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from faiss-gpu-cu12) (12.5.3.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2->faiss-gpu-cu12) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2->faiss-gpu-cu12) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2->faiss-gpu-cu12) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2->faiss-gpu-cu12) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2->faiss-gpu-cu12) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2->faiss-gpu-cu12) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2->faiss-gpu-cu12) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2->faiss-gpu-cu12) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2->faiss-gpu-cu12) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2->faiss-gpu-cu12) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2->faiss-gpu-cu12) (2024.2.0)\nDownloading faiss_gpu_cu12-1.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (48.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-gpu-cu12\nSuccessfully installed faiss-gpu-cu12-1.12.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"pip install faiss-gpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T12:30:24.086152Z","iopub.execute_input":"2025-10-18T12:30:24.086727Z","iopub.status.idle":"2025-10-18T12:30:25.595411Z","shell.execute_reply.started":"2025-10-18T12:30:24.086695Z","shell.execute_reply":"2025-10-18T12:30:25.594479Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from implicit.gpu.als import AlternatingLeastSquares as GPU_ALS","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T12:31:57.767693Z","iopub.execute_input":"2025-10-18T12:31:57.767933Z","iopub.status.idle":"2025-10-18T12:31:57.787838Z","shell.execute_reply.started":"2025-10-18T12:31:57.767912Z","shell.execute_reply":"2025-10-18T12:31:57.787255Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"GPU ALS not available (No CUDA extension has been built, can't train on GPU.), falling back to CPU.\nTraining ALS (CPU)..","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from implicit.gpu.als import AlternatingLeastSquares as GPU_ALS\nals = GPU_ALS(factors=factors, regularization=reg, iterations=iters, random_state=42)\nprint(\"Training ALS (GPU)...\")\nals.fit(ui.T, show_progress=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T12:34:14.877477Z","iopub.execute_input":"2025-10-18T12:34:14.877982Z","iopub.status.idle":"2025-10-18T12:34:14.888610Z","shell.execute_reply.started":"2025-10-18T12:34:14.877962Z","shell.execute_reply":"2025-10-18T12:34:14.887650Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/2828858681.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimplicit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAlternatingLeastSquares\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mGPU_ALS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPU_ALS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfactors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training ALS (GPU)...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mui\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'factors' is not defined"],"ename":"NameError","evalue":"name 'factors' is not defined","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"# -----------------------------\n# Utils\n# -----------------------------\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef average_precision_at_k(true_items_set, ranked_items, k=20):\n    if not true_items_set:\n        return 0.0\n    score = 0.0\n    hits = 0\n    for i, item in enumerate(ranked_items[:k]):\n        if item in true_items_set:\n            hits += 1\n            score += hits / (i + 1)\n    return score / min(len(true_items_set), k)\n\ndef map_at_k(gt_by_user, preds_by_user, k=20):\n    aps = []\n    for u, true_set in gt_by_user.items():\n        preds = preds_by_user.get(u, [])\n        aps.append(average_precision_at_k(true_set, preds, k=k))\n    return float(np.mean(aps)) if aps else 0.0\n\n# -----------------------------\n# Load data\n# -----------------------------\ndef load_data(train_path=\"train_data.pq\", sample_path=\"sample_submission.csv\"):\n    print(\"Reading parquet with Polars...\")\n    dfpl = pl.read_parquet(train_path)\n    # Ensure dtypes\n    dfpl = dfpl.with_columns([\n        pl.col(\"user_id\").cast(pl.Int64),\n        pl.col(\"item_id\").cast(pl.Int64),\n        pl.col(\"date\").cast(pl.Int32)\n    ])\n    df = dfpl.to_pandas()\n    del dfpl\n    gc.collect()\n\n    print(\"Mapping ids to contiguous indices...\")\n    df[\"uid\"], user_uniques = pd.factorize(df[\"user_id\"], sort=True)\n    df[\"iid\"], item_uniques = pd.factorize(df[\"item_id\"], sort=True)\n\n    uid2orig = np.array(user_uniques)\n    iid2orig = np.array(item_uniques)\n\n    # Sort by user, then by date for sequence models\n    df.sort_values([\"uid\", \"date\"], inplace=True)\n    df.reset_index(drop=True, inplace=True)\n\n    print(\"Loading sample users...\")\n    sample = pd.read_csv(sample_path)\n    # Sample repeats user_id 20 times; we only need unique users\n    sample_users = sample[\"user_id\"].unique()\n    # Map sample users to internal uids\n    # Build map from original user_id to uid\n    user_orig2uid = {orig: idx for idx, orig in enumerate(uid2orig)}\n    sample_uids = []\n    for u in sample_users:\n        if u in user_orig2uid:\n            sample_uids.append(user_orig2uid[u])\n        else:\n            # shouldn't happen; but keep -1 to handle later\n            sample_uids.append(-1)\n    sample_uids = np.array(sample_uids, dtype=np.int64)\n\n    return df, uid2orig, iid2orig, sample_users, sample_uids\n\n# -----------------------------\n# Split: last 7 days as validation holdout\n# -----------------------------\ndef time_split(df, holdout_days=7):\n    max_day = int(df[\"date\"].max())\n    val_start = max_day - (holdout_days - 1)\n    train_mask = df[\"date\"] < val_start\n    val_mask = df[\"date\"] >= val_start\n\n    train_df = df[train_mask].copy()\n    val_df = df[val_mask].copy()\n\n    # Ground truth for validation: set of items per user in holdout period\n    gt_val = {}\n    for uid, grp in tqdm(val_df.groupby(\"uid\", sort=False), desc=\"Build val GT\"):\n        gt_val[uid] = set(grp[\"iid\"].tolist())\n\n    return train_df, val_df, gt_val\n\n# -----------------------------\n# Build CSR matrix for implicit ALS\n# -----------------------------\ndef build_user_item_csr(df, n_users, n_items):\n    rows = df[\"uid\"].values.astype(np.int32)\n    cols = df[\"iid\"].values.astype(np.int32)\n    data = np.ones_like(rows, dtype=np.float32)\n    ui = sp.coo_matrix((data, (rows, cols)), shape=(n_users, n_items), dtype=np.float32).tocsr()\n    return ui\n\n# -----------------------------\n# ALS Retrieval\n# -----------------------------\ndef train_als(user_item_csr, use_gpu=True, factors=128, reg=1e-4, iters=20):\n    try:\n        import implicit\n        from implicit.nearest_neighbours import bm25_weight\n        ui = bm25_weight(user_item_csr, K1=1.2, B=0.75).tocsr()\n        gc.collect()\n\n        if use_gpu:\n            #try:\n                from implicit.gpu.als import AlternatingLeastSquares as GPU_ALS\n                als = GPU_ALS(factors=factors, regularization=reg, iterations=iters, random_state=42)\n                print(\"Training ALS (GPU)...\")\n                als.fit(ui.T, show_progress=True)\n                return als, ui\n            #except Exception as e:\n                print(f\"GPU ALS not available ({e}), falling back to CPU.\")\n        from implicit.als import AlternatingLeastSquares as CPU_ALS\n        als = CPU_ALS(factors=factors, regularization=reg, iterations=iters, use_cg=True, random_state=42)\n        print(\"Training ALS (CPU)...\")\n        als.fit(ui.T, show_progress=True)\n        return als, ui\n    except Exception as e:\n        print(f\"Implicit not installed or failed: {e}\")\n        return None, user_item_csr\n\ndef als_recommend_batch(model, user_item_csr, user_ids, N=300, filter_seen=False, batch_size=1024):\n    recs = {}\n    desc = \"ALS recommend\"\n    for i in tqdm(range(0, len(user_ids), batch_size), desc=desc):\n        batch = user_ids[i: i+batch_size]\n        for u in batch:\n            if u < 0:\n                recs[u] = (np.empty((0,), dtype=np.int32), np.empty((0,), dtype=np.float32))\n                continue\n            try:\n                ids, scores = model.recommend(\n                    userid=int(u),\n                    user_items=user_item_csr,\n                    N=N,\n                    recalculate_user=True,\n                    filter_already_liked_items=filter_seen\n                )\n                recs[u] = (ids.astype(np.int32), scores.astype(np.float32))\n            except Exception:\n                recs[u] = (np.empty((0,), dtype=np.int32), np.empty((0,), dtype=np.float32))\n    return recs\n\n# -----------------------------\n# FAISS index for item similarity (ALS item factors)\n# -----------------------------\ndef build_faiss_index(item_factors):\n    try:\n        import faiss\n    except Exception as e:\n        print(f\"FAISS not available: {e}\")\n        return None, None\n\n    d = item_factors.shape[1]\n    try:\n        res = faiss.StandardGpuResources() if torch.cuda.is_available() else None\n    except Exception:\n        res = None\n\n    index = faiss.IndexFlatIP(d)\n    if res is not None:\n        index = faiss.index_cpu_to_gpu(res, 0, index)\n    # Optionally normalize for cosine similarity; but ALS uses IP\n    # faiss.normalize_L2(item_factors)\n    index.add(item_factors.astype(np.float32))\n    return index, res\n\ndef similar_items_for_users(index, item_factors, user_histories, topk_per_item=50, last_k=10):\n    \"\"\"\n    user_histories: dict uid -> np.array of recent item ids (0-based)\n    returns dict uid -> (item_ids, scores) aggregated over last_k items\n    \"\"\"\n    try:\n        import faiss\n    except Exception:\n        index = None\n\n    recs = {}\n    if index is None:\n        return recs\n\n    for uid, hist in tqdm(user_histories.items(), desc=\"FAISS item2item\"):\n        if hist.size == 0:\n            recs[uid] = (np.empty((0,), dtype=np.int32), np.empty((0,), dtype=np.float32))\n            continue\n        queries = hist[-last_k:]\n        # Get neighbors for each query\n        # Note: include self in results; we will drop identical ids\n        distances, neighbors = index.search(item_factors[queries].astype(np.float32), k=topk_per_item+1)\n        # Aggregate by sum of scores, exclude original items\n        agg = defaultdict(float)\n        hist_set = set(queries.tolist())\n        for row in range(neighbors.shape[0]):\n            for j in range(neighbors.shape[1]):\n                it = int(neighbors[row, j])\n                if it in hist_set:\n                    continue\n                agg[it] += float(distances[row, j])\n        if not agg:\n            recs[uid] = (np.empty((0,), dtype=np.int32), np.empty((0,), dtype=np.float32))\n        else:\n            items = np.fromiter(agg.keys(), dtype=np.int32)\n            scores = np.fromiter(agg.values(), dtype=np.float32)\n            order = np.argsort(-scores)\n            recs[uid] = (items[order], scores[order])\n    return recs\n\n# -----------------------------\n# Popularity candidates\n# -----------------------------\ndef popularity_scores(df_train, df_recent_days=7, n_items=None):\n    max_day = int(df_train[\"date\"].max())\n    recent_start = max_day - (df_recent_days - 1)\n\n    all_cnt = df_train.groupby(\"iid\").size()\n    rec_cnt = df_train[df_train[\"date\"] >= recent_start].groupby(\"iid\").size()\n\n    # Align\n    if n_items is None:\n        n_items = int(df_train[\"iid\"].max()) + 1\n    all_vec = np.zeros(n_items, dtype=np.float32)\n    rec_vec = np.zeros(n_items, dtype=np.float32)\n\n    all_vec[all_cnt.index.values] = all_cnt.values.astype(np.float32)\n    rec_vec[rec_cnt.index.values] = rec_cnt.values.astype(np.float32)\n\n    # Normalize\n    all_vec /= (all_vec.max() + 1e-9)\n    rec_vec /= (rec_vec.max() + 1e-9)\n\n    # Weighted blend (tuneable)\n    pop = 0.3 * all_vec + 0.7 * rec_vec\n    return pop\n\ndef top_pop_items(pop_scores, topk=500):\n    idx = np.argsort(-pop_scores)[:topk]\n    return idx, pop_scores[idx]\n\n# -----------------------------\n# Build user sequences (offsets) for SASRec\n# -----------------------------\ndef build_user_offsets(df, n_users):\n    # df must be sorted by [\"uid\", \"date\"]\n    uids = df[\"uid\"].values.astype(np.int64)\n    iids = df[\"iid\"].values.astype(np.int64)\n    # Keep 0-based for ALS; SASRec will use +1 for padding\n    counts = np.bincount(uids, minlength=n_users).astype(np.int64)\n    offsets = np.zeros(n_users + 1, dtype=np.int64)\n    offsets[1:] = np.cumsum(counts)\n    # items array already in user-sorted order\n    return uids, iids, offsets\n\ndef get_user_last_items(iids, offsets, uids):\n    user_hist = {}\n    for u in tqdm(np.unique(uids), desc=\"Collect last items per user\"):\n        s, e = offsets[u], offsets[u+1]\n        if e - s > 0:\n            user_hist[u] = iids[s:e]\n        else:\n            user_hist[u] = np.array([], dtype=np.int64)\n    return user_hist\n\n# -----------------------------\n# SASRec model\n# -----------------------------\nclass SASRec(nn.Module):\n    def __init__(self, n_items_padded, max_len=50, d_model=128, n_heads=4, n_layers=2, dropout=0.2):\n        super().__init__()\n        self.n_items = n_items_padded\n        self.max_len = max_len\n        self.item_emb = nn.Embedding(n_items_padded, d_model, padding_idx=0)\n        self.pos_emb = nn.Embedding(max_len, d_model)\n        layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dim_feedforward=d_model*4,\n                                           dropout=dropout, batch_first=True)\n        self.encoder = nn.TransformerEncoder(layer, num_layers=n_layers, norm=nn.LayerNorm(d_model))\n        self.dropout = nn.Dropout(dropout)\n\n        nn.init.normal_(self.item_emb.weight, mean=0.0, std=0.02)\n        nn.init.normal_(self.pos_emb.weight, mean=0.0, std=0.02)\n\n    def forward(self, seq_ids):\n        # seq_ids: (B, L) with 0 = PAD\n        B, L = seq_ids.size()\n        positions = torch.arange(L, device=seq_ids.device).unsqueeze(0).expand(B, L)\n        x = self.item_emb(seq_ids) + self.pos_emb(positions)\n        x = self.dropout(x)\n\n        # Causal mask (prevent attending to future positions)\n        causal_mask = torch.triu(torch.ones(L, L, device=seq_ids.device), diagonal=1).bool()\n        key_padding_mask = (seq_ids == 0)  # True for PAD\n\n        x = self.encoder(x, mask=causal_mask, src_key_padding_mask=key_padding_mask)\n        # Gather last non-pad timestep\n        lengths = (~key_padding_mask).sum(dim=1).clamp(min=1)  # at least 1\n        idx = (lengths - 1).unsqueeze(1).unsqueeze(2).expand(-1, 1, x.size(-1))  # B x 1 x D\n        out = x.gather(1, idx).squeeze(1)  # B x D\n        return out  # user representation\n\n    def item_embedding(self, item_ids):\n        return self.item_emb(item_ids)\n\n# -----------------------------\n# SASRec dataset\n# -----------------------------\nclass SequenceDataset(Dataset):\n    def __init__(self, items_by_user, offsets, max_len=50, users=None):\n        \"\"\"\n        items_by_user: 1D np.array of item ids (0-based), sorted by user/date\n        offsets: np.array of size n_users+1\n        \"\"\"\n        self.items = items_by_user.astype(np.int64)  # 0-based\n        self.offsets = offsets.astype(np.int64)\n        self.max_len = max_len\n        self.n_users = offsets.size - 1\n\n        if users is None:\n            # Only users with at least 2 interactions\n            all_users = np.arange(self.n_users, dtype=np.int64)\n            lens = self.offsets[1:] - self.offsets[:-1]\n            self.users = all_users[lens >= 2]\n        else:\n            self.users = np.array(users, dtype=np.int64)\n\n        # Shuffle initial order\n        self.shuffle_users()\n\n    def shuffle_users(self):\n        np.random.shuffle(self.users)\n\n    def __len__(self):\n        return len(self.users)\n\n    def __getitem__(self, idx):\n        u = self.users[idx]\n        s, e = self.offsets[u], self.offsets[u+1]\n        seq = self.items[s:e]  # 0-based\n        L = len(seq)\n        # Sample a cut point: prefix -> next item\n        # t in [1..L-1], target = seq[t]\n        t = np.random.randint(1, L)\n        prefix = seq[:t]\n        target = seq[t]\n\n        # Truncate to last max_len of prefix\n        if len(prefix) > self.max_len:\n            prefix = prefix[-self.max_len:]\n        # Convert to padded space: add 1\n        prefix_padded = prefix + 1  # 1..n_items\n        # Left-pad with zeros\n        pad_len = self.max_len - len(prefix_padded)\n        if pad_len > 0:\n            prefix_padded = np.pad(prefix_padded, (pad_len, 0), constant_values=0)\n\n        target_padded = int(target + 1)\n        return prefix_padded.astype(np.int64), target_padded\n\ndef collate_batch(batch):\n    seqs_np = np.stack([b[0] for b in batch], axis=0).astype(np.int64, copy=False)\n    seqs = torch.from_numpy(seqs_np)\n    targets = torch.tensor([b[1] for b in batch], dtype=torch.long)\n    return seqs, targets\n\n# -----------------------------\n# Train SASRec with in-batch negatives\n# -----------------------------\ndef train_sasrec(model, dataset, epochs=2, batch_size=1024, lr=1e-3, val_eval_fn=None):\n    model = model.to(DEVICE)\n    optim = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n    loss_fn = nn.CrossEntropyLoss()\n\n    steps_per_epoch = math.ceil(len(dataset) / batch_size)\n    for epoch in range(1, epochs + 1):\n        dataset.shuffle_users()\n        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2,\n                            collate_fn=collate_batch, pin_memory=True)\n        model.train()\n        pbar = tqdm(loader, total=steps_per_epoch, desc=f\"SASRec epoch {epoch}\")\n        losses = []\n        for seqs, targets in pbar:\n            seqs = seqs.to(DEVICE, non_blocking=True)\n            targets = targets.to(DEVICE, non_blocking=True)\n\n            user_vec = model(seqs)                       # (B, D)\n            pos_embs = model.item_embedding(targets)     # (B, D)\n            logits = user_vec @ pos_embs.T               # (B, B), in-batch negatives\n            labels = torch.arange(seqs.size(0), device=DEVICE)\n            loss = loss_fn(logits, labels)\n\n            optim.zero_grad(set_to_none=True)\n            loss.backward()\n            optim.step()\n\n            losses.append(loss.item())\n            if len(losses) % 20 == 0:\n                pbar.set_postfix(loss=np.mean(losses[-20:]))\n\n        avg_loss = np.mean(losses) if losses else 0.0\n        print(f\"Epoch {epoch} train loss: {avg_loss:.4f}\")\n\n        if val_eval_fn is not None:\n            val_map = val_eval_fn(model)\n            print(f\"Epoch {epoch} VAL mAP@20: {val_map:.5f}\")\n\n# -----------------------------\n# Candidate merger and reranking\n# -----------------------------\ndef merge_candidates(als_recs, item2item_recs, pop_items, pop_scores, users, topk_merge=500,\n                     w_als=1.0, w_i2i=1.0, w_pop=0.3):\n    \"\"\"\n    Return dict uid -> np.array of candidate item ids (0-based) with blended scores\n    \"\"\"\n    merged = {}\n    for u in tqdm(users, desc=\"Merge candidates\"):\n        scores = defaultdict(float)\n        # ALS\n        if als_recs is not None and u in als_recs:\n            ids, sc = als_recs[u]\n            for iid, s in zip(ids, sc):\n                scores[int(iid)] += w_als * float(s)\n        # i2i\n        if item2item_recs is not None and u in item2item_recs:\n            ids, sc = item2item_recs[u]\n            for iid, s in zip(ids, sc):\n                scores[int(iid)] += w_i2i * float(s)\n        # POP\n        if pop_items is not None:\n            for iid, s in zip(pop_items, pop_scores):\n                scores[int(iid)] += w_pop * float(s)\n\n        if not scores:\n            merged[u] = np.array([], dtype=np.int32)\n        else:\n            items = np.fromiter(scores.keys(), dtype=np.int32)\n            scs = np.fromiter(scores.values(), dtype=np.float32)\n            order = np.argsort(-scs)[:topk_merge]\n            merged[u] = items[order]\n    return merged\n\n@torch.no_grad()\ndef rerank_with_sasrec(model, user_candidates, items_by_user, offsets, max_len=50, batch_size=2048):\n    \"\"\"\n    user_candidates: dict uid -> np.array of candidates (0-based)\n    returns dict uid -> sorted list of item ids (0-based) by SASRec score\n    \"\"\"\n    model.eval()\n    uids = list(user_candidates.keys())\n    results = {}\n\n    # Prebuild user sequences (padded) for batch scoring\n    seq_cache = {}\n    for u in tqdm(uids, desc=\"Prepare sequences\"):\n        s, e = offsets[u], offsets[u+1]\n        hist = items_by_user[s:e]  # 0-based\n        if hist.size == 0:\n            seq_cache[u] = torch.zeros((max_len,), dtype=torch.long)\n        else:\n            seq = hist[-max_len:] + 1  # to padded space\n            pad_len = max_len - len(seq)\n            if pad_len > 0:\n                seq = np.pad(seq, (pad_len, 0), constant_values=0)\n            seq_cache[u] = torch.tensor(seq, dtype=torch.long)\n\n    # Batch over users\n    for i in tqdm(range(0, len(uids), batch_size), desc=\"Re-ranking\"):\n        batch_u = uids[i: i+batch_size]\n        # Build seq batch\n        seq_batch = torch.stack([seq_cache[u] for u in batch_u], dim=0).to(DEVICE)\n        user_vecs = model(seq_batch)  # (B, D)\n\n        # For each user, score their candidates\n        for j, u in enumerate(batch_u):\n            cands = user_candidates[u]\n            if cands.size == 0:\n                results[u] = []\n                continue\n            cands_pad = torch.tensor(cands + 1, dtype=torch.long, device=DEVICE)  # to padded idx\n            item_embs = model.item_embedding(cands_pad)  # (C, D)\n            logits = torch.mv(item_embs, user_vecs[j])   # (C,)\n            # sort by scores desc\n            order = torch.argsort(logits, descending=True).detach().cpu().numpy()\n            ranked = cands[order]\n            results[u] = ranked.tolist()\n    return results\n\n# -----------------------------\n# End-to-end\n# -----------------------------\ndef main(train_path=\"/kaggle/input/stupidshit777/train_data.pq\", sample_path=\"/kaggle/input/stupidshit777/sample_submission (11).csv\", submission_path=\"submission1.csv\",\n         holdout_days=7, max_seq_len=50):\n    # Load\n    df, uid2orig, iid2orig, sample_users_orig, sample_uids = load_data(train_path, sample_path)\n    n_users = int(df[\"uid\"].max()) + 1\n    n_items = int(df[\"iid\"].max()) + 1\n\n    # Split\n    train_df, val_df, gt_val = time_split(df, holdout_days=holdout_days)\n    print(f\"Train interactions: {len(train_df):,}, Val interactions: {len(val_df):,}\")\n    del val_df\n    gc.collect()\n\n    # CSR for ALS\n    user_item_csr = build_user_item_csr(train_df, n_users, n_items)\n\n    # Train ALS\n    als_model, weighted_ui = train_als(user_item_csr, use_gpu=torch.cuda.is_available(),\n                                       factors=128, reg=1e-4, iters=20)\n    als_item_factors = None\n    if als_model is not None:\n        try:\n            als_item_factors = als_model.item_factors  # (n_items, d)\n        except:\n            als_item_factors = None\n\n    # Build user sequences for SASRec\n    uids_sorted, iids_sorted, offsets = build_user_offsets(train_df, n_users)\n    # For item2item (FAISS), we need last items per user (we'll only for users we care about)\n    # We'll restrict to val users and test users to speed up\n    users_for_hist = np.unique(np.concatenate([np.array(list(gt_val.keys()), dtype=np.int64),\n                                               sample_uids[sample_uids >= 0]]))\n    user_histories = {}\n    for u in tqdm(users_for_hist, desc=\"Build histories for target users\"):\n        s, e = offsets[u], offsets[u+1]\n        user_histories[u] = iids_sorted[s:e]\n\n    # Popularity\n    pop_scores_vec = popularity_scores(train_df, df_recent_days=7, n_items=n_items)\n    pop_items, pop_scores = top_pop_items(pop_scores_vec, topk=300)\n\n    # ALS candidates\n    target_users_val = np.array(list(gt_val.keys()), dtype=np.int64)\n    als_val_recs = als_recommend_batch(als_model, weighted_ui if als_model is not None else user_item_csr,\n                                       target_users_val, N=300, filter_seen=False, batch_size=2048) if als_model else {}\n\n    # FAISS item2item candidates\n    faiss_index, _ = build_faiss_index(als_item_factors) if als_item_factors is not None else (None, None)\n    i2i_val_recs = similar_items_for_users(faiss_index, als_item_factors, user_histories,\n                                           topk_per_item=80, last_k=10) if faiss_index is not None else {}\n\n    # Merge candidates for VAL\n    cand_val = merge_candidates(als_val_recs, i2i_val_recs, pop_items, pop_scores, target_users_val,\n                                topk_merge=500, w_als=1.0, w_i2i=1.0, w_pop=0.3)\n\n    # SASRec training dataset\n    sasrec_items = iids_sorted  # 0-based\n    sasrec_dataset = SequenceDataset(sasrec_items, offsets, max_len=max_seq_len)\n\n    # Model\n    sasrec_model = SASRec(n_items_padded=n_items + 1, max_len=max_seq_len, d_model=128, n_heads=4, n_layers=2, dropout=0.2)\n\n    # Define validation evaluation function (rerank val candidates -> mAP@20)\n    def eval_val(model):\n        preds_val = rerank_with_sasrec(model, cand_val, sasrec_items, offsets, max_len=max_seq_len, batch_size=2048)\n        # Convert to simple dict uid->list (0-based)\n        # Compute mAP@20\n        return map_at_k(gt_val, preds_val, k=20)\n\n    # Train SASRec with val monitoring\n    train_sasrec(sasrec_model, sasrec_dataset, epochs=2, batch_size=1024, lr=3e-4, val_eval_fn=eval_val)\n\n    # Build candidates for TEST users (from sample)\n    test_users = sample_uids[sample_uids >= 0]\n    test_users = np.unique(test_users)\n    # ALS\n    als_test_recs = als_recommend_batch(als_model, weighted_ui if als_model is not None else user_item_csr,\n                                        test_users, N=300, filter_seen=False, batch_size=4096) if als_model else {}\n    # i2i\n    # Ensure we have histories\n    for u in test_users:\n        if u not in user_histories:\n            s, e = offsets[u], offsets[u+1]\n            user_histories[u] = iids_sorted[s:e]\n    i2i_test_recs = similar_items_for_users(faiss_index, als_item_factors, user_histories,\n                                            topk_per_item=80, last_k=10) if faiss_index is not None else {}\n    # Merge\n    cand_test = merge_candidates(als_test_recs, i2i_test_recs, pop_items, pop_scores, test_users,\n                                 topk_merge=500, w_als=1.0, w_i2i=1.0, w_pop=0.3)\n\n    # Rerank for TEST\n    preds_test = rerank_with_sasrec(sasrec_model, cand_test, sasrec_items, offsets, max_len=max_seq_len, batch_size=4096)\n\n    # Prepare submission: sample has 20 rows per user\n    print(\"Writing submission...\")\n    sample = pd.read_csv(sample_path)\n    \n    # Явно приводим маппинг к int64\n    user_orig2uid = {int(orig): int(idx) for idx, orig in enumerate(uid2orig)}\n    item_idx2orig = np.asarray(iid2orig, dtype=np.int64)  # только int64\n    \n    def ensure_int_ids(arr, name=\"\"):\n        a = np.asarray(arr)\n        if not np.issubdtype(a.dtype, np.integer):\n            raise TypeError(f\"{name}: ожидаю item_id (int), получил {a.dtype}. Пример: {a[:10]}\")\n        return a.astype(np.int64, copy=False)\n    \n    # кеш предсказаний\n    pop20 = top_pop_items(pop_scores_vec, topk=20)[0]\n    pop20_orig = item_idx2orig[pop20]\n    pred_item_cache = {}\n    \n    for u_orig in tqdm(sample[\"user_id\"].unique(), desc=\"Cache predictions\"):\n        u_orig = int(u_orig)\n        uid = user_orig2uid.get(u_orig, -1)\n        if uid == -1:\n            pred_item_cache[u_orig] = pop20_orig\n            continue\n    \n        ranked0 = preds_test.get(uid, None)\n        if ranked0 is None or len(ranked0) == 0:\n            ranked0 = cand_test.get(uid, np.array([], dtype=np.int32)).tolist()\n    \n        # дополняем популярными без повторов\n        seen = set(ranked0)\n        for it in pop_items:\n            if len(ranked0) >= 20: break\n            it = int(it)\n            if it not in seen:\n                ranked0.append(it); seen.add(it)\n        ranked0 = np.array(ranked0[:20], dtype=np.int64)\n    \n        # маппим к оригинальным id и валидируем\n        pred_item_cache[u_orig] = ensure_int_ids(item_idx2orig[ranked0], name=f\"user {u_orig}\")\n    \n    # аккуратно раскладываем по 20 строк каждого юзера\n    from collections import defaultdict\n    counters = defaultdict(int)\n    out_items = np.empty(len(sample), dtype=np.int64)\n    \n    for idx, u in tqdm(enumerate(sample[\"user_id\"].values), total=len(sample), desc=\"Fill submission\"):\n        u = int(u)\n        i = counters[u]\n        arr = pred_item_cache[u]\n        if i >= len(arr):\n            # подстраховка\n            arr = pop20_orig\n            pred_item_cache[u] = arr\n            i = 0\n        out_items[idx] = int(arr[i])\n        counters[u] += 1\n    \n    sample[\"item_id\"] = out_items.astype(np.int64)\n    # финальная гарантия типа\n    assert np.issubdtype(sample[\"item_id\"].dtype, np.integer), sample[\"item_id\"].dtype\n    sample.to_csv(submission_path, index=False)\n    print(f\"Saved submission to {submission_path}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-18T14:10:27.900888Z","iopub.execute_input":"2025-10-18T14:10:27.901649Z","iopub.status.idle":"2025-10-18T14:27:59.145613Z","shell.execute_reply.started":"2025-10-18T14:10:27.901628Z","shell.execute_reply":"2025-10-18T14:27:59.144951Z"}},"outputs":[{"name":"stdout","text":"Reading parquet with Polars...\nMapping ids to contiguous indices...\nLoading sample users...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Build val GT:   0%|          | 0/592309 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b00a378f98814f88bdc72f741aea0a19"}},"metadata":{}},{"name":"stdout","text":"Train interactions: 7,326,431, Val interactions: 1,451,544\nImplicit not installed or failed: No module named 'implicit'\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Build histories for target users:   0%|          | 0/782173 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"808606648c004fcbba8172e51973e6a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Merge candidates:   0%|          | 0/592309 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a304bdd6d2ed4d94b88888b9d2aa2d78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"SASRec epoch 1:   0%|          | 0/1145 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dfb3177d49947b79ca83d863cf1389c"}},"metadata":{}},{"name":"stdout","text":"Epoch 1 train loss: 6.9032\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Prepare sequences:   0%|          | 0/592309 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7b485af115042e8ae2d80c9acc3567d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Re-ranking:   0%|          | 0/290 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2daf3eaf8f914ef7a3595be1ca37f596"}},"metadata":{}},{"name":"stdout","text":"Epoch 1 VAL mAP@20: 0.04629\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"SASRec epoch 2:   0%|          | 0/1145 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df8f746c22a948fcb3c091e272f53341"}},"metadata":{}},{"name":"stdout","text":"Epoch 2 train loss: 6.8856\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Prepare sequences:   0%|          | 0/592309 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d37ed0892cb40fe9dc21d36ec0845b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Re-ranking:   0%|          | 0/290 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f56f5a62ff3741c8893752010ed54a3d"}},"metadata":{}},{"name":"stdout","text":"Epoch 2 VAL mAP@20: 0.04629\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Merge candidates:   0%|          | 0/293230 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebc5314668b241aaacd9eaa371c54180"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Prepare sequences:   0%|          | 0/293230 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5861dd95f23342dfbd2fbabb9c55f092"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Re-ranking:   0%|          | 0/72 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"090859f56dbf4862aabd9df3695b888e"}},"metadata":{}},{"name":"stdout","text":"Writing submission...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Cache predictions:   0%|          | 0/293230 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe0c950fe4644368b5f2a7f34ba482c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fill submission:   0%|          | 0/5864600 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aae3d15f056c44f98adf908c48a19ba3"}},"metadata":{}},{"name":"stdout","text":"Saved submission to submission1.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os, gc, math, random\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom tqdm import tqdm\nfrom collections import defaultdict, Counter\n\n# -----------------------------\n# Utils\n# -----------------------------\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n\nset_seed(42)\n\ndef average_precision_at_k(true_items_set, ranked_items, k=20):\n    if not true_items_set:\n        return 0.0\n    score = 0.0\n    hits = 0\n    for i, item in enumerate(ranked_items[:k]):\n        if item in true_items_set:\n            hits += 1\n            score += hits / (i + 1)\n    return score / min(len(true_items_set), k)\n\ndef map_at_k(gt_by_user, preds_by_user, k=20):\n    aps = []\n    for u, true_set in gt_by_user.items():\n        preds = preds_by_user.get(u, [])\n        aps.append(average_precision_at_k(true_set, preds, k=k))\n    return float(np.mean(aps)) if aps else 0.0\n\n# -----------------------------\n# Load & factorize\n# -----------------------------\ndef load_data(train_path=\"train_data.pq\", sample_path=\"sample_submission.csv\"):\n    print(\"Reading parquet with Polars...\")\n    dfpl = pl.read_parquet(train_path)\n    dfpl = dfpl.with_columns([\n        pl.col(\"user_id\").cast(pl.Int64),\n        pl.col(\"item_id\").cast(pl.Int64),\n        pl.col(\"date\").cast(pl.Int32)\n    ])\n    df = dfpl.to_pandas()\n    del dfpl; gc.collect()\n\n    print(\"Mapping ids to contiguous indices...\")\n    df[\"uid\"], user_uniques = pd.factorize(df[\"user_id\"], sort=True)\n    df[\"iid\"], item_uniques = pd.factorize(df[\"item_id\"], sort=True)\n\n    uid2orig = np.asarray(user_uniques, dtype=np.int64)\n    iid2orig = np.asarray(item_uniques, dtype=np.int64)\n\n    # Sort by user/date\n    df.sort_values([\"uid\", \"date\"], inplace=True)\n    df.reset_index(drop=True, inplace=True)\n\n    print(\"Loading sample users...\")\n    sample = pd.read_csv(sample_path)\n    sample_users = sample[\"user_id\"].unique().astype(np.int64)\n\n    # Map sample users to internal uids\n    user_orig2uid = {int(orig): int(i) for i, orig in enumerate(uid2orig)}\n    sample_uids = np.array([user_orig2uid.get(int(u), -1) for u in sample_users], dtype=np.int64)\n\n    return df, uid2orig, iid2orig, sample_users, sample_uids\n\n# -----------------------------\n# Split: last 7 days as validation holdout (for tuning)\n# -----------------------------\ndef time_split(df, holdout_days=7):\n    max_day = int(df[\"date\"].max())\n    val_start = max_day - (holdout_days - 1)\n    train_mask = df[\"date\"] < val_start\n    val_mask = ~train_mask\n    train_df = df[train_mask].copy()\n    val_df = df[val_mask].copy()\n\n    gt_val = {}\n    for uid, grp in tqdm(val_df.groupby(\"uid\", sort=False), desc=\"Build val GT\"):\n        gt_val[int(uid)] = set(grp[\"iid\"].astype(int).tolist())\n    return train_df, val_df, gt_val\n\n# -----------------------------\n# Build offsets (CSR-like)\n# -----------------------------\ndef build_user_offsets(df, n_users):\n    uids = df[\"uid\"].values.astype(np.int64)\n    iids = df[\"iid\"].values.astype(np.int64)\n    dates = df[\"date\"].values.astype(np.int32)\n\n    counts = np.bincount(uids, minlength=n_users).astype(np.int64)\n    offsets = np.zeros(n_users + 1, dtype=np.int64)\n    offsets[1:] = np.cumsum(counts)\n    return uids, iids, dates, offsets\n\n# -----------------------------\n# Popularity (global)\n# -----------------------------\ndef popularity_scores(df, recent_days=7, n_items=None):\n    max_day = int(df[\"date\"].max())\n    recent_start = max_day - (recent_days - 1)\n\n    all_cnt = df.groupby(\"iid\").size()\n    rec_cnt = df[df[\"date\"] >= recent_start].groupby(\"iid\").size()\n\n    if n_items is None:\n        n_items = int(df[\"iid\"].max()) + 1\n    all_vec = np.zeros(n_items, dtype=np.float32)\n    rec_vec = np.zeros(n_items, dtype=np.float32)\n\n    all_vec[all_cnt.index.values] = all_cnt.values.astype(np.float32)\n    rec_vec[rec_cnt.index.values] = rec_cnt.values.astype(np.float32)\n\n    all_vec /= (all_vec.max() + 1e-9)\n    rec_vec /= (rec_vec.max() + 1e-9)\n\n    pop = 0.3 * all_vec + 0.7 * rec_vec\n    return pop\n\ndef top_pop_items(pop_scores, topk=500):\n    idx = np.argsort(-pop_scores)[:topk]\n    return idx, pop_scores[idx]\n\n# -----------------------------\n# Co-visitation graph (pure counting, no ML)\n# -----------------------------\ndef build_covisit_adjacency(iids, dates, offsets,\n                            max_last_per_user=60, window=20,\n                            time_decay=0.08,  # per day\n                            symmetric=True,\n                            cap_neighbors=None):\n    \"\"\"\n    Returns: dict[item] -> dict[neighbor] -> score\n    Weights:\n      w = (1 / gap) * exp(-time_decay * (max_day - date_j))\n    Only last `max_last_per_user` interactions per user, pairs within `window`.\n    \"\"\"\n    n_users = offsets.size - 1\n    max_day = int(dates.max()) if len(dates) > 0 else 0\n    adj = defaultdict(lambda: defaultdict(float))\n\n    print(f\"Building co-vis adjacency: max_last_per_user={max_last_per_user}, window={window}\")\n    for u in tqdm(range(n_users), desc=\"Covis per user\"):\n        s, e = int(offsets[u]), int(offsets[u+1])\n        m = e - s\n        if m <= 1:\n            continue\n        start = max(s, e - max_last_per_user)\n        seq = iids[start:e]      # item ids\n        dts = dates[start:e]     # days\n        L = len(seq)\n        # iterate sequential pairs within window\n        for i in range(L):\n            item_i = int(seq[i])\n            di = int(dts[i])\n            # j after i\n            max_j = min(L, i + 1 + window)\n            for j in range(i + 1, max_j):\n                item_j = int(seq[j])\n                dj = int(dts[j])\n                gap = j - i\n                w = (1.0 / gap) * math.exp(-time_decay * (max_day - dj))\n                adj[item_i][item_j] += w\n                if symmetric:\n                    adj[item_j][item_i] += w\n\n    # Optional pruning per item: keep only top cap_neighbors\n    if cap_neighbors is not None and cap_neighbors > 0:\n        print(f\"Pruning adjacency to top-{cap_neighbors} neighbors per item...\")\n        for it in tqdm(list(adj.keys()), desc=\"Prune neighbors\"):\n            neigh = adj[it]\n            if len(neigh) > cap_neighbors:\n                # keep top by score\n                top = sorted(neigh.items(), key=lambda kv: kv[1], reverse=True)[:cap_neighbors]\n                adj[it] = defaultdict(float, top)\n\n    return adj\n\n# -----------------------------\n# Personalized repeat (no ML)\n# -----------------------------\ndef user_repeat_scores(hist_items, hist_dates, max_day, time_decay=0.08):\n    \"\"\"\n    Sum exp(-time_decay * age_in_days) per item in user's history.\n    Returns dict[item] -> score\n    \"\"\"\n    rep = defaultdict(float)\n    for it, d in zip(hist_items, hist_dates):\n        rep[int(it)] += math.exp(-time_decay * (max_day - int(d)))\n    return rep\n\n# -----------------------------\n# Recommend for a batch of users (algorithmic)\n# -----------------------------\ndef recommend_batch(users, offsets, iids, dates, adj, pop_items, pop_scores,\n                    last_k=10, topk=20,\n                    w_covis=1.0, w_seed_recency=0.85,  # seed recency downweight per step back\n                    w_repeat=0.3, w_pop=0.2,\n                    cap_candidates=600):\n    \"\"\"\n    Returns dict uid -> list of item ids (0-based), length topk\n    \"\"\"\n    results = {}\n    max_day = int(dates.max()) if len(dates) > 0 else 0\n    for u in tqdm(users, desc=\"Recommend users\"):\n        u = int(u)\n        s, e = int(offsets[u]), int(offsets[u+1])\n        if e - s == 0:\n            # cold-start: top-pop\n            cand = list(map(int, pop_items[:topk]))\n            results[u] = cand\n            continue\n\n        hist_items = iids[s:e]\n        hist_dates = dates[s:e]\n        # seeds: last_k items (most recent at end)\n        seeds = hist_items[-last_k:]\n        seed_recency_weights = []\n        for idx in range(len(seeds)):\n            # last seed gets 1.0, previous gets w_seed_recency, etc.\n            power = len(seeds) - 1 - idx\n            seed_recency_weights.append(w_seed_recency ** power)\n\n        scores = defaultdict(float)\n\n        # 1) co-vis neighbors aggregated\n        for seed, srw in zip(seeds, seed_recency_weights):\n            seed = int(seed)\n            neigh = adj.get(seed, None)\n            if not neigh:\n                continue\n            for nb, w in neigh.items():\n                scores[int(nb)] += w_covis * srw * float(w)\n\n        # 2) repeat booster (time-decayed personal popularity)\n        rep = user_repeat_scores(hist_items, hist_dates, max_day, time_decay=0.08)\n        for it, w in rep.items():\n            scores[int(it)] += w_repeat * float(w)\n\n        # 3) global pop as fallback/regularizer\n        for it, w in zip(pop_items, pop_scores):\n            scores[int(it)] += w_pop * float(w)\n\n        # collect top candidates\n        if not scores:\n            cand = list(map(int, pop_items[:topk]))\n            results[u] = cand\n        else:\n            # take top by score\n            items = np.fromiter(scores.keys(), dtype=np.int32)\n            scs = np.fromiter(scores.values(), dtype=np.float32)\n            order = np.argsort(-scs)[:max(cap_candidates, topk)]\n            ranked = items[order]\n            # Dedup and cut to topk\n            unique_ranked = []\n            seen = set()\n            for it in ranked:\n                it = int(it)\n                if it in seen:\n                    continue\n                seen.add(it)\n                unique_ranked.append(it)\n                if len(unique_ranked) >= topk:\n                    break\n            # if still short, pad with pop\n            if len(unique_ranked) < topk:\n                for it in pop_items:\n                    it = int(it)\n                    if it in seen:\n                        continue\n                    unique_ranked.append(it)\n                    seen.add(it)\n                    if len(unique_ranked) >= topk:\n                        break\n            results[u] = unique_ranked[:topk]\n    return results\n\n# -----------------------------\n# End-to-end\n# -----------------------------\ndef main(train_path=\"train_data.pq\", sample_path=\"sample_submission.csv\", submission_path=\"submission.csv\",\n         holdout_days=7,\n         max_last_per_user=60, window=20, covis_cap=300,\n         last_k=10, topk_pred=20):\n\n    # Load\n    df, uid2orig, iid2orig, sample_users_orig, sample_uids = load_data(train_path, sample_path)\n    n_users = int(df[\"uid\"].max()) + 1\n    n_items = int(df[\"iid\"].max()) + 1\n\n    # Split for evaluation/tuning\n    train_df, val_df, gt_val = time_split(df, holdout_days=holdout_days)\n    print(f\"Train interactions: {len(train_df):,}, Val interactions: {len(val_df):,}\")\n\n    # Build arrays for TRAIN\n    uids_tr, iids_tr, dates_tr, offsets_tr = build_user_offsets(train_df, n_users)\n\n    # Popularity on TRAIN\n    pop_scores_tr = popularity_scores(train_df, recent_days=7, n_items=n_items)\n    pop_items_tr, pop_scores_vals_tr = top_pop_items(pop_scores_tr, topk=500)\n\n    # Co-vis adjacency on TRAIN\n    covis_adj_tr = build_covisit_adjacency(\n        iids=iids_tr, dates=dates_tr, offsets=offsets_tr,\n        max_last_per_user=max_last_per_user, window=window,\n        time_decay=0.08, symmetric=True, cap_neighbors=covis_cap\n    )\n\n    # Recommend for VAL users (uids present in GT)\n    val_users = np.array(list(gt_val.keys()), dtype=np.int64)\n    preds_val = recommend_batch(\n        users=val_users, offsets=offsets_tr, iids=iids_tr, dates=dates_tr, adj=covis_adj_tr,\n        pop_items=pop_items_tr, pop_scores=pop_scores_vals_tr,\n        last_k=last_k, topk=topk_pred,\n        w_covis=1.0, w_seed_recency=0.85, w_repeat=0.3, w_pop=0.2,\n        cap_candidates=600\n    )\n    val_map = map_at_k(gt_val, preds_val, k=20)\n    print(f\"Validation mAP@20 (algorithmic): {val_map:.5f}\")\n\n    # -----------------------------\n    # Refit on FULL (train + last 7 days) for final predictions\n    # -----------------------------\n    print(\"Refit on FULL data (including last 7 days)...\")\n    uids_full, iids_full, dates_full, offsets_full = build_user_offsets(df, n_users)\n\n    pop_scores_full = popularity_scores(df, recent_days=7, n_items=n_items)\n    pop_items_full, pop_scores_vals_full = top_pop_items(pop_scores_full, topk=500)\n\n    covis_adj_full = build_covisit_adjacency(\n        iids=iids_full, dates=dates_full, offsets=offsets_full,\n        max_last_per_user=max_last_per_user, window=window,\n        time_decay=0.08, symmetric=True, cap_neighbors=covis_cap\n    )\n\n    # Predict for TEST users (from sample)\n    test_users = np.unique(sample_uids[sample_uids >= 0])\n    preds_test = recommend_batch(\n        users=test_users, offsets=offsets_full, iids=iids_full, dates=dates_full, adj=covis_adj_full,\n        pop_items=pop_items_full, pop_scores=pop_scores_vals_full,\n        last_k=last_k, topk=topk_pred,\n        w_covis=1.0, w_seed_recency=0.85, w_repeat=0.3, w_pop=0.2,\n        cap_candidates=600\n    )\n\n    # -----------------------------\n    # Build submission\n    # -----------------------------\n    print(\"Writing submission...\")\n    sample = pd.read_csv(sample_path)\n    user_orig2uid = {int(orig): int(i) for i, orig in enumerate(uid2orig)}\n    item_idx2orig = np.asarray(iid2orig, dtype=np.int64)\n\n    # Precompute per-user 20 items (orig ids)\n    pred_item_cache = {}\n    pop20_orig = item_idx2orig[top_pop_items(pop_scores_full, topk=20)[0]]\n\n    for u_orig in tqdm(sample[\"user_id\"].unique(), desc=\"Cache predictions\"):\n        u_orig = int(u_orig)\n        uid = user_orig2uid.get(u_orig, -1)\n        if uid == -1:\n            pred_item_cache[u_orig] = pop20_orig\n            continue\n        ranked0 = preds_test.get(uid, [])\n        if not ranked0:\n            ranked0 = list(map(int, pop_items_full[:topk_pred]))\n        ranked0 = np.asarray(ranked0[:topk_pred], dtype=np.int64)\n        pred_item_cache[u_orig] = item_idx2orig[ranked0]\n\n    # Fill submission: 20 rows per user in order\n    from collections import defaultdict\n    counters = defaultdict(int)\n    out_items = np.empty(len(sample), dtype=np.int64)\n\n    for idx, u in tqdm(enumerate(sample[\"user_id\"].values), total=len(sample), desc=\"Fill submission\"):\n        u = int(u)\n        i = counters[u]\n        arr = pred_item_cache[u]\n        if i >= len(arr):\n            arr = pop20_orig\n            pred_item_cache[u] = arr\n            i = 0\n        out_items[idx] = int(arr[i])\n        counters[u] += 1\n\n    sample[\"item_id\"] = out_items\n    # sanity: ensure integer ids\n    assert np.issubdtype(sample[\"item_id\"].dtype, np.integer)\n    sample.to_csv(submission_path, index=False)\n    print(f\"Saved submission to {submission_path}\")\n\nif __name__ == \"__main__\":\n    # Пример запуска: пути подставь свои\n    main(\n        train_path=\"/kaggle/input/stupidshit777/train_data.pq\",\n        sample_path=\"/kaggle/input/stupidshit777/sample_submission (11).csv\",\n        submission_path=\"submission.csv\",\n        holdout_days=7,\n        max_last_per_user=60,\n        window=20,\n        covis_cap=300,\n        last_k=10,\n        topk_pred=20\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T14:34:47.038161Z","iopub.execute_input":"2025-10-18T14:34:47.038468Z","iopub.status.idle":"2025-10-18T14:45:25.329355Z","shell.execute_reply.started":"2025-10-18T14:34:47.038446Z","shell.execute_reply":"2025-10-18T14:45:25.328655Z"}},"outputs":[{"name":"stdout","text":"Reading parquet with Polars...\nMapping ids to contiguous indices...\nLoading sample users...\n","output_type":"stream"},{"name":"stderr","text":"Build val GT: 100%|██████████| 592309/592309 [00:44<00:00, 13328.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train interactions: 7,326,431, Val interactions: 1,451,544\nBuilding co-vis adjacency: max_last_per_user=60, window=20\n","output_type":"stream"},{"name":"stderr","text":"Covis per user: 100%|██████████| 2682603/2682603 [00:48<00:00, 55529.50it/s] \n","output_type":"stream"},{"name":"stdout","text":"Pruning adjacency to top-300 neighbors per item...\n","output_type":"stream"},{"name":"stderr","text":"Prune neighbors: 100%|██████████| 620175/620175 [00:06<00:00, 89229.58it/s] \nRecommend users: 100%|██████████| 592309/592309 [03:13<00:00, 3053.18it/s] \n","output_type":"stream"},{"name":"stdout","text":"Validation mAP@20 (algorithmic): 0.05538\nRefit on FULL data (including last 7 days)...\nBuilding co-vis adjacency: max_last_per_user=60, window=20\n","output_type":"stream"},{"name":"stderr","text":"Covis per user: 100%|██████████| 2682603/2682603 [00:59<00:00, 44964.31it/s] \n","output_type":"stream"},{"name":"stdout","text":"Pruning adjacency to top-300 neighbors per item...\n","output_type":"stream"},{"name":"stderr","text":"Prune neighbors: 100%|██████████| 700420/700420 [00:09<00:00, 76351.91it/s] \nRecommend users: 100%|██████████| 293230/293230 [04:07<00:00, 1185.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Writing submission...\n","output_type":"stream"},{"name":"stderr","text":"Cache predictions: 100%|██████████| 293230/293230 [00:01<00:00, 204217.93it/s]\nFill submission: 100%|██████████| 5864600/5864600 [00:04<00:00, 1317464.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved submission to submission.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom collections import defaultdict\nimport warnings\nfrom tqdm import tqdm\n\nwarnings.filterwarnings('ignore')\n\nclass TShoppingRecommender:\n    def __init__(self, recent_days_window=3):\n        self.user_recent_clicks = defaultdict(list)\n        self.global_popularity = defaultdict(int)\n        self.last_day = None\n        self.recent_days_window = recent_days_window\n        \n    def fit(self, train_data):\n        \"\"\"Обучение модели на исторических данных\"\"\"\n        print(\"Обработка тренировочных данных...\")\n        \n        # Находим последний день в данных\n        self.last_day = train_data['date'].max()\n        print(f\"Последний день в данных: {self.last_day}\")\n        print(f\"Используется окно из {self.recent_days_window} последних дней\")\n        \n        # Собираем клики пользователей за последние N дней\n        recent_days = [self.last_day - i for i in range(self.recent_days_window)]\n        recent_data = train_data[train_data['date'].isin(recent_days)]\n        \n        print(f\"Найдено взаимодействий за последние {self.recent_days_window} дней: {len(recent_data)}\")\n        \n        print(\"Сбор персональных предпочтений...\")\n        # Используем tqdm для прогресса\n        for _, row in tqdm(recent_data.iterrows(), total=len(recent_data), desc=\"Обработка кликов\"):\n            user_id = row['user_id']\n            item_id = row['item_id']\n            date = row['date']\n            \n            # Сохраняем клики с учетом даты (более поздние имеют больший вес)\n            self.user_recent_clicks[user_id].append((item_id, date))\n        \n        print(\"Сортировка кликов по дате...\")\n        # Сортируем клики каждого пользователя по дате (сначала последние)\n        for user_id in tqdm(self.user_recent_clicks.keys(), desc=\"Сортировка пользователей\"):\n            self.user_recent_clicks[user_id].sort(key=lambda x: x[1], reverse=True)\n        \n        # Собираем глобальную популярность товаров за последний день\n        last_day_data = train_data[train_data['date'] == self.last_day]\n        item_counts = last_day_data['item_id'].value_counts()\n        \n        print(\"Расчет глобальной популярности...\")\n        for item_id, count in tqdm(item_counts.items(), total=len(item_counts), desc=\"Популярные товары\"):\n            self.global_popularity[item_id] = count\n            \n        print(f\"Обработано пользователей: {len(self.user_recent_clicks)}\")\n        print(f\"Уникальных популярных товаров: {len(self.global_popularity)}\")\n    \n    def predict(self, user_ids, top_k=20):\n        \"\"\"Предсказание топ-K товаров для каждого пользователя\"\"\"\n        print(\"Генерация рекомендаций...\")\n        \n        # Создаем глобальный рейтинг популярности\n        global_ranking = list(self.global_popularity.keys())\n        \n        submissions = []\n        \n        # Используем tqdm для прогресса предсказания\n        for user_id in tqdm(user_ids, desc=\"Генерация рекомендаций\"):\n            user_recommendations = []\n            \n            # Берем уникальные товары из последних кликов пользователя\n            if user_id in self.user_recent_clicks:\n                recent_items = []\n                for item_id, date in self.user_recent_clicks[user_id]:\n                    if item_id not in recent_items:\n                        recent_items.append(item_id)\n                    if len(recent_items) >= top_k:\n                        break\n                \n                user_recommendations.extend(recent_items)\n            \n            # Если недостаточно персональных рекомендаций, добавляем глобально популярные\n            if len(user_recommendations) < top_k:\n                # Берем только те товары, которых еще нет в рекомендациях\n                additional_items = [item for item in global_ranking \n                                  if item not in user_recommendations]\n                \n                # Добавляем столько, сколько нужно до top_k\n                needed = top_k - len(user_recommendations)\n                user_recommendations.extend(additional_items[:needed])\n            \n            # Обеспечиваем, что рекомендаций ровно top_k\n            user_recommendations = user_recommendations[:top_k]\n            \n            # Добавляем в submission\n            for item_id in user_recommendations:\n                submissions.append({\n                    'user_id': user_id,\n                    'item_id': item_id\n                })\n        \n        return pd.DataFrame(submissions)\n\ndef main():\n    # Загрузка данных\n    print(\"Загрузка данных...\")\n    train_data = pd.read_parquet('/kaggle/input/stupidshit777/train_data.pq')\n    \n    # Загрузка sample submission для получения списка пользователей\n    sample_submission = pd.read_csv('/kaggle/input/stupidshit777/sample_submission (11).csv')\n    test_users = sample_submission['user_id'].unique()\n    \n    print(f\"Всего пользователей в тесте: {len(test_users)}\")\n    print(f\"Всего взаимодействий в тренировочных данных: {len(train_data)}\")\n    print(f\"Уникальных пользователей: {train_data['user_id'].nunique()}\")\n    print(f\"Уникальных товаров: {train_data['item_id'].nunique()}\")\n    \n    # Настройка параметров\n    RECENT_DAYS_WINDOW = 17  # Можно изменить на 3, 5, 10 и т.д.\n    \n    # Обучение модели\n    model = TShoppingRecommender(recent_days_window=RECENT_DAYS_WINDOW)\n    model.fit(train_data)\n    \n    # Предсказание\n    submission_df = model.predict(test_users, top_k=20)\n    \n    # Проверка формата\n    print(f\"\\nПроверка формата submission:\")\n    print(f\"Всего строк в submission: {len(submission_df)}\")\n    print(f\"Ожидалось: {len(test_users) * 20}\")\n    \n    # Сохранение результатов\n    submission_df.to_csv('submission.csv', index=False)\n    print(\"\\nSubmission сохранен в submission.csv\")\n    \n    # Пример рекомендаций для первых 5 пользователей\n    print(\"\\nПример рекомендаций для первых 5 пользователей:\")\n    for i, user_id in enumerate(test_users[:5]):\n        user_recs = submission_df[submission_df['user_id'] == user_id]['item_id'].tolist()\n        print(f\"Пользователь {user_id}: {user_recs[:5]}... (всего {len(user_recs)} рекомендаций)\")\n    \n    # Статистика по рекомендациям\n    print(f\"\\nСтатистика:\")\n    print(f\"Использовано дней для персональных предпочтений: {RECENT_DAYS_WINDOW}\")\n    print(f\"Размер глобального пула популярных товаров: {len(model.global_popularity)}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T15:25:06.166948Z","iopub.execute_input":"2025-10-18T15:25:06.167233Z","execution_failed":"2025-10-18T16:35:05.814Z"}},"outputs":[{"name":"stdout","text":"Reading parquet with Polars...\nMapping ids to contiguous indices...\nLoading sample users...\n","output_type":"stream"},{"name":"stderr","text":"Build val GT: 100%|██████████| 592309/592309 [00:45<00:00, 13076.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train interactions: 7,326,431, Val interactions: 1,451,544\nBuilding co-vis graph: last=100, short_w=8, long_w=25\n","output_type":"stream"},{"name":"stderr","text":"Covis per user: 100%|██████████| 2682603/2682603 [01:27<00:00, 30510.86it/s] \nPrune/sort neighbors: 100%|██████████| 634257/634257 [00:23<00:00, 26516.17it/s] \nPrune/sort neighbors: 100%|██████████| 600257/600257 [00:16<00:00, 35755.27it/s] \nRecommend users: 100%|██████████| 592309/592309 [1:03:47<00:00, 154.74it/s] \n","output_type":"stream"},{"name":"stdout","text":"Validation mAP@20 (algorithmic): 0.04337\nRefit on FULL data (including last 7 days)...\nBuilding co-vis graph: last=100, short_w=8, long_w=25\n","output_type":"stream"},{"name":"stderr","text":"Covis per user: 100%|██████████| 2682603/2682603 [01:48<00:00, 24667.76it/s] \nPrune/sort neighbors: 100%|██████████| 717614/717614 [00:32<00:00, 22353.08it/s] \nPrune/sort neighbors: 100%|██████████| 681615/681615 [00:19<00:00, 35870.99it/s] \nRecommend users:   0%|          | 187/293230 [00:05<2:19:51, 34.92it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"4","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-18T14:08:24.357Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------\n# Global accelerators\n# -----------------------------\nimport os\nos.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n\nimport torch\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\ntry:\n    from torch.backends.cuda import sdp_kernel\n    sdp_kernel.enable_flash_sdp(True)\n    sdp_kernel.enable_mem_efficient_sdp(True)\n    sdp_kernel.enable_math_sdp(False)\nexcept Exception:\n    pass\nif hasattr(torch, \"set_float32_matmul_precision\"):\n    torch.set_float32_matmul_precision(\"high\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------\n# RP3beta Retrieval (implicit CPU)\n# -----------------------------\ndef train_rp3beta(user_item_csr, K=200, alpha=0.4, beta=0.7):\n    try:\n        from implicit.nearest_neighbours import RP3betaRecommender\n        from implicit.nearest_neighbours import bm25_weight\n    except Exception as e:\n        print(f\"[RP3] implicit недоступен: {e}. Пропускаем RP3beta.\")\n        return None, user_item_csr\n\n    try:\n        ui = user_item_csr.tocsr().astype(np.float32, copy=False)\n        ui_w = bm25_weight(ui, K1=1.2, B=0.75).tocsr()\n        model = RP3betaRecommender(K=K, alpha=alpha, beta=beta)\n        print(\"Training RP3beta (CPU)...\")\n        model.fit(ui_w.T, show_progress=True)\n        return model, ui_w\n    except Exception as e:\n        print(f\"[RP3] ошибка обучения: {e}. Пропускаем RP3beta.\")\n        return None, user_item_csr\n\ndef rp3_recommend_batch(model, user_item_csr, user_ids, N=300, filter_seen=False, batch_size=4096):\n    recs = {}\n    if model is None:\n        return recs\n    for i in tqdm(range(0, len(user_ids), batch_size), desc=\"RP3 recommend\"):\n        batch = user_ids[i: i+batch_size]\n        for u in batch:\n            if u < 0:\n                recs[u] = (np.empty((0,), dtype=np.int32), np.empty((0,), dtype=np.float32))\n                continue\n            try:\n                ids, scores = model.recommend(\n                    userid=int(u),\n                    user_items=user_item_csr,\n                    N=N,\n                    recalculate_user=True,\n                    filter_already_liked_items=filter_seen\n                )\n                recs[u] = (ids.astype(np.int32), scores.astype(np.float32))\n            except Exception:\n                recs[u] = (np.empty((0,), dtype=np.int32), np.empty((0,), dtype=np.float32))\n    return recs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------\n# Utils\n# -----------------------------\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef average_precision_at_k(true_items_set, ranked_items, k=20):\n    if not true_items_set:\n        return 0.0\n    score = 0.0\n    hits = 0\n    for i, item in enumerate(ranked_items[:k]):\n        if item in true_items_set:\n            hits += 1\n            score += hits / (i + 1)\n    return score / min(len(true_items_set), k)\n\ndef map_at_k(gt_by_user, preds_by_user, k=20):\n    aps = []\n    for u, true_set in gt_by_user.items():\n        preds = preds_by_user.get(u, [])\n        aps.append(average_precision_at_k(true_set, preds, k=k))\n    return float(np.mean(aps)) if aps else 0.0\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T12:38:29.832966Z","iopub.execute_input":"2025-10-18T12:38:29.833441Z","iopub.status.idle":"2025-10-18T12:38:29.837009Z","shell.execute_reply.started":"2025-10-18T12:38:29.833420Z","shell.execute_reply":"2025-10-18T12:38:29.836466Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------\n# Load data\n# -----------------------------\ndef load_data(train_path=\"train_data.pq\", sample_path=\"sample_submission.csv\"):\n    print(\"Reading parquet with Polars...\")\n    dfpl = pl.read_parquet(train_path)\n    # Ensure dtypes\n    dfpl = dfpl.with_columns([\n        pl.col(\"user_id\").cast(pl.Int64),\n        pl.col(\"item_id\").cast(pl.Int64),\n        pl.col(\"date\").cast(pl.Int32)\n    ])\n    df = dfpl.to_pandas()\n    del dfpl\n    gc.collect()\n\n    print(\"Mapping ids to contiguous indices...\")\n    df[\"uid\"], user_uniques = pd.factorize(df[\"user_id\"], sort=True)\n    df[\"iid\"], item_uniques = pd.factorize(df[\"item_id\"], sort=True)\n\n    uid2orig = np.array(user_uniques)\n    iid2orig = np.array(item_uniques)\n\n    # Sort by user, then by date for sequence models\n    df.sort_values([\"uid\", \"date\"], inplace=True)\n    df.reset_index(drop=True, inplace=True)\n\n    print(\"Loading sample users...\")\n    sample = pd.read_csv(sample_path)\n    # Sample repeats user_id 20 times; we only need unique users\n    sample_users = sample[\"user_id\"].unique()\n    # Map sample users to internal uids\n    # Build map from original user_id to uid\n    user_orig2uid = {orig: idx for idx, orig in enumerate(uid2orig)}\n    sample_uids = []\n    for u in sample_users:\n        if u in user_orig2uid:\n            sample_uids.append(user_orig2uid[u])\n        else:\n            # shouldn't happen; but keep -1 to handle later\n            sample_uids.append(-1)\n    sample_uids = np.array(sample_uids, dtype=np.int64)\n\n    return df, uid2orig, iid2orig, sample_users, sample_uids\n\n# -----------------------------\n# Split: last 7 days as validation holdout\n# -----------------------------\ndef time_split(df, holdout_days=7):\n    max_day = int(df[\"date\"].max())\n    val_start = max_day - (holdout_days - 1)\n    train_mask = df[\"date\"] < val_start\n    val_mask = df[\"date\"] >= val_start\n\n    train_df = df[train_mask].copy()\n    val_df = df[val_mask].copy()\n\n    # Ground truth for validation: set of items per user in holdout period\n    gt_val = {}\n    for uid, grp in tqdm(val_df.groupby(\"uid\", sort=False), desc=\"Build val GT\"):\n        gt_val[uid] = set(grp[\"iid\"].tolist())\n\n    return train_df, val_df, gt_val\n\n# -----------------------------\n# Build CSR matrix for implicit ALS\n# -----------------------------\ndef build_user_item_csr(df, n_users, n_items):\n    rows = df[\"uid\"].values.astype(np.int32)\n    cols = df[\"iid\"].values.astype(np.int32)\n    data = np.ones_like(rows, dtype=np.float32)\n    ui = sp.coo_matrix((data, (rows, cols)), shape=(n_users, n_items), dtype=np.float32).tocsr()\n    return ui\n\n# -----------------------------\n# ALS Retrieval\n# -----------------------------\ndef train_als(user_item_csr, use_gpu=True, factors=128, reg=1e-4, iters=20):\n    try:\n        import implicit\n        from implicit.nearest_neighbours import bm25_weight\n        ui = bm25_weight(user_item_csr, K1=1.2, B=0.75).tocsr()\n        gc.collect()\n\n        if use_gpu:\n            try:\n                from implicit.gpu.als import AlternatingLeastSquares as GPU_ALS\n                als = GPU_ALS(factors=factors, regularization=reg, iterations=iters, random_state=42)\n                print(\"Training ALS (GPU)...\")\n                als.fit(ui.T, show_progress=True)\n                return als, ui\n            except Exception as e:\n                print(f\"GPU ALS not available ({e}), falling back to CPU.\")\n        from implicit.als import AlternatingLeastSquares as CPU_ALS\n        als = CPU_ALS(factors=factors, regularization=reg, iterations=iters, use_cg=True, random_state=42)\n        print(\"Training ALS (CPU)...\")\n        als.fit(ui.T, show_progress=True)\n        return als, ui\n    except Exception as e:\n        print(f\"Implicit not installed or failed: {e}\")\n        return None, user_item_csr\n\ndef als_recommend_batch(model, user_item_csr, user_ids, N=300, filter_seen=False, batch_size=1024):\n    recs = {}\n    desc = \"ALS recommend\"\n    for i in tqdm(range(0, len(user_ids), batch_size), desc=desc):\n        batch = user_ids[i: i+batch_size]\n        for u in batch:\n            if u < 0:\n                recs[u] = (np.empty((0,), dtype=np.int32), np.empty((0,), dtype=np.float32))\n                continue\n            try:\n                ids, scores = model.recommend(\n                    userid=int(u),\n                    user_items=user_item_csr,\n                    N=N,\n                    recalculate_user=True,\n                    filter_already_liked_items=filter_seen\n                )\n                recs[u] = (ids.astype(np.int32), scores.astype(np.float32))\n            except Exception:\n                recs[u] = (np.empty((0,), dtype=np.int32), np.empty((0,), dtype=np.float32))\n    return recs\n\n# -----------------------------\n# FAISS index for item similarity (ALS item factors)\n# -----------------------------\ndef build_faiss_index(item_factors):\n    try:\n        import faiss\n    except Exception as e:\n        print(f\"FAISS not available: {e}\")\n        return None, None\n\n    d = item_factors.shape[1]\n    try:\n        res = faiss.StandardGpuResources() if torch.cuda.is_available() else None\n    except Exception:\n        res = None\n\n    index = faiss.IndexFlatIP(d)\n    if res is not None:\n        index = faiss.index_cpu_to_gpu(res, 0, index)\n    # Optionally normalize for cosine similarity; but ALS uses IP\n    # faiss.normalize_L2(item_factors)\n    index.add(item_factors.astype(np.float32))\n    return index, res\n\ndef similar_items_for_users(index, item_factors, user_histories, topk_per_item=50, last_k=10):\n    \"\"\"\n    user_histories: dict uid -> np.array of recent item ids (0-based)\n    returns dict uid -> (item_ids, scores) aggregated over last_k items\n    \"\"\"\n    try:\n        import faiss\n    except Exception:\n        index = None\n\n    recs = {}\n    if index is None:\n        return recs\n\n    for uid, hist in tqdm(user_histories.items(), desc=\"FAISS item2item\"):\n        if hist.size == 0:\n            recs[uid] = (np.empty((0,), dtype=np.int32), np.empty((0,), dtype=np.float32))\n            continue\n        queries = hist[-last_k:]\n        # Get neighbors for each query\n        # Note: include self in results; we will drop identical ids\n        distances, neighbors = index.search(item_factors[queries].astype(np.float32), k=topk_per_item+1)\n        # Aggregate by sum of scores, exclude original items\n        agg = defaultdict(float)\n        hist_set = set(queries.tolist())\n        for row in range(neighbors.shape[0]):\n            for j in range(neighbors.shape[1]):\n                it = int(neighbors[row, j])\n                if it in hist_set:\n                    continue\n                agg[it] += float(distances[row, j])\n        if not agg:\n            recs[uid] = (np.empty((0,), dtype=np.int32), np.empty((0,), dtype=np.float32))\n        else:\n            items = np.fromiter(agg.keys(), dtype=np.int32)\n            scores = np.fromiter(agg.values(), dtype=np.float32)\n            order = np.argsort(-scores)\n            recs[uid] = (items[order], scores[order])\n    return recs\n\n# -----------------------------\n# Popularity candidates\n# -----------------------------\ndef popularity_scores(df_train, df_recent_days=7, n_items=None):\n    max_day = int(df_train[\"date\"].max())\n    recent_start = max_day - (df_recent_days - 1)\n\n    all_cnt = df_train.groupby(\"iid\").size()\n    rec_cnt = df_train[df_train[\"date\"] >= recent_start].groupby(\"iid\").size()\n\n    # Align\n    if n_items is None:\n        n_items = int(df_train[\"iid\"].max()) + 1\n    all_vec = np.zeros(n_items, dtype=np.float32)\n    rec_vec = np.zeros(n_items, dtype=np.float32)\n\n    all_vec[all_cnt.index.values] = all_cnt.values.astype(np.float32)\n    rec_vec[rec_cnt.index.values] = rec_cnt.values.astype(np.float32)\n\n    # Normalize\n    all_vec /= (all_vec.max() + 1e-9)\n    rec_vec /= (rec_vec.max() + 1e-9)\n\n    # Weighted blend (tuneable)\n    pop = 0.3 * all_vec + 0.7 * rec_vec\n    return pop\n\ndef top_pop_items(pop_scores, topk=500):\n    idx = np.argsort(-pop_scores)[:topk]\n    return idx, pop_scores[idx]\n\n# -----------------------------\n# Build user sequences (offsets) for SASRec\n# -----------------------------\ndef build_user_offsets(df, n_users):\n    # df must be sorted by [\"uid\", \"date\"]\n    uids = df[\"uid\"].values.astype(np.int64)\n    iids = df[\"iid\"].values.astype(np.int64)\n    # Keep 0-based for ALS; SASRec will use +1 for padding\n    counts = np.bincount(uids, minlength=n_users).astype(np.int64)\n    offsets = np.zeros(n_users + 1, dtype=np.int64)\n    offsets[1:] = np.cumsum(counts)\n    # items array already in user-sorted order\n    return uids, iids, offsets\n\ndef get_user_last_items(iids, offsets, uids):\n    user_hist = {}\n    for u in tqdm(np.unique(uids), desc=\"Collect last items per user\"):\n        s, e = offsets[u], offsets[u+1]\n        if e - s > 0:\n            user_hist[u] = iids[s:e]\n        else:\n            user_hist[u] = np.array([], dtype=np.int64)\n    return user_hist\n\n# -----------------------------\n# SASRec model\n# -----------------------------\n# -----------------------------\n# SASRec (norm_first + SDPA-friendly)\n# -----------------------------\nclass SASRec(nn.Module):\n    def __init__(self, n_items_padded, max_len=50, d_model=192, n_heads=4, n_layers=3, dropout=0.2):\n        super().__init__()\n        self.n_items = n_items_padded\n        self.max_len = max_len\n        self.item_emb = nn.Embedding(n_items_padded, d_model, padding_idx=0)\n        self.pos_emb = nn.Embedding(max_len, d_model)\n        layer = nn.TransformerEncoderLayer(\n            d_model=d_model, nhead=n_heads, dim_feedforward=d_model*4,\n            dropout=dropout, batch_first=True, norm_first=True\n        )\n        self.encoder = nn.TransformerEncoder(layer, num_layers=n_layers, norm=nn.LayerNorm(d_model))\n        self.dropout = nn.Dropout(dropout)\n        nn.init.normal_(self.item_emb.weight, mean=0.0, std=0.02)\n        nn.init.normal_(self.pos_emb.weight, mean=0.0, std=0.02)\n\n    def forward(self, seq_ids):\n        B, L = seq_ids.size()\n        positions = torch.arange(L, device=seq_ids.device).unsqueeze(0).expand(B, L)\n        x = self.item_emb(seq_ids) + self.pos_emb(positions)\n        x = self.dropout(x)\n        causal = torch.triu(torch.ones(L, L, device=seq_ids.device), diagonal=1).bool()\n        key_pad = (seq_ids == 0)\n        x = self.encoder(x, mask=causal, src_key_padding_mask=key_pad)\n        lengths = (~key_pad).sum(dim=1).clamp(min=1)\n        idx = (lengths - 1).unsqueeze(1).unsqueeze(2).expand(-1, 1, x.size(-1))\n        out = x.gather(1, idx).squeeze(1)\n        return out\n\n    def item_embedding(self, item_ids):\n        return self.item_emb(item_ids)\n\n# -----------------------------\n# Train SASRec with AMP + compile + cosine warmup + early stop\n# -----------------------------\ndef train_sasrec(model, dataset, epochs=3, batch_size=1024, lr=3e-4, val_eval_fn=None, amp_dtype=None,\n                 grad_clip=1.0, patience=2, save_path=\"sasrec_best.pt\"):\n    model = model.to(DEVICE)\n    if amp_dtype is None:\n        amp_dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float16\n    try:\n        model = torch.compile(model, mode=\"max-autotune\")\n    except Exception:\n        pass\n\n    optim = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.05)\n    scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == \"cuda\"))\n\n    steps_per_epoch = math.ceil(len(dataset) / batch_size)\n    total_steps = steps_per_epoch * epochs\n    warmup = max(1, int(0.1 * total_steps))\n\n    def lr_lambda(step):\n        if step < warmup:\n            return float(step + 1) / float(warmup)\n        progress = (step - warmup) / max(1, (total_steps - warmup))\n        return 0.5 * (1.0 + math.cos(math.pi * progress))\n\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=lr_lambda)\n    best_map, bad_epochs = -1.0, 0\n\n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2,\n                        collate_fn=collate_batch, pin_memory=True, persistent_workers=True)\n\n    for epoch in range(1, epochs + 1):\n        dataset.shuffle_users()\n        model.train()\n        pbar = tqdm(loader, total=steps_per_epoch, desc=f\"SASRec epoch {epoch}\")\n        losses = []\n        for step, (seqs, targets) in enumerate(pbar, start=1):\n            seqs = seqs.to(DEVICE, non_blocking=True)\n            targets = targets.to(DEVICE, non_blocking=True)\n            with torch.cuda.amp.autocast(enabled=(DEVICE.type==\"cuda\"), dtype=amp_dtype):\n                user_vec = model(seqs)\n                pos_embs = model.item_embedding(targets)\n                logits = user_vec @ pos_embs.T\n                # явная диагональная маска (чистые in-batch negatives)\n                logits = logits - torch.eye(logits.size(0), device=logits.device) * 1e9\n                labels = torch.arange(seqs.size(0), device=DEVICE)\n                loss = loss_fn(logits, labels)\n            optim.zero_grad(set_to_none=True)\n            scaler.scale(loss).backward()\n            if grad_clip and grad_clip > 0:\n                scaler.unscale_(optim)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(optim)\n            scaler.update()\n            scheduler.step()\n            losses.append(loss.item())\n            if len(losses) % 20 == 0:\n                pbar.set_postfix(loss=np.mean(losses[-20:]), lr=optim.param_groups[0][\"lr\"])\n        print(f\"Epoch {epoch} train loss: {np.mean(losses):.4f}\")\n\n        if val_eval_fn is not None:\n            with torch.no_grad(), torch.cuda.amp.autocast(enabled=(DEVICE.type==\"cuda\"), dtype=amp_dtype):\n                val_map = val_eval_fn(model)\n            print(f\"Epoch {epoch} VAL mAP@20: {val_map:.5f}\")\n            if val_map > best_map + 1e-5:\n                best_map, bad_epochs = val_map, 0\n                try:\n                    torch.save(model.state_dict(), save_path)\n                except Exception:\n                    pass\n            else:\n                bad_epochs += 1\n                if bad_epochs >= patience:\n                    print(f\"Early stop on epoch {epoch}, best mAP@20 = {best_map:.5f}\")\n                    break\n\n@torch.no_grad()\ndef rerank_with_sasrec(model, user_candidates, items_by_user, offsets, max_len=50, batch_size=2048, amp_dtype=None):\n    model.eval()\n    if amp_dtype is None:\n        amp_dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float16\n\n    uids = list(user_candidates.keys())\n    results = {}\n\n    # кэш последовательностей\n    seq_cache = {}\n    for u in tqdm(uids, desc=\"Prepare sequences\"):\n        s, e = offsets[u], offsets[u+1]\n        hist = items_by_user[s:e]\n        if hist.size == 0:\n            seq_cache[u] = torch.zeros((max_len,), dtype=torch.long)\n        else:\n            seq = hist[-max_len:] + 1\n            pad = max_len - len(seq)\n            if pad > 0:\n                seq = np.pad(seq, (pad, 0), constant_values=0)\n            seq_cache[u] = torch.from_numpy(seq.astype(np.int64, copy=False))\n\n    for i in tqdm(range(0, len(uids), batch_size), desc=\"Re-ranking\"):\n        batch_u = uids[i: i+batch_size]\n        seq_batch = torch.stack([seq_cache[u] for u in batch_u], dim=0).to(DEVICE, non_blocking=True)\n        cmax = max((len(user_candidates[u]) for u in batch_u), default=0)\n        if cmax == 0:\n            for u in batch_u:\n                results[u] = []\n            continue\n\n        cand_mat = np.zeros((len(batch_u), cmax), dtype=np.int64)\n        cand_lens = np.zeros((len(batch_u),), dtype=np.int32)\n        for bi, u in enumerate(batch_u):\n            arr = user_candidates[u]\n            cand_lens[bi] = len(arr)\n            cand_mat[bi, :len(arr)] = arr + 1  # к padded space\n\n        cand_t = torch.from_numpy(cand_mat).to(DEVICE, non_blocking=True)\n\n        with torch.cuda.amp.autocast(enabled=(DEVICE.type==\"cuda\"), dtype=amp_dtype):\n            user_vecs = model(seq_batch)                  # (B, D)\n            item_embs = model.item_embedding(cand_t)      # (B, Cmax, D)\n            logits = (item_embs * user_vecs.unsqueeze(1)).sum(dim=-1)  # (B, Cmax)\n            logits = logits.masked_fill(cand_t.eq(0), float(\"-inf\"))\n            topk = min(500, cmax)\n            _, idxs = torch.topk(logits, k=topk, dim=1)\n\n        for bi, u in enumerate(batch_u):\n            valid = cand_lens[bi]\n            if valid == 0:\n                results[u] = []\n                continue\n            order = idxs[bi, :min(topk, valid)].detach().cpu().numpy()\n            ranked = cand_mat[bi][order] - 1\n            results[u] = ranked.tolist()\n    return results\n\n# -----------------------------\n# SASRec dataset\n# -----------------------------\nclass SequenceDataset(Dataset):\n    def __init__(self, items_by_user, offsets, max_len=50, users=None):\n        \"\"\"\n        items_by_user: 1D np.array of item ids (0-based), sorted by user/date\n        offsets: np.array of size n_users+1\n        \"\"\"\n        self.items = items_by_user.astype(np.int64)  # 0-based\n        self.offsets = offsets.astype(np.int64)\n        self.max_len = max_len\n        self.n_users = offsets.size - 1\n\n        if users is None:\n            # Only users with at least 2 interactions\n            all_users = np.arange(self.n_users, dtype=np.int64)\n            lens = self.offsets[1:] - self.offsets[:-1]\n            self.users = all_users[lens >= 2]\n        else:\n            self.users = np.array(users, dtype=np.int64)\n\n        # Shuffle initial order\n        self.shuffle_users()\n\n    def shuffle_users(self):\n        np.random.shuffle(self.users)\n\n    def __len__(self):\n        return len(self.users)\n\n    def __getitem__(self, idx):\n        u = self.users[idx]\n        s, e = self.offsets[u], self.offsets[u+1]\n        seq = self.items[s:e]  # 0-based\n        L = len(seq)\n        # Sample a cut point: prefix -> next item\n        # t in [1..L-1], target = seq[t]\n        t = np.random.randint(1, L)\n        prefix = seq[:t]\n        target = seq[t]\n\n        # Truncate to last max_len of prefix\n        if len(prefix) > self.max_len:\n            prefix = prefix[-self.max_len:]\n        # Convert to padded space: add 1\n        prefix_padded = prefix + 1  # 1..n_items\n        # Left-pad with zeros\n        pad_len = self.max_len - len(prefix_padded)\n        if pad_len > 0:\n            prefix_padded = np.pad(prefix_padded, (pad_len, 0), constant_values=0)\n\n        target_padded = int(target + 1)\n        return prefix_padded.astype(np.int64), target_padded\n\ndef collate_batch(batch):\n    seqs_np = np.stack([b[0] for b in batch], axis=0).astype(np.int64, copy=False)\n    seqs = torch.from_numpy(seqs_np)\n    targets = torch.tensor([b[1] for b in batch], dtype=torch.long)\n    return seqs, targets\n\n# -----------------------------\n# Train SASRec with in-batch negatives\n# -----------------------------\n\n\n# -----------------------------\n# Candidate merger and reranking\n# -----------------------------\ndef merge_candidates(als_recs, i2i_recs, pop_items, pop_scores, users, topk_merge=500,\n                     w_als=1.0, w_i2i=1.0, w_pop=0.3, rp3_recs=None, w_rp3=1.0):\n    merged = {}\n    for u in tqdm(users, desc=\"Merge candidates\"):\n        scores = defaultdict(float)\n        if als_recs is not None and u in als_recs:\n            ids, sc = als_recs[u]\n            for iid, s in zip(ids, sc):\n                scores[int(iid)] += w_als * float(s)\n        if rp3_recs is not None and u in rp3_recs:\n            ids, sc = rp3_recs[u]\n            for iid, s in zip(ids, sc):\n                scores[int(iid)] += w_rp3 * float(s)\n        if i2i_recs is not None and u in i2i_recs:\n            ids, sc = i2i_recs[u]\n            for iid, s in zip(ids, sc):\n                scores[int(iid)] += w_i2i * float(s)\n        if pop_items is not None:\n            for iid, s in zip(pop_items, pop_scores):\n                scores[int(iid)] += w_pop * float(s)\n\n        if not scores:\n            merged[u] = np.array([], dtype=np.int32)\n        else:\n            items = np.fromiter(scores.keys(), dtype=np.int32)\n            scs = np.fromiter(scores.values(), dtype=np.float32)\n            order = np.argsort(-scs)[:topk_merge]\n            merged[u] = items[order]\n    return merged\n\n\n# -----------------------------\n# End-to-end\n# -----------------------------\ndef main(train_path=\"/kaggle/input/stupidshit777/train_data.pq\", sample_path=\"/kaggle/input/stupidshit777/sample_submission (11).csv\", submission_path=\"submission1.csv\",\n         holdout_days=7, max_seq_len=50):\n    # Load\n    df, uid2orig, iid2orig, sample_users_orig, sample_uids = load_data(train_path, sample_path)\n    n_users = int(df[\"uid\"].max()) + 1\n    n_items = int(df[\"iid\"].max()) + 1\n\n    # Split (для тюнинга)\n    train_df, val_df, gt_val = time_split(df, holdout_days=holdout_days)\n    print(f\"Train interactions: {len(train_df):,}, Val interactions: {len(val_df):,}\")\n\n    # Stage A: Обучение на первых 47 днях + валидация на последних 7\n    # CSR/ALS для train\n    user_item_csr = build_user_item_csr(train_df, n_users, n_items)\n    als_model, weighted_ui = train_als(user_item_csr, use_gpu=False, factors=128, reg=1e-4, iters=20)\n\n    als_item_factors, als_user_factors = None, None\n    if als_model is not None:\n        als_item_factors = getattr(als_model, \"item_factors\", getattr(als_model, \"item_factors_\", None))\n        als_user_factors = getattr(als_model, \"user_factors\", getattr(als_model, \"user_factors_\", None))\n\n    faiss_index, _ = build_faiss_index(als_item_factors) if als_item_factors is not None else (None, None)\n\n    # RP3beta (train)\n    rp3_model, rp3_ui = train_rp3beta(user_item_csr, K=200, alpha=0.4, beta=0.7)\n\n    # Sequences (train)\n    uids_sorted, iids_sorted, offsets = build_user_offsets(train_df, n_users)\n\n    # Истории целевых юзеров (val + test) на train-отрезке\n    users_for_hist = np.unique(np.concatenate([np.array(list(gt_val.keys()), dtype=np.int64),\n                                               sample_uids[sample_uids >= 0]]))\n    user_histories = {}\n    for u in tqdm(users_for_hist, desc=\"Build histories (train)\"):\n        s, e = offsets[u], offsets[u+1]\n        user_histories[u] = iids_sorted[s:e]\n\n    # Популярность (train)\n    pop_scores_vec = popularity_scores(train_df, df_recent_days=7, n_items=n_items)\n    pop_items, pop_scores = top_pop_items(pop_scores_vec, topk=300)\n\n    # Кандидаты на валидацию\n    target_users_val = np.array(list(gt_val.keys()), dtype=np.int64)\n    als_val_recs = als_recommend_batch(als_model, weighted_ui if als_model is not None else user_item_csr,\n                                       target_users_val, N=300, filter_seen=False, batch_size=2048) if als_model else {}\n    rp3_val_recs = rp3_recommend_batch(rp3_model, rp3_ui if rp3_model is not None else user_item_csr,\n                                       target_users_val, N=300, filter_seen=False, batch_size=4096) if rp3_model else {}\n    i2i_val_recs = similar_items_for_users(faiss_index, als_item_factors, user_histories,\n                                           topk_per_item=80, last_k=10) if faiss_index is not None else {}\n\n    cand_val = merge_candidates(als_val_recs, i2i_val_recs, pop_items, pop_scores, target_users_val,\n                                topk_merge=500, w_als=1.0, w_i2i=1.0, w_pop=0.2, rp3_recs=rp3_val_recs, w_rp3=1.0)\n\n    # SASRec train (на train-части)\n    sasrec_items = iids_sorted\n    sasrec_dataset = SequenceDataset(sasrec_items, offsets, max_len=max_seq_len)\n    sasrec_model = SASRec(n_items_padded=n_items + 1, max_len=max_seq_len, d_model=192, n_heads=4, n_layers=3, dropout=0.2)\n\n    def eval_val(model):\n        preds_val = rerank_with_sasrec(model, cand_val, sasrec_items, offsets, max_len=max_seq_len, batch_size=2048)\n        return map_at_k(gt_val, preds_val, k=20)\n\n    train_sasrec(sasrec_model, sasrec_dataset, epochs=3, batch_size=1024, lr=3e-4, val_eval_fn=eval_val,\n                 patience=2, save_path=\"sasrec_best.pt\")\n\n    # Stage B: Финальный рефит на всех 47+7 днях\n    print(\"Refit ALL modules on full data (train+val)...\")\n    full_df = df  # все дни\n    # CSR/ALS/RP3 на FULL\n    user_item_csr_full = build_user_item_csr(full_df, n_users, n_items)\n    als_model_full, weighted_ui_full = train_als(user_item_csr_full, use_gpu=False, factors=128, reg=1e-4, iters=20)\n    als_item_factors_full, als_user_factors_full = None, None\n    if als_model_full is not None:\n        als_item_factors_full = getattr(als_model_full, \"item_factors\", getattr(als_model_full, \"item_factors_\", None))\n        als_user_factors_full = getattr(als_model_full, \"user_factors\", getattr(als_model_full, \"user_factors_\", None))\n    faiss_index_full, _ = build_faiss_index(als_item_factors_full) if als_item_factors_full is not None else (None, None)\n\n    rp3_model_full, rp3_ui_full = train_rp3beta(user_item_csr_full, K=200, alpha=0.4, beta=0.7)\n\n    # Sequences FULL\n    uids_sorted_full, iids_sorted_full, offsets_full = build_user_offsets(full_df, n_users)\n\n    # Истории для тестовых пользователей — по FULL\n    test_users = np.unique(sample_uids[sample_uids >= 0])\n    user_histories_full = {}\n    for u in tqdm(test_users, desc=\"Build histories (full)\"):\n        s, e = offsets_full[u], offsets_full[u+1]\n        user_histories_full[u] = iids_sorted_full[s:e]\n\n    # Популярность FULL\n    pop_scores_vec_full = popularity_scores(full_df, df_recent_days=7, n_items=n_items)\n    pop_items_full, pop_scores_full = top_pop_items(pop_scores_vec_full, topk=300)\n\n    # Кандидаты для TEST (на FULL)\n    als_test_recs = als_recommend_batch(als_model_full, weighted_ui_full if als_model_full is not None else user_item_csr_full,\n                                        test_users, N=300, filter_seen=False, batch_size=4096) if als_model_full else {}\n    rp3_test_recs = rp3_recommend_batch(rp3_model_full, rp3_ui_full if rp3_model_full is not None else user_item_csr_full,\n                                        test_users, N=300, filter_seen=False, batch_size=16384) if rp3_model_full else {}\n    i2i_test_recs = similar_items_for_users(faiss_index_full, als_item_factors_full, user_histories_full,\n                                            topk_per_item=80, last_k=10) if faiss_index_full is not None else {}\n    cand_test = merge_candidates(als_test_recs, i2i_test_recs, pop_items_full, pop_scores_full, test_users,\n                                 topk_merge=500, w_als=1.0, w_i2i=1.0, w_pop=0.2, rp3_recs=rp3_test_recs, w_rp3=1.0)\n\n    # Реранкер FULL: подгружаем лучший чекпоинт и дообучаем на FULL 1-2 эпохи маленьким LR (быстрый fine-tune)\n    sasrec_model_full = SASRec(n_items_padded=n_items + 1, max_len=max_seq_len, d_model=192, n_heads=4, n_layers=3, dropout=0.2)\n    try:\n        sasrec_model_full.load_state_dict(torch.load(\"sasrec_best.pt\", map_location=\"cpu\"), strict=False)\n        print(\"Loaded best SASRec weights from train-part.\")\n    except Exception as e:\n        print(f\"Failed to load SASRec best weights: {e}. Using current weights.\")\n\n    full_dataset = SequenceDataset(iids_sorted_full, offsets_full, max_len=max_seq_len)\n    # Короткий фт на FULL с меньшим LR\n    train_sasrec(sasrec_model_full, full_dataset, epochs=2, batch_size=1024, lr=1.5e-4, val_eval_fn=None, patience=1)\n\n    # Реранк по FULL секвенциям\n    preds_test = rerank_with_sasrec(sasrec_model_full, cand_test, iids_sorted_full, offsets_full,\n                                    max_len=max_seq_len, batch_size=2048)\n\n    # Сабмишен (на FULL поп/обратные маппинги)\n    print(\"Writing submission...\")\n    sample = pd.read_csv(sample_path)\n    user_orig2uid = {orig: idx for idx, orig in enumerate(uid2orig)}\n    item_idx2orig = iid2orig\n\n    pop_scores_vec = pop_scores_vec_full\n    pop_items, pop_scores = pop_items_full, pop_scores_full\n\n    pred_item_cache = {}\n    pop20 = top_pop_items(pop_scores_vec, topk=20)[0]\n    for u_orig in tqdm(sample[\"user_id\"].unique(), desc=\"Cache predictions\"):\n        uid = user_orig2uid.get(int(u_orig), -1)\n        if uid == -1:\n            pred_item_cache[int(u_orig)] = item_idx2orig[pop20]\n            continue\n        ranked0 = preds_test.get(uid)\n        if not ranked0:\n            ranked0 = cand_test.get(uid, np.array([], dtype=np.int32)).tolist()\n        seen_set = set(ranked0)\n        for i in pop_items:\n            if len(ranked0) >= 20: break\n            if int(i) not in seen_set:\n                ranked0.append(int(i)); seen_set.add(int(i))\n        ranked0 = ranked0[:20]\n        pred_item_cache[int(u_orig)] = item_idx2orig[np.array(ranked0, dtype=np.int32)]\n\n    from collections import defaultdict\n    counters = defaultdict(int)\n    out_items = np.empty(len(sample), dtype=item_idx2orig.dtype)\n    for idx, u in tqdm(enumerate(sample[\"user_id\"].values), total=len(sample), desc=\"Fill submission\"):\n        i = counters[int(u)]\n        out_items[idx] = pred_item_cache[int(u)][i]\n        counters[int(u)] += 1\n\n    sample[\"item_id\"] = out_items\n    sample.to_csv(submission_path, index=False)\n    print(f\"Saved submission to {submission_path}\")bmission to {submission_path}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T12:38:29.832966Z","iopub.execute_input":"2025-10-18T12:38:29.833441Z","iopub.status.idle":"2025-10-18T12:38:29.837009Z","shell.execute_reply.started":"2025-10-18T12:38:29.833420Z","shell.execute_reply":"2025-10-18T12:38:29.836466Z"}},"outputs":[],"execution_count":null}]}
