{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91496,"databundleVersionId":11802066,"sourceType":"competition"},{"sourceId":13546940,"sourceType":"datasetVersion","datasetId":8603577},{"sourceId":13548163,"sourceType":"datasetVersion","datasetId":8604470}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall transformers -y\n!pip install -U \"transformers==4.56.0\" ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T12:11:33.581030Z","iopub.execute_input":"2025-11-17T12:11:33.581295Z","iopub.status.idle":"2025-11-17T12:11:49.637171Z","shell.execute_reply.started":"2025-11-17T12:11:33.581267Z","shell.execute_reply":"2025-11-17T12:11:49.635949Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Found existing installation: transformers 4.53.3\nUninstalling transformers-4.53.3:\n  Successfully uninstalled transformers-4.53.3\nCollecting transformers==4.56.0\n  Downloading transformers-4.56.0-py3-none-any.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.0) (3.19.1)\nCollecting huggingface-hub<1.0,>=0.34.0 (from transformers==4.56.0)\n  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.0) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.0) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.0) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.0) (2.32.5)\nCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers==4.56.0)\n  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.0) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.0) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.0) (2025.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.0) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.0) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.56.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.56.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.56.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.56.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.56.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.56.0) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.56.0) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.56.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.56.0) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.56.0) (2025.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.56.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.56.0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.56.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.56.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.56.0) (2024.2.0)\nDownloading transformers-4.56.0-py3-none-any.whl (11.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 1.0.0rc2\n    Uninstalling huggingface-hub-1.0.0rc2:\n      Successfully uninstalled huggingface-hub-1.0.0rc2\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface-hub-0.36.0 tokenizers-0.22.1 transformers-4.56.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pip install faiss-cpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T12:11:49.641555Z","iopub.execute_input":"2025-11-17T12:11:49.642227Z","iopub.status.idle":"2025-11-17T12:11:54.144627Z","shell.execute_reply.started":"2025-11-17T12:11:49.642200Z","shell.execute_reply":"2025-11-17T12:11:54.143888Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting faiss-cpu\n  Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nDownloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.13.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nimport faiss\nimport numpy as np\nfrom tqdm import tqdm\nimport os, zipfile, torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T12:11:55.933291Z","iopub.execute_input":"2025-11-17T12:11:55.933588Z","iopub.status.idle":"2025-11-17T12:12:22.140066Z","shell.execute_reply.started":"2025-11-17T12:11:55.933556Z","shell.execute_reply":"2025-11-17T12:12:22.139245Z"}},"outputs":[{"name":"stderr","text":"2025-11-17 12:12:08.106884: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763381528.301393      39 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763381528.351330      39 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Константы ===\nDATASET_NAME = \"Lipsrow/ruwiki_cleaned\"\nMODEL_EMB = \"sergeyzh/LaBSE-ru-turbo\"\n#MODEL_LLM = \"Qwen/Qwen3-4B-FP8\"\nDB_PATH = \"vector_db.faiss\"\nZIP_PATH = \"vector_db.zip\"\n\n# === 1. Загрузка и подготовка данных ===\nprint(\"Загружаем датасет...\")\nds = load_dataset(DATASET_NAME, split=\"train\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T12:12:22.141278Z","iopub.execute_input":"2025-11-17T12:12:22.142031Z","iopub.status.idle":"2025-11-17T12:13:39.376366Z","shell.execute_reply.started":"2025-11-17T12:12:22.142008Z","shell.execute_reply":"2025-11-17T12:13:39.375790Z"}},"outputs":[{"name":"stdout","text":"Загружаем датасет...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/24.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f2d749296a64614a5acbfaad94da439"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ruwiki.json:   0%|          | 0.00/7.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63641219925c413681e846bd52e41931"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/500000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7fa147d832f4c05a243864d39b2b30d"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import json\nimport time\n\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nimport faiss\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom sentence_transformers import SentenceTransformer\n\n# ====================== Константы путей/параметров ======================\n\nFAISS_PATH = \"/kaggle/input/vectordb/vector_db.faiss\"      # готовый faiss-индекс\nTEXTS_PATH = \"/kaggle/input/textss/texts.jsonl\"            # JSONL: по строке {\"text\": \"<абзац>\"}\n\nRAG_TOPK = 3\nRAG_EMB_MODEL = \"sergeyzh/LaBSE-ru-turbo\"\nRAG_QUERY_BATCH = 12\n\nGEN_MODEL_ID = \"Qwen/Qwen3-4B-FP8\"\nGEN_MAX_NEW = 200\nGEN_MAX_LEN = 4096\n\nSYSTEM_PROMPT = \"\"\"Ты — эрудированный ИИ-эксперт. Твоя главная сила — твои собственные обширные знания. Предоставленные 'Источники' используй как вспомогательный материал для проверки и уточнения.\n\nСледуй этому порядку действий:\n\n1.  **Сначала ответь сам.** Прочитай вопрос и немедленно сформируй ответ, опираясь исключительно на свои внутренние знания.\n\n2.  **Затем проверь по источникам.** Проанализируй предоставленные 'Источники' и сравни их с твоим ответом.\n\n3.  **Синтезируй финальный ответ по правилам:**\n    а) Если источники **подтверждают или дополняют** твои знания, объедини информацию в единый, исчерпывающий ответ.\n    б) Если источники **нерелевантны** (не относятся к вопросу), полностью проигнорируй их и используй только свой первоначальный ответ.\n    в) Если источники **противоречат** твоим знаниям, отдай приоритет информации из источников, ответ: \"не знаю\".\n    г) Финальный ответ пометь как CORRECT ANSWER: [ твой ответ или \"не знаю\" ]\n\nВажные правила, которые действуют всегда:\n- **Честность:** Если ответа нет ни в твоих знаниях, ни в источниках, просто и чётко напиши: \"не знаю\". Не выдумывай.\n- **Краткость:** Отвечай по существу, без вступлений и лишней \"воды\". \n\"\"\"\n\ndef extract_correct_answer(text: str) -> str:\n    if not text:\n        return \"не знаю\"\n    low = text.lower()\n    key = \"correct answer:\"\n    if key in low:\n        i = low.index(key)\n        s = text[i+len(key):].strip()\n        s = s.strip(\"[]()«»\\\"' \").strip()\n        return s if s else \"не знаю\"\n    return \"не знаю\"\n\n# ====================== GPU diag (опционально) ======================\n\ndef _mi(b):\n    return f\"{b/1024/1024:.1f} MiB\"\n\ndef print_cuda_overview():\n    try:\n        print(f\"[CUDA] torch={torch.__version__} | available={torch.cuda.is_available()} | torch.cuda={getattr(torch.version, 'cuda', None)}\")\n        dc = torch.cuda.device_count()\n        print(f\"[CUDA] device_count={dc}\")\n        for i in range(dc):\n            name = torch.cuda.get_device_name(i)\n            free, total = torch.cuda.mem_get_info(i)\n            alloc = torch.cuda.memory_allocated(i)\n            resv  = torch.cuda.memory_reserved(i)\n            print(f\"[CUDA][dev{i}] {name} | free={_mi(free)} | total={_mi(total)} | allocated={_mi(alloc)} | reserved={_mi(resv)}\")\n    except Exception as e:\n        print(f\"[CUDA] overview error: {e}\")\n\nprint(\"[DEBUG] BOOTSTRAP\")\nprint_cuda_overview()\n\n# ====================== Загрузка индекса и текстов ======================\n\nprint(f\"[RAG] Loading FAISS: {FAISS_PATH}\")\nindex = faiss.read_index(FAISS_PATH)\n\nprint(f\"[RAG] Loading texts: {TEXTS_PATH}\")\ntexts = []\nwith open(TEXTS_PATH, \"r\", encoding=\"utf-8\") as f:\n    for line in tqdm(f, desc=\"load-texts\", unit=\"lines\"):\n        try:\n            obj = json.loads(line)\n            texts.append(obj.get(\"text\", \"\"))\n        except Exception:\n            texts.append(\"\")\nprint(f\"[RAG] texts loaded: {len(texts)}\")\n\n# ====================== Модели ======================\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprint(f\"[RAG] Loading embedder: {RAG_EMB_MODEL} on {device}\")\nembedder = SentenceTransformer(RAG_EMB_MODEL, device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T12:14:21.244148Z","iopub.execute_input":"2025-11-17T12:14:21.244783Z","iopub.status.idle":"2025-11-17T12:15:02.476971Z","shell.execute_reply.started":"2025-11-17T12:14:21.244758Z","shell.execute_reply":"2025-11-17T12:15:02.476075Z"}},"outputs":[{"name":"stdout","text":"[DEBUG] BOOTSTRAP\n[CUDA] torch=2.6.0+cu124 | available=True | torch.cuda=12.4\n[CUDA] device_count=1\n[CUDA][dev0] Tesla P100-PCIE-16GB | free=16013.1 MiB | total=16269.2 MiB | allocated=0.0 MiB | reserved=0.0 MiB\n[RAG] Loading FAISS: /kaggle/input/vectordb/vector_db.faiss\n[RAG] Loading texts: /kaggle/input/textss/texts.jsonl\n","output_type":"stream"},{"name":"stderr","text":"load-texts: 500000lines [00:12, 40462.43lines/s]\n","output_type":"stream"},{"name":"stdout","text":"[RAG] texts loaded: 500000\n[RAG] Loading embedder: sergeyzh/LaBSE-ru-turbo on cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/368 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d16585a5c7e043a39b8523aa1598a3ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07afa0c73b73478d8d711b6052c967b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/56.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1c7c7ab77b547c6a744883168c05477"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/887 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d786260e38764e0c909f3d1d587b9c85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/513M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5ba50dca3e54f9494bb4cd6b65a155c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49c8c3c65ed94422870c518472daa9ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"387ffccbed814ed0b0818228a24624b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/732 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b22158b16ca048b3b24129eb9b000557"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/196 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b33287f76bd4715ac62da64b13ba16f"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"!pip install llama-cpp-python","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T12:15:02.478270Z","iopub.execute_input":"2025-11-17T12:15:02.478554Z","iopub.status.idle":"2025-11-17T12:17:30.945810Z","shell.execute_reply.started":"2025-11-17T12:15:02.478531Z","shell.execute_reply":"2025-11-17T12:17:30.944624Z"}},"outputs":[{"name":"stdout","text":"Collecting llama-cpp-python\n  Downloading llama_cpp_python-0.3.16.tar.gz (50.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.15.0)\nRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (1.26.4)\nCollecting diskcache>=5.6.1 (from llama-cpp-python)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.20.0->llama-cpp-python) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\nDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.16-cp311-cp311-linux_x86_64.whl size=4503243 sha256=16d85400ed5e85c8483f12db9c0944bc5640fb95769adc357eff01593bf4c5a8\n  Stored in directory: /root/.cache/pip/wheels/d8/5b/e5/a7d4b5765da347d314e8155197440c9995a962f8e4a5f52b23\nSuccessfully built llama-cpp-python\nInstalling collected packages: diskcache, llama-cpp-python\nSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.3.16\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\nfrom llama_cpp import Llama\n\nmodel = Llama.from_pretrained(\n\trepo_id=\"yandex/YandexGPT-5-Lite-8B-instruct-GGUF\",\n\tfilename=\"YandexGPT-5-Lite-8B-instruct-Q4_K_M.gguf\",\n    device= 'gpu'\n)\n\nФ\nmodel.create_chat_completion(\n\tmessages=[\n            {\"role\": \"system\", \"content\": rag_system_prompt + texts_prepared},\n            {\"role\": \"user\", \"content\": question},\n        ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T12:20:02.917767Z","iopub.execute_input":"2025-11-17T12:20:02.918062Z","iopub.status.idle":"2025-11-17T12:20:08.384770Z","shell.execute_reply.started":"2025-11-17T12:20:02.918040Z","shell.execute_reply":"2025-11-17T12:20:08.384169Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\n# === Вспомогательные функции ===\ndef apply_chat(prompts, tokenizer):\n    \"\"\"Форматирование списка сообщений в текст через шаблон Qwen (без think).\"\"\"\n    texts_fmt = []\n    for p in prompts:\n        messages = [{\"role\": \"user\", \"content\": p}]\n        try:\n            t = tokenizer.apply_chat_template(\n                messages, tokenize=False, add_generation_prompt=True, enable_thinking=False\n            )\n        except TypeError:\n            t = tokenizer.apply_chat_template(\n                messages, tokenize=False, add_generation_prompt=True\n            )\n        texts_fmt.append(t)\n    return texts_fmt\n\n@torch.inference_mode()\ndef batch_generate(prompts, tokenizer, model, max_new_tokens=200, max_length=4096):\n    \"\"\"Быстрая генерация на GPU без pipeline (ровно как ты хочешь).\"\"\"\n    if not prompts:\n        return []\n    texts_fmt = apply_chat(prompts, tokenizer)\n\n    t0 = time.perf_counter()\n    model_inputs = tokenizer(\n        texts_fmt,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=max_length\n    ).to(model.device)\n    t1 = time.perf_counter()\n\n    gen = model.generate(\n        **model_inputs,\n        max_new_tokens=max_new_tokens,\n        do_sample=False,\n        num_beams=1,\n        use_cache=True,\n        pad_token_id=tokenizer.pad_token_id,\n        eos_token_id=tokenizer.eos_token_id\n    )\n    t2 = time.perf_counter()\n\n    attn = model_inputs[\"attention_mask\"]\n    input_lens = attn.sum(dim=1).tolist()\n\n    outputs = []\n    for i, gen_ids in enumerate(gen):\n        in_len = int(input_lens[i])\n        out_ids = gen_ids[in_len:]\n        text = tokenizer.decode(out_ids, skip_special_tokens=True).strip()\n        outputs.append(text)\n\n    print(f\"[DEBUG] tokenize: {t1 - t0:.2f}s, generate: {t2 - t1:.2f}s, total: {t2 - t0:.2f}s\")\n    return outputs\n\n# === RAG для батча ===\ndef fast_rag_batch(queries, top_k=3, max_new_tokens=200):\n    \"\"\"\n    1) Векторизация\n    2) Поиск контекста\n    3) Формирование промпта (system + user)\n    4) Генерация через model.generate()\n    \"\"\"\n    q_vecs = embedder.encode(\n        queries,\n        convert_to_numpy=True,\n        normalize_embeddings=True,\n        batch_size=64,\n        show_progress_bar=False,\n        device=device\n    ).astype(\"float32\")\n\n    _, idxs = index.search(q_vecs, top_k)\n    contexts = []\n    for ids in idxs:\n        ctx = \"\\n\\n\".join([f\"Статья {int(i)}: {texts[int(i)]}\" for i in ids if 0 <= int(i) < len(texts)])\n        contexts.append(ctx)\n\n    prompts = []\n    for q, ctx in zip(queries, contexts):\n        content = f\"\"\"Используя правила выше, ответь на вопрос.\n\nИсточники:\n---\n{ctx}\n---\n\nВопрос: {q}\n\nОтвет:\"\"\"\n        full_prompt = f\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\\n<|im_start|>user\\n{content}<|im_end|>\\n<|im_start|>assistant\\n\"\n        prompts.append(full_prompt)\n\n    answers = batch_generate(prompts, tokenizer, model, max_new_tokens=max_new_tokens, max_length=GEN_MAX_LEN)\n    return answers\n\n# ====================== Запуск: читаем вопросы → батчами RAG → пишем ответы ======================\n\n'''print(\"[DEBUG] Читаю input.json...\")\nwith open(\"input.json\", \"r\", encoding=\"utf-8\") as f:\n    all_questions = json.load(f)\nprint(f\"[DEBUG] Прочитано {len(all_questions)} вопросов\")'''\nall_questions = [\n    \"Кто написал Евгения Онегина?\",\n    \"Когда распался Советский Союз?\",\n    \"Что такое квантовая запутанность?\",\n    'Кто автор книги \"Детство в Соломбале\"?',\n    'Как зовут человека, который написал «Детство в Соломбале»?',\n    'Кто написал книгу «Детство в Соломбале»?',\n    'В каком году начал поддерживаться сеттинг Eberron?',\n    'Какое окрашивание приобретает безводный метанол при растворении небольшого количества медного купороса?',\n    \"Какой античный математик изобрёл первый дизельный двигатель внутреннего сгорания?\",\n    'Кто первый человек высадившийся на солнце?',\n    'Как звали отца Александра Сергеевича Пушкина?',\n    \"Какая компания осуществляет доставку роботами в Москве?\",\n    \"Кто имеет крупнейшую доставку роботами на территории Москвы?\",\n    \"В Москве есть доставка роботами, какая компания выполняет её?\"\n] * 10\n\nfinal_answers = []\nwith tqdm(total=len(all_questions), desc=\"RAG\", unit=\"q\") as pbar:\n    for i in range(0, len(all_questions), RAG_QUERY_BATCH):\n        batch_q = all_questions[i:i + RAG_QUERY_BATCH]\n        raw = fast_rag_batch(batch_q, top_k=RAG_TOPK, max_new_tokens=GEN_MAX_NEW)\n        # извлекаем только CORRECT ANSWER, иначе \"не знаю\"\n        print('\\n'.join(raw))\n        cleaned = [extract_correct_answer(x) for x in raw]\n        cleaned = [a if a.strip() else \"не знаю\" for a in cleaned]\n        final_answers.extend(cleaned)\n        pbar.update(len(batch_q))\n\nprint(\"[DEBUG] Записываю output.json...\")\nwith open(\"output.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(final_answers, f, ensure_ascii=False)\n\nprint(\"[DEBUG] Готово.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T21:58:57.142437Z","iopub.execute_input":"2025-10-29T21:58:57.142955Z","execution_failed":"2025-10-29T22:02:19.796Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"хулитак долго, поч так много печатает и поч раньше 3 промпта были быстрее","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, json\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T20:41:30.128057Z","iopub.execute_input":"2025-10-29T20:41:30.128728Z","iopub.status.idle":"2025-10-29T20:41:30.134517Z","shell.execute_reply.started":"2025-10-29T20:41:30.128687Z","shell.execute_reply":"2025-10-29T20:41:30.132775Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"texts = []\nMAX_WORDS = 200\nfor text in tqdm(ds['text'], desc=\"Извлечение первых абзацев\"):\n    first_paragraph = text.strip().split(\"\\n\")[0]\n    words = first_paragraph.split()\n    if len(words) > MAX_WORDS:\n        first_paragraph = \" \".join(words[:MAX_WORDS])\n    texts.append(first_paragraph)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T20:43:33.671019Z","iopub.execute_input":"2025-10-29T20:43:33.671342Z","iopub.status.idle":"2025-10-29T20:44:01.894113Z","shell.execute_reply.started":"2025-10-29T20:43:33.671316Z","shell.execute_reply":"2025-10-29T20:44:01.893094Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, json\nfrom tqdm import tqdm\n\nout_path = \"texts.jsonl\"\n\nwith open(out_path, \"w\", encoding=\"utf-8\") as f:\n    for t in tqdm(texts, desc=\"write-texts\"):\n        para = t  # если уже первый абзац – можно убрать\n        if not para:\n            continue\n        f.write(json.dumps({\"text\": para}, ensure_ascii=False) + \"\\n\")\n\nprint(\"saved:\", out_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T20:44:01.896162Z","iopub.execute_input":"2025-10-29T20:44:01.896453Z","iopub.status.idle":"2025-10-29T20:44:07.338602Z","shell.execute_reply.started":"2025-10-29T20:44:01.896431Z","shell.execute_reply":"2025-10-29T20:44:07.337487Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loaded_texts = []\nwith open(\"/kaggle/working/texts.jsonl\", \"r\", encoding=\"utf-8\") as f:\n    for line in tqdm(f, desc=\"read-texts\"):\n        obj = json.loads(line)\n        loaded_texts.append(obj.get(\"text\", \"\"))\nprint(\"loaded:\", len(loaded_texts))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T20:44:14.961904Z","iopub.execute_input":"2025-10-29T20:44:14.962214Z","iopub.status.idle":"2025-10-29T20:44:19.105055Z","shell.execute_reply.started":"2025-10-29T20:44:14.962183Z","shell.execute_reply":"2025-10-29T20:44:19.104138Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport time\nimport zipfile\nimport gc\n\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nimport faiss\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom sentence_transformers import SentenceTransformer\n\n# ====================== GPU DIAG ======================\n\ndef _mi(b):\n    return f\"{b/1024/1024:.1f} MiB\"\n\ndef print_cuda_overview():\n    try:\n        print(f\"[CUDA] torch={torch.__version__} | available={torch.cuda.is_available()} | torch.cuda={getattr(torch.version, 'cuda', None)}\")\n        dc = torch.cuda.device_count()\n        print(f\"[CUDA] device_count={dc}\")\n        for i in range(dc):\n            name = torch.cuda.get_device_name(i)\n            free, total = torch.cuda.mem_get_info(i)\n            alloc = torch.cuda.memory_allocated(i)\n            resv  = torch.cuda.memory_reserved(i)\n            print(f\"[CUDA][dev{i}] {name} | free={_mi(free)} | total={_mi(total)} | allocated={_mi(alloc)} | reserved={_mi(resv)}\")\n    except Exception as e:\n        print(f\"[CUDA] overview error: {e}\")\n\ndef print_cuda_mem(tag):\n    try:\n        if not torch.cuda.is_available():\n            print(f\"[CUDA] {tag}: cuda not available\"); return\n        try: torch.cuda.synchronize()\n        except Exception: pass\n        i = torch.cuda.current_device()\n        name = torch.cuda.get_device_name(i)\n        free, total = torch.cuda.mem_get_info(i)\n        alloc = torch.cuda.memory_allocated(i)\n        resv  = torch.cuda.memory_reserved(i)\n        print(f\"[CUDA] {tag} | dev={i} {name} | free={_mi(free)} | total={_mi(total)} | allocated={_mi(alloc)} | reserved={_mi(resv)}\")\n    except Exception as e:\n        print(f\"[CUDA] {tag} error: {e}\")\n\n# ====================== RAG CONFIG ======================\n\nRAG_INDEX_ZIP = \"./rag_index.zip\"\nFAISS_PATH = \"/kaggle/input/vectordb/vector_db.faiss\"\nTEXTS_PATH = \"/kaggle/input/textss/texts.jsonl\"\n\nRAG_TOPK = 3\nRAG_EMB_MODEL = \"sergeyzh/LaBSE-ru-turbo\"\nRAG_QUERY_BATCH = 16\nGEN_INNER_BATCH = 8 # Этот параметр больше не используется\n\nGEN_MODEL_ID = \"Qwen/Qwen3-4B-FP8\"\nGEN_MAX_NEW = 200\nGEN_MAX_LEN = 2048\n\nSYSTEM_PROMPT = \"\"\"Ты — эрудированный ИИ-эксперт. Твоя главная сила — твои собственные обширные знания. Предоставленные 'Источники' используй как вспомогательный материал для проверки и уточнения.\n\nСледуй этому порядку действий:\n\n1.  **Сначала ответь сам.** Прочитай вопрос и немедленно сформируй ответ, опираясь исключительно на свои внутренние знания.\n\n2.  **Затем проверь по источникам.** Проанализируй предоставленные 'Источники' и сравни их с твоим ответом.\n\n3.  **Синтезируй финальный ответ по правилам:**\n    а) Если источники **подтверждают или дополняют** твои знания, объедини информацию в единый, исчерпывающий ответ.\n    б) Если источники **нерелевантны** (не относятся к вопросу), полностью проигнорируй их и используй только свой первоначальный ответ.\n    в) Если источники **противоречат** твоим знаниям, отдай приоритет информации из источников, ответ: \"не знаю\"\n\nВажные правила, которые действуют всегда:\n- **Честность:** Если ответа нет ни в твоих знаниях, ни в источниках, просто и чётко напиши: \"не знаю\". Не выдумывай.\n- **Краткость:** Отвечай по существу, без вступлений и лишней \"воды\". \n\"\"\"\n\ndef extract_correct_answer(text: str) -> str:\n    if not text:\n        return \"не знаю\"\n    low = text.lower()\n    key = \"correct answer:\"\n    if key in low:\n        i = low.index(key)\n        s = text[i+len(key):].strip()\n        s = s.strip(\"[]()«»\\\"' \").strip()\n        return s if s else \"не знаю\"\n    return \"не знаю\"\n\n# ====================== SOLUTION (RAG only) ======================\n\nclass Solution:\n    def __init__(self, model_id=GEN_MODEL_ID, device='cuda'):\n        print(f\"[DEBUG] Solution: init, device={device}, model_id={model_id}\")\n        self.device = device\n        self._global_start_time = None\n\n        # 1) Индекс\n        print(f\"[RAG] Loading FAISS: {FAISS_PATH}\")\n        self.faiss_index = faiss.read_index(FAISS_PATH)\n\n        print(f\"[RAG] Loading texts: {TEXTS_PATH}\")\n        self.rag_texts = []\n        with open(TEXTS_PATH, \"r\", encoding=\"utf-8\") as f:\n            for line in tqdm(f, desc=\"load-texts\", unit=\"lines\"):\n                try:\n                    obj = json.loads(line)\n                    self.rag_texts.append(obj.get(\"text\", \"\"))\n                except Exception:\n                    self.rag_texts.append(\"\")\n        print(f\"[RAG] texts loaded: {len(self.rag_texts)}\")\n\n        # 2) Эмбеддер\n        print(f\"[RAG] Loading embedder: {RAG_EMB_MODEL} on {device}\")\n        self.rag_embedder = SentenceTransformer(RAG_EMB_MODEL, device=device)\n\n        # 3) LLM\n        print_cuda_mem(\"before model load\")\n        t0 = time.perf_counter()\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_id,\n            trust_remote_code=True,\n            torch_dtype = 'auto'\n        ).to('cuda')\n        # === Подготовка модели ===\n        model.eval()\n        t1 = time.perf_counter()\n        print(f\"[DEBUG] Solution: model loaded in {t1 - t0:.2f}s\")\n        print_cuda_mem(\"after model load\")\n\n        t2 = time.perf_counter()\n        self.tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True, trust_remote_code=True)\n        t3 = time.perf_counter()\n        print(f\"[DEBUG] Solution: tokenizer loaded in {t3 - t2:.2f}s\")\n\n        self.model.eval()\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n\n        self.tokenizer.padding_side = \"right\"\n        self.tokenizer.truncation_side = \"left\"\n\n        print(f\"[DEBUG] device_map=cuda\")\n\n    def _apply_chat(self, prompts):\n        texts = []\n        for p in prompts:\n            if \"<|im_start|>\" in p:\n                texts.append(p); continue\n            messages = [{\"role\": \"user\", \"content\": p}]\n            try:\n                t = self.tokenizer.apply_chat_template(\n                    messages, tokenize=False, add_generation_prompt=True, enable_thinking=False\n                )\n            except TypeError:\n                t = self.tokenizer.apply_chat_template(\n                    messages, tokenize=False, add_generation_prompt=True\n                )\n            texts.append(t)\n        return texts\n\n    # =========================================================================\n    # === НОВАЯ СТРАТЕГИЯ БАТЧИНГА (из Script A) ===\n    # =========================================================================\n    def _batch_generate(self, prompts, max_new_tokens, max_length):\n        print(f\"[DEBUG] _batch_generate: start, prompts={len(prompts)}, max_new_tokens={max_new_tokens}, max_length={max_length}\")\n        if not prompts:\n            return []\n\n        texts = self._apply_chat(prompts)\n        t_tok0 = time.perf_counter()\n        model_inputs = self.tokenizer(\n            texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length\n        ).to(self.device)\n        t_tok1 = time.perf_counter()\n\n        bsz = int(model_inputs.input_ids.shape[0])\n        seqlen = int(model_inputs.input_ids.shape[1])\n        tot_in_tok = int(model_inputs[\"attention_mask\"].sum().item())\n        print(f\"[DEBUG] _batch_generate: tokenized bsz={bsz}, seqlen={seqlen}, total_input_tokens={tot_in_tok}, tokenize_time={t_tok1 - t_tok0:.2f}s\")\n\n        print_cuda_mem(\"gen: before generate\")\n\n        t_gen0 = time.perf_counter()\n        with torch.no_grad():\n            gen = self.model.generate(\n                **model_inputs,\n                max_new_tokens=max_new_tokens,\n                do_sample=False,\n                num_beams=1,\n                use_cache=True,\n                pad_token_id=self.tokenizer.pad_token_id,\n                eos_token_id=self.tokenizer.eos_token_id\n            )\n        t_gen1 = time.perf_counter()\n        print(f\"[DEBUG] _batch_generate: generated shape={tuple(gen.shape)}, gen_time={t_gen1 - t_gen0:.2f}s\")\n        print_cuda_mem(\"gen: after generate\")\n\n        attn = model_inputs[\"attention_mask\"]\n        input_lens = attn.sum(dim=1).tolist()\n        outputs = []\n        total_new = 0\n        for i, gen_ids in enumerate(gen):\n            in_len = int(input_lens[i])\n            out_ids = gen_ids[in_len:]\n            total_new += int(out_ids.shape[0])\n            outputs.append(self.tokenizer.decode(out_ids, skip_special_tokens=True).strip())\n        print(f\"[DEBUG] _batch_generate: total_new_tokens={total_new}, avg_new={total_new / max(1, len(prompts)):.1f}\")\n        return outputs\n    # =========================================================================\n    # === КОНЕЦ НОВОЙ СТРАТЕГИИ ===\n    # =========================================================================\n\n    def _rag_batch(self, queries, top_k=RAG_TOPK, max_new_tokens=GEN_MAX_NEW, max_length=GEN_MAX_LEN):\n        # 1) Векторизация запросов\n        q_vecs = self.rag_embedder.encode(\n            queries,\n            convert_to_numpy=True,\n            normalize_embeddings=True,\n            batch_size=64,\n            show_progress_bar=False,\n            device=self.device\n        ).astype(\"float32\")\n\n        # 2) FAISS search\n        _, I = self.faiss_index.search(q_vecs, top_k)\n\n        # 3) Формирование промптов\n        prompts = []\n        for q, ids in zip(queries, I):\n            ctx_lines = []\n            for ii in ids:\n                idx = int(ii)\n                if 0 <= idx < len(self.rag_texts):\n                    ctx_lines.append(f\"Статья {idx}: {self.rag_texts[idx]}\")\n            ctx = \"\\n\\n\".join(ctx_lines)\n            content = f\"\"\"Используя правила выше, ответь на вопрос.\n\nИсточники:\n---\n{ctx}\n---\n\nВопрос: {q}\n\nОтвет:\"\"\"\n            full_prompt = f\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\\n<|im_start|>user\\n{content}<|im_end|>\\n<|im_start|>assistant\\n\"\n            prompts.append(full_prompt)\n\n        # 4) Генерация (теперь используется простая и быстрая стратегия батчинга)\n        outs = self._batch_generate(prompts, max_new_tokens=max_new_tokens, max_length=max_length)\n        finals = outs\n        return finals\n\n    def answer_all(self, questions):\n        print(f\"[DEBUG] answer_all: total questions = {len(questions)}\")\n        self._global_start_time = time.perf_counter()\n        answers = []\n        total = len(questions)\n        with tqdm(total=total, desc=\"RAG-batches\", unit=\"q\") as pbar:\n            for i in range(0, total, RAG_QUERY_BATCH):\n                batch_q = questions[i:i+RAG_QUERY_BATCH]\n                batch_ans = self._rag_batch(batch_q, top_k=RAG_TOPK, max_new_tokens=GEN_MAX_NEW, max_length=GEN_MAX_LEN)\n                print(batch_ans)\n                answers.extend(batch_ans)\n                pbar.update(len(batch_q))\n                # чистка между крупными батчами\n                gc.collect()\n                try: torch.cuda.empty_cache()\n                except Exception: pass\n        elapsed = time.perf_counter() - self._global_start_time\n        print(f\"[DEBUG] answer_all: elapsed={elapsed:.2f}s\")\n        answers = [a if a.strip() else \"не знаю\" for a in answers]\n        return answers\n\n# ====================== RUN (no main) ======================\n\nprint(\"[DEBUG] BOOTSTRAP\")\nprint_cuda_overview()\n\n# демо-вопросы, или подставь свой ввод\nmodel_input = [\n    \"Кто написал Евгения Онегина?\",\n    \"Когда распался Советский Союз?\",\n    \"Что такое квантовая запутанность?\",\n    'Кто автор книги \"Детство в Соломбале\"?',\n    'Как зовут человека, который написал «Детство в Соломбале»?',\n    'Кто написал книгу «Детство в Соломбале»?',\n    'В каком году начал поддерживаться сеттинг Eberron?',\n    'Какое окрашивание приобретает безводный метанол при растворении небольшого количества медного купороса?',\n    \"Какой античный математик изобрёл первый дизельный двигатель внутреннего сгорания?\",\n    'Кто первый человек высадившийся на солнце?',\n    'Как звали отца Александра Сергеевича Пушкина?',\n    \"Какая компания осуществляет доставку роботами в Москве?\",\n    \"Кто имеет крупнейшую доставку роботами на территории Москвы?\",\n    \"В Москве есть доставка роботами, какая компания выполняет её?\"\n] * 10\n\nt_all0 = time.perf_counter()\nprint(\"[DEBUG] Создаю Solution()...\")\nsolution = Solution()\nprint(\"[DEBUG] Solution создан. Запускаю RAG...\")\nmodel_output = solution.answer_all(model_input)\nt_all1 = time.perf_counter()\nprint(f\"[DEBUG] Готово: {len(model_output)} ответов, total_time={t_all1 - t_all0:.2f}s\")\n\nprint_cuda_mem(\"final\")\n\nprint(\"[DEBUG] Записываю output.json...\")\nwith open('output.json', 'w', encoding='utf-8') as f:\n    json.dump(model_output, f, ensure_ascii=False)\nprint(\"[DEBUG] КОНЕЦ: output.json записан успешно\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T21:38:06.331550Z","iopub.execute_input":"2025-10-29T21:38:06.332554Z","execution_failed":"2025-10-29T21:42:38.353Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T18:28:50.194225Z","iopub.execute_input":"2025-10-29T18:28:50.194422Z","iopub.status.idle":"2025-10-29T18:28:50.197317Z","shell.execute_reply.started":"2025-10-29T18:28:50.194407Z","shell.execute_reply":"2025-10-29T18:28:50.196775Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.DataFrame({'text' : texts})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T18:28:56.606774Z","iopub.execute_input":"2025-10-29T18:28:56.607057Z","iopub.status.idle":"2025-10-29T18:28:58.409863Z","shell.execute_reply.started":"2025-10-29T18:28:56.607039Z","shell.execute_reply":"2025-10-29T18:28:58.409074Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.to_csv('texts',index = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T18:29:33.891044Z","iopub.execute_input":"2025-10-29T18:29:33.891326Z","iopub.status.idle":"2025-10-29T18:29:40.245287Z","shell.execute_reply.started":"2025-10-29T18:29:33.891307Z","shell.execute_reply":"2025-10-29T18:29:40.244490Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = 'cuda'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T18:25:28.899506Z","iopub.execute_input":"2025-10-29T18:25:28.900236Z","iopub.status.idle":"2025-10-29T18:25:28.902996Z","shell.execute_reply.started":"2025-10-29T18:25:28.900214Z","shell.execute_reply":"2025-10-29T18:25:28.902377Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"50 /x 500000 / 1200","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === 2. Создание эмбеддингов ===\nprint(\"Создаём эмбеддинги...\")\nembedder = SentenceTransformer(MODEL_EMB).to(device)\nembeddings = []\n\nfor i in tqdm(range(0, len(texts), 256), desc=\"Векторизация\"):\n    batch = texts[i:i+256]\n    vecs = embedder.encode(batch, convert_to_numpy=True, show_progress_bar=False, device = device,normalize_embeddings=True)\n    embeddings.append(vecs)\n\nembeddings = np.vstack(embeddings).astype(\"float32\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T16:31:56.467834Z","iopub.execute_input":"2025-10-29T16:31:56.468596Z","iopub.status.idle":"2025-10-29T17:01:45.594051Z","shell.execute_reply.started":"2025-10-29T16:31:56.468549Z","shell.execute_reply":"2025-10-29T17:01:45.593404Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embeddings[0].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T17:02:05.210791Z","iopub.execute_input":"2025-10-29T17:02:05.211436Z","iopub.status.idle":"2025-10-29T17:02:05.214882Z","shell.execute_reply.started":"2025-10-29T17:02:05.211412Z","shell.execute_reply":"2025-10-29T17:02:05.214364Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === 3. Создание FAISS индекса ===\nprint(\"Создаём FAISS индекс...\")\ndim = embeddings.shape[1]\nindex = faiss.IndexFlatIP(dim)\nindex.add(embeddings)\nfaiss.write_index(index, DB_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T17:02:07.862138Z","iopub.execute_input":"2025-10-29T17:02:07.862798Z","iopub.status.idle":"2025-10-29T17:02:09.949422Z","shell.execute_reply.started":"2025-10-29T17:02:07.862776Z","shell.execute_reply":"2025-10-29T17:02:09.948820Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === 4. Архивация базы ===\nprint(\"Архивируем векторную базу...\")\nwith zipfile.ZipFile(ZIP_PATH, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n    zipf.write(DB_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T17:02:31.324778Z","iopub.execute_input":"2025-10-29T17:02:31.325457Z","iopub.status.idle":"2025-10-29T17:04:08.240394Z","shell.execute_reply.started":"2025-10-29T17:02:31.325433Z","shell.execute_reply":"2025-10-29T17:04:08.239609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DB_PATH = '/kaggle/input/vectordb/vector_db.faiss'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T18:25:13.419743Z","iopub.execute_input":"2025-10-29T18:25:13.420345Z","iopub.status.idle":"2025-10-29T18:25:13.423059Z","shell.execute_reply.started":"2025-10-29T18:25:13.420325Z","shell.execute_reply":"2025-10-29T18:25:13.422582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Создаём эмбеддинги...\")\nembedder = SentenceTransformer(MODEL_EMB).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T18:26:17.744764Z","iopub.execute_input":"2025-10-29T18:26:17.745252Z","iopub.status.idle":"2025-10-29T18:26:22.748404Z","shell.execute_reply.started":"2025-10-29T18:26:17.745235Z","shell.execute_reply":"2025-10-29T18:26:22.747578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T18:25:43.688297Z","iopub.execute_input":"2025-10-29T18:25:43.688956Z","iopub.status.idle":"2025-10-29T18:25:43.691746Z","shell.execute_reply.started":"2025-10-29T18:25:43.688933Z","shell.execute_reply":"2025-10-29T18:25:43.691223Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === --- НА СТАДИИ ЗАПУСКА В ДОКЕРЕ --- ===\n# Распаковываем и загружаем базу:\nif not os.path.exists(DB_PATH):\n    print(\"Распаковываем базу данных...\")\n    with zipfile.ZipFile(ZIP_PATH, \"r\") as zipf:\n        zipf.extractall(\".\")\n\nindex = faiss.read_index(DB_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T18:26:22.749309Z","iopub.execute_input":"2025-10-29T18:26:22.749543Z","iopub.status.idle":"2025-10-29T18:26:32.115109Z","shell.execute_reply.started":"2025-10-29T18:26:22.749524Z","shell.execute_reply":"2025-10-29T18:26:32.114423Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === 5. Инициализация LLM ===\nprint(\"Загружаем модель для retrieval (Qwen)...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_LLM)\nmodel = AutoModelForCausalLM.from_pretrained(\n            MODEL_LLM,\n            torch_dtype=\"auto\",\n            device_map=\"cuda\",     # при OOM можно заменить на \"auto\"\n            trust_remote_code=True\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T18:26:33.668213Z","iopub.execute_input":"2025-10-29T18:26:33.668856Z","iopub.status.idle":"2025-10-29T18:26:46.149369Z","shell.execute_reply.started":"2025-10-29T18:26:33.668834Z","shell.execute_reply":"2025-10-29T18:26:46.148723Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T18:26:54.841808Z","iopub.execute_input":"2025-10-29T18:26:54.842098Z","iopub.status.idle":"2025-10-29T18:26:54.846335Z","shell.execute_reply.started":"2025-10-29T18:26:54.842079Z","shell.execute_reply":"2025-10-29T18:26:54.845795Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T18:27:02.444343Z","iopub.execute_input":"2025-10-29T18:27:02.444598Z","iopub.status.idle":"2025-10-29T18:27:05.777464Z","shell.execute_reply.started":"2025-10-29T18:27:02.444580Z","shell.execute_reply":"2025-10-29T18:27:05.776647Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === 6. Функция поиска и генерации ===\ndef retrieve_and_answer(query, top_k=3, max_new_tokens=120):\n    query_vec = embedder.encode([query], convert_to_numpy=True).astype(\"float32\")\n    scores, ids = index.search(query_vec, top_k)\n    context = \"\\n\\n\".join([texts[i] for i in ids[0]])\n    prompt = f\"Контекст:\\n{context}\\n\\nВопрос: {query}\\nОтвет на основе контекста:\"\n    result = generator(prompt, max_new_tokens=max_new_tokens, do_sample=False)\n    return result[0][\"generated_text\"]\nt = time.time()\n# === 7. Пример использования ===\nquery = \"Кто был первым президентом России?\"\nprint(\"\\n=== РЕЗУЛЬТАТ ===\")\nprint(retrieve_and_answer(query))\nprint(time.time() - t)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T17:09:37.382502Z","iopub.execute_input":"2025-10-29T17:09:37.382807Z","iopub.status.idle":"2025-10-29T17:09:52.919328Z","shell.execute_reply.started":"2025-10-29T17:09:37.382785Z","shell.execute_reply":"2025-10-29T17:09:52.918750Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.padding_side = \"right\"  # ← ВОТ ЭТУ СТРОКУ ИЗМЕНИ\ntokenizer.truncation_side = \"left\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T17:23:46.754417Z","iopub.execute_input":"2025-10-29T17:23:46.755129Z","iopub.status.idle":"2025-10-29T17:23:46.757953Z","shell.execute_reply.started":"2025-10-29T17:23:46.755105Z","shell.execute_reply":"2025-10-29T17:23:46.757401Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch, time\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T18:57:21.502996Z","iopub.execute_input":"2025-10-29T18:57:21.503624Z","iopub.status.idle":"2025-10-29T18:57:21.507239Z","shell.execute_reply.started":"2025-10-29T18:57:21.503603Z","shell.execute_reply":"2025-10-29T18:57:21.506703Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"SYSTEM_PROMPT = \"\"\"Ты — умный, честный и осторожный ИИ-ассистент. Следуй этим правилам строго:\n\n1. **Оценивай релевантность источников.** Если предоставленные 'Источники' не содержат информации по теме вопроса или явно не отвечают на него — считай, что ответа в источниках нет.\n\n2. **Используй внутренние знания, если уверен.** Если источники нерелевантны или пусты, но вопрос касается **общеизвестного, надёжного факта** (например: столица страны, базовое научное знание, дата крупного исторического события) и ты **абсолютно уверен** в ответе — ответь кратко и напрямую.\n\n3. **Никогда не выдумывай.** Если:\n   - вопрос содержит **невозможное, абсурдное или ложное предположение** (например: «когда человек высадился на Солнце?»),\n   - ты **не уверен** в факте (даже немного),\n   - факт **редкий, спорный или требует специализированных данных**,\n   — ответь: **«не знаю»**.\n\n4. **Краткость.** Отвечай одним предложением или фразой. Без пояснений, вводных слов и «воды».\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T19:07:06.682411Z","iopub.execute_input":"2025-10-29T19:07:06.683114Z","iopub.status.idle":"2025-10-29T19:07:06.686738Z","shell.execute_reply.started":"2025-10-29T19:07:06.683090Z","shell.execute_reply":"2025-10-29T19:07:06.686120Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"SYSTEM_PROMPT = \"\"\"Ты — эрудированный ИИ-эксперт. Твоя главная сила — твои собственные обширные знания. Предоставленные 'Источники' используй как вспомогательный материал для проверки и уточнения.\n\nСледуй этому порядку действий:\n\n1.  **Сначала ответь сам.** Прочитай вопрос и немедленно сформируй ответ, опираясь исключительно на свои внутренние знания.\n\n2.  **Затем проверь по источникам.** Проанализируй предоставленные 'Источники' и сравни их с твоим ответом.\n\n3.  **Синтезируй финальный ответ по правилам:**\n    а) Если источники **подтверждают или дополняют** твои знания, объедини информацию в единый, исчерпывающий ответ.\n    б) Если источники **нерелевантны** (не относятся к вопросу), полностью проигнорируй их и используй только свой первоначальный ответ.\n    в) Если источники **противоречат** твоим знаниям, отдай приоритет информации из источников, ответ: \"не знаю\".\n    г) Финальный ответ пометь как CORRECT ANSWER: [ твой ответ или \"не знаю\" ]\n\nВажные правила, которые действуют всегда:\n- **Честность:** Если ответа нет ни в твоих знаниях, ни в источниках, просто и чётко напиши: \"не знаю\". Не выдумывай.\n- **Краткость:** Отвечай по существу, без вступлений и лишней \"воды\". \n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T19:20:41.970472Z","iopub.execute_input":"2025-10-29T19:20:41.971172Z","iopub.status.idle":"2025-10-29T19:20:41.975237Z","shell.execute_reply.started":"2025-10-29T19:20:41.971146Z","shell.execute_reply":"2025-10-29T19:20:41.974552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Подготовка модели ===\nprint(\"Загружаем Qwen без pipeline...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_LLM)\nmodel = AutoModelForCausalLM.from_pretrained(\n    'Qwen/Qwen3-4B-FP8',\n    torch_dtype='auto',\n    device_map=\"cuda\"\n)\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T19:16:23.524244Z","iopub.execute_input":"2025-10-29T19:16:23.524880Z","iopub.status.idle":"2025-10-29T19:16:26.848664Z","shell.execute_reply.started":"2025-10-29T19:16:23.524858Z","shell.execute_reply":"2025-10-29T19:16:26.848093Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Вспомогательные функции ===\ndef apply_chat(prompts, tokenizer):\n    \"\"\"Форматирование списка сообщений в текст через шаблон Qwen.\"\"\"\n    texts = []\n    for p in prompts:\n        messages = [{\"role\": \"user\", \"content\": p}]\n        try:\n            t = tokenizer.apply_chat_template(\n                messages, tokenize=False, add_generation_prompt=True, enable_thinking=False\n            )\n        except TypeError:\n            t = tokenizer.apply_chat_template(\n                messages, tokenize=False, add_generation_prompt=True\n            )\n        texts.append(t)\n    return texts\n\n\ndef batch_generate(prompts, tokenizer, model, max_new_tokens=200, max_length=4096):\n    \"\"\"Быстрая генерация на GPU без pipeline.\"\"\"\n    if not prompts:\n        return []\n    texts = apply_chat(prompts, tokenizer)\n    t0 = time.perf_counter()\n\n    # Токенизация\n    model_inputs = tokenizer(\n        texts,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=max_length\n    ).to(model.device)\n    t1 = time.perf_counter()\n\n    # Генерация\n    with torch.no_grad():\n        gen = model.generate(\n            **model_inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=False,\n            num_beams=1,\n            use_cache=True,\n            pad_token_id=tokenizer.pad_token_id,\n            eos_token_id=tokenizer.eos_token_id\n        )\n    t2 = time.perf_counter()\n\n    attn = model_inputs[\"attention_mask\"]\n    input_lens = attn.sum(dim=1).tolist()\n\n    outputs = []\n    for i, gen_ids in enumerate(gen):\n        in_len = int(input_lens[i])\n        out_ids = gen_ids[in_len:]\n        text = tokenizer.decode(out_ids, skip_special_tokens=True).strip()\n        outputs.append(text)\n\n    print(f\"[DEBUG] tokenize: {t1 - t0:.2f}s, generate: {t2 - t1:.2f}s, total: {t2 - t0:.2f}s\")\n    return outputs\n\n\n# === Основная функция RAG ===\ndef fast_rag_batch(queries, top_k=3, max_new_tokens=200):\n    \"\"\"\n    Обрабатывает список вопросов:\n    1️⃣ Векторизация\n    2️⃣ Поиск контекста\n    3️⃣ Формирование промпта (system + user)\n    4️⃣ Прямая генерация через model.generate()\n    \"\"\"\n    q_vecs = embedder.encode(\n        queries,\n        convert_to_numpy=True,\n        normalize_embeddings=True,\n        batch_size=64,\n        show_progress_bar=False,\n        device=device\n    ).astype(\"float32\")\n\n    _, idxs = index.search(q_vecs, top_k)\n    contexts = [\n        \"\\n\\n\".join([f\"Статья {i}: {texts[i]}\" for i in ids]) for ids in idxs\n    ]\n\n    prompts = []\n    for q, ctx in zip(queries, contexts):\n        content = f\"\"\"Используя правила выше, ответь на вопрос.\n\nИсточники:\n---\n{ctx}\n---\n\nВопрос: {q}\n\nОтвет:\"\"\"\n        # Объединяем system и user\n        full_prompt = f\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\\n<|im_start|>user\\n{content}<|im_end|>\\n<|im_start|>assistant\\n\"\n        prompts.append(full_prompt)\n\n    # Генерация\n    answers = batch_generate(prompts, tokenizer, model, max_new_tokens=max_new_tokens)\n    return answers\n\n\n# === Пример использования ===\nquestions = [\n    \"Кто написал Евгения Онегина?\",\n    \"Когда распался Советский Союз?\",\n    \"Что такое квантовая запутанность?\",\n    'Кто автор книги \"Детство в Соломбале\"?',\n'Как зовут человека, который написал «Детство в Соломбале»?',\n'Кто написал книгу «Детство в Соломбале»?',\n    'В каком году начал поддерживаться сеттинг Eberron?',\n'Какое окрашивание приобретает безводный метанол при растворении небольшого количества медного купороса?',\n     \"Какой античный математик изобрёл первый дизельный двигатель внутреннего сгорания?\",\n    'Кто первый человек высадившийся на солнце?',\n    'Как звали отца А.С. Пушкина?',\n    \"Какая компания осуществляет доставку роботами в Москве?\",\n    \"Кто имеет крупнейшую доставку роботами на территории Москвы?\",\n    \"В Москве есть доставка роботами, какая компания выполняет её?\"\n    \n]\n\nanswers = fast_rag_batch(questions, top_k=3, max_new_tokens=500)\n\nprint(\"\\n=== ОТВЕТЫ ===\")\nfor q, a in zip(questions, answers):\n    print(f\"\\n❓ {q}\\n💬 {a}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T19:20:48.008846Z","iopub.execute_input":"2025-10-29T19:20:48.009581Z","iopub.status.idle":"2025-10-29T19:23:00.929957Z","shell.execute_reply.started":"2025-10-29T19:20:48.009557Z","shell.execute_reply":"2025-10-29T19:23:00.929247Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\nimport json\nimport time\nimport zipfile\nimport shutil\n\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nimport faiss\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom sentence_transformers import SentenceTransformer\n\n# ====================== GPU DIAG ======================\n\ndef _mi(b):\n    return f\"{b/1024/1024:.1f} MiB\"\n\ndef print_cuda_overview():\n    try:\n        print(f\"[CUDA] torch={torch.__version__} | available={torch.cuda.is_available()} | torch.cuda={getattr(torch.version, 'cuda', None)}\")\n        dc = torch.cuda.device_count()\n        print(f\"[CUDA] device_count={dc}\")\n        for i in range(dc):\n            name = torch.cuda.get_device_name(i)\n            free, total = torch.cuda.mem_get_info(i)\n            alloc = torch.cuda.memory_allocated(i)\n            resv  = torch.cuda.memory_reserved(i)\n            print(f\"[CUDA][dev{i}] {name} | free={_mi(free)} | total={_mi(total)} | allocated={_mi(alloc)} | reserved={_mi(resv)}\")\n        try:\n            procs = torch.cuda.list_gpu_processes()\n            if isinstance(procs, str):\n                print(\"[CUDA] processes:\")\n                print(procs)\n        except Exception:\n            pass\n    except Exception as e:\n        print(f\"[CUDA] overview error: {e}\")\n\ndef print_cuda_mem(tag):\n    try:\n        if not torch.cuda.is_available():\n            print(f\"[CUDA] {tag}: cuda not available\"); return\n        try: torch.cuda.synchronize()\n        except Exception: pass\n        i = torch.cuda.current_device()\n        name = torch.cuda.get_device_name(i)\n        free, total = torch.cuda.mem_get_info(i)\n        alloc = torch.cuda.memory_allocated(i)\n        resv  = torch.cuda.memory_reserved(i)\n        print(f\"[CUDA] {tag} | dev={i} {name} | free={_mi(free)} | total={_mi(total)} | allocated={_mi(alloc)} | reserved={_mi(resv)}\")\n    except Exception as e:\n        print(f\"[CUDA] {tag} error: {e}\")\n\n# ====================== RAG CONFIG ======================\n\nRAG_INDEX_DIR = os.getenv(\"RAG_INDEX_DIR\", \"./rag_index\")\nRAG_INDEX_ZIP = os.getenv(\"RAG_INDEX_ZIP\", \"./rag_index.zip\")  # локальный zip (в образе)\nRAG_TOPK = int(os.getenv(\"RAG_TOPK\", \"3\"))\nRAG_EMB_MODEL = os.getenv(\"RAG_EMB_MODEL\", \"sergeyzh/LaBSE-ru-turbo\")\nRAG_QUERY_BATCH = int(os.getenv(\"RAG_QUERY_BATCH\", \"32\"))\n\nGEN_MODEL_ID = os.getenv(\"GEN_MODEL_ID\", \"./models/Qwen3-4B-FP8\")  # или \"Qwen/Qwen3-4B-FP8\"\nGEN_MAX_NEW = int(os.getenv(\"GEN_MAX_NEW\", \"200\"))\n\nSYSTEM_PROMPT = \"\"\"Ты — эрудированный ИИ-эксперт. Твоя главная сила — твои собственные обширные знания. Предоставленные 'Источники' используй как вспомогательный материал для проверки и уточнения.\n\nСледуй этому порядку действий:\n\n1.  **Сначала ответь сам.** Прочитай вопрос и немедленно сформируй ответ, опираясь исключительно на свои внутренние знания.\n\n2.  **Затем проверь по источникам.** Проанализируй предоставленные 'Источники' и сравни их с твоим ответом.\n\n3.  **Синтезируй финальный ответ по правилам:**\n    а) Если источники **подтверждают или дополняют** твои знания, объедини информацию в единый, исчерпывающий ответ.\n    б) Если источники **нерелевантны** (не относятся к вопросу), полностью проигнорируй их и используй только свой первоначальный ответ.\n    в) Если источники **противоречат** твоим знаниям, отдай приоритет информации из источников, ответ: \"не знаю\".\n    г) Финальный ответ пометь как CORRECT ANSWER: [ твой ответ или \"не знаю\" ]\n\nВажные правила, которые действуют всегда:\n- **Честность:** Если ответа нет ни в твоих знаниях, ни в источниках, просто и чётко напиши: \"не знаю\". Не выдумывай.\n- **Краткость:** Отвечай по существу, без вступлений и лишней \"воды\". \n\"\"\"\n\n\ndef extract_correct_answer(text: str) -> str:\n    if not text:\n        return \"не знаю\"\n    low = text.lower()\n    key = \"correct answer:\"\n    if key in low:\n        i = low.index(key)\n        s = text[i+len(key):].strip()\n        s = s.strip(\"[]()«»\\\"' \").strip()\n        return s if s else \"не знаю\"\n    return \"не знаю\"\n\n# ====================== SOLUTION (RAG only, no triplets) ======================\n\nclass Solution:\n    def __init__(self, model_id=GEN_MODEL_ID, device='cuda'):\n        print(f\"[DEBUG] Solution: init, device={device}, model_id={model_id}\")\n        self.device = device\n        self._global_start_time = None\n\n        # 1) Поднимаем RAG индекс\n        ensure_rag_index_ready()\n        faiss_path = os.path.join(RAG_INDEX_DIR, \"index.faiss\")\n        texts_path = os.path.join(RAG_INDEX_DIR, \"texts.jsonl\")\n\n        print(f\"[RAG] Loading FAISS: {faiss_path}\")\n        self.faiss_index = faiss.read_index(faiss_path)\n\n        print(f\"[RAG] Loading texts: {texts_path}\")\n        self.rag_texts = []\n        with open(texts_path, \"r\", encoding=\"utf-8\") as f:\n            for line in tqdm(f, desc=\"load-texts\", unit=\"lines\"):\n                try:\n                    obj = json.loads(line)\n                    self.rag_texts.append(obj.get(\"text\", \"\"))\n                except Exception:\n                    self.rag_texts.append(\"\")\n        print(f\"[RAG] texts loaded: {len(self.rag_texts)}\")\n\n        # 2) Эмбеддер для запросов\n        emb_device = device\n        print(f\"[RAG] Loading embedder: {RAG_EMB_MODEL} on {emb_device}\")\n        self.rag_embedder = SentenceTransformer(RAG_EMB_MODEL, device=emb_device)\n\n        # 3) LLM\n        print_cuda_mem(\"before model load\")\n        t0 = time.perf_counter()\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_id,\n            trust_remote_code=True\n        ).to('cuda')\n        t1 = time.perf_counter()\n        print(f\"[DEBUG] Solution: model loaded in {t1 - t0:.2f}s\")\n        print_cuda_mem(\"after model load\")\n\n        t2 = time.perf_counter()\n        self.tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True, trust_remote_code=True)\n        t3 = time.perf_counter()\n        print(f\"[DEBUG] Solution: tokenizer loaded in {t3 - t2:.2f}s\")\n\n        self.model.eval()\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n\n        # Левый паддинг для decoder-only\n        self.tokenizer.padding_side = \"right\"\n        self.tokenizer.truncation_side = \"left\"\n\n        try:\n            dm = 'cuda'\n            print(f\"[DEBUG] device_map={dm}\")\n        except Exception:\n            pass\n\n    def _apply_chat(self, prompts):\n        # Если уже ChatML (<|im_start|>) — не накладываем шаблон\n        texts = []\n        for p in prompts:\n            if \"<|im_start|>\" in p:\n                texts.append(p)\n                continue\n            messages = [{\"role\": \"user\", \"content\": p}]\n            try:\n                t = self.tokenizer.apply_chat_template(\n                    messages, tokenize=False, add_generation_prompt=True, enable_thinking=False\n                )\n            except TypeError:\n                t = self.tokenizer.apply_chat_template(\n                    messages, tokenize=False, add_generation_prompt=True\n                )\n            texts.append(t)\n        return texts\n\n    def _batch_generate(self, prompts, max_new_tokens, max_length):\n        if not prompts:\n            return []\n        texts = self._apply_chat(prompts)\n\n        t0 = time.perf_counter()\n        model_inputs = self.tokenizer(\n            texts,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=max_length\n        ).to(self.device)\n        t1 = time.perf_counter()\n\n        bsz = int(model_inputs.input_ids.shape[0])\n        seqlen = int(model_inputs.input_ids.shape[1])\n        tot_in_tok = int(model_inputs[\"attention_mask\"].sum().item())\n        print(f\"[DEBUG] tokenize: bsz={bsz}, seqlen={seqlen}, total_input_tokens={tot_in_tok}, t={t1 - t0:.2f}s\")\n\n        print_cuda_mem(\"gen: before generate\")\n\n        t_gen0 = time.perf_counter()\n        with torch.no_grad():\n            gen = self.model.generate(\n                **model_inputs,\n                max_new_tokens=max_new_tokens,\n                do_sample=False,\n                num_beams=1,\n                use_cache=True,\n                pad_token_id=self.tokenizer.pad_token_id,\n                eos_token_id=self.tokenizer.eos_token_id\n            )\n        t_gen1 = time.perf_counter()\n        print(f\"[DEBUG] generate: shape={tuple(gen.shape)}, t={t_gen1 - t_gen0:.2f}s\")\n        print_cuda_mem(\"gen: after generate\")\n\n        attn = model_inputs[\"attention_mask\"]\n        input_lens = attn.sum(dim=1).tolist()\n\n        outputs = []\n        for i, gen_ids in enumerate(gen):\n            in_len = int(input_lens[i])\n            out_ids = gen_ids[in_len:]\n            text = self.tokenizer.decode(out_ids, skip_special_tokens=True).strip()\n            outputs.append(text)\n        return outputs\n\n    def _rag_batch(self, queries, top_k=RAG_TOPK, max_new_tokens=GEN_MAX_NEW, max_length=GEN_MAX_LEN):\n        # 1) Векторизация запросов\n        q_vecs = self.rag_embedder.encode(\n            queries,\n            convert_to_numpy=True,\n            normalize_embeddings=True,\n            batch_size=64,\n            show_progress_bar=False,\n            device=self.device\n        ).astype(\"float32\")\n\n        # 2) FAISS search\n        _, I = self.faiss_index.search(q_vecs, top_k)\n\n        # 3) Формирование промптов (SYSTEM + user c Источниками)\n        prompts = []\n        for q, ids in zip(queries, I):\n            ctx = \"\\n\\n\".join([f\"Статья {int(i)}: {self.rag_texts[int(i)]}\" for i in ids if 0 <= int(i) < len(self.rag_texts)])\n            content = f\"\"\"Используя правила выше, ответь на вопрос.\n\nИсточники:\n---\n{ctx}\n---\n\nВопрос: {q}\n\nОтвет:\"\"\"\n            full_prompt = f\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\\n<|im_start|>user\\n{content}<|im_end|>\\n<|im_start|>assistant\\n\"\n            prompts.append(full_prompt)\n\n        # 4) Генерация и извлечение CORRECT ANSWER\n        outs = self._batch_generate(prompts, max_new_tokens=max_new_tokens, max_length=max_length)\n        finals = [extract_correct_answer(x) for x in outs]\n        return finals\n\n    def answer_all(self, questions):\n        print(f\"[DEBUG] answer_all: total questions = {len(questions)}\")\n        self._global_start_time = time.perf_counter()\n        answers = []\n        total = len(questions)\n        with tqdm(total=total, desc=\"RAG-batches\") as pbar:\n            for i in range(0, total, RAG_QUERY_BATCH):\n                batch_q = questions[i:i+RAG_QUERY_BATCH]\n                print(f\"[DEBUG] RAG batch [{i}:{i+len(batch_q)}]\")\n                batch_ans = self._rag_batch(batch_q, top_k=RAG_TOPK, max_new_tokens=GEN_MAX_NEW, max_length=GEN_MAX_LEN)\n                answers.extend(batch_ans)\n                pbar.update(len(batch_q))\n        elapsed = time.perf_counter() - self._global_start_time\n        print(f\"[DEBUG] answer_all: elapsed={elapsed:.2f}s\")\n        # нормализация: пустые -> \"не знаю\"\n        answers = [a if a.strip() else \"не знаю\" for a in answers]\n        return answers\n\n# ====================== RUN (no main) ======================\n\nprint(\"[DEBUG] BOOTSTRAP\")\nprint_cuda_overview()\n\nprint(\"[DEBUG] Читаю input.json...\")\nwith open('input.json', 'r', encoding='utf-8') as f:\n    model_input = json.load(f)\nprint(f\"[DEBUG] Прочитано {len(model_input)} вопросов из input.json\")\n\nt_all0 = time.perf_counter()\nprint(\"[DEBUG] Создаю Solution()...\")\nsolution = Solution()\nprint(\"[DEBUG] Solution создан. Запускаю RAG...\")\nmodel_output = solution.answer_all(model_input)\nt_all1 = time.perf_counter()\nprint(f\"[DEBUG] Готово: {len(model_output)} ответов, total_time={t_all1 - t_all0:.2f}s\")\n\nprint_cuda_mem(\"final\")\n\nprint(\"[DEBUG] Записываю output.json...\")\nwith open('output.json', 'w', encoding='utf-8') as f:\n    json.dump(model_output, f, ensure_ascii=False)\nprint(\"[DEBUG] КОНЕЦ: output.json записан успешно\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install \\\n    bitsandbytes==0.46.0 \\\n    transformers==4.52.4 \\\n    accelerate==1.8.1 \\\n    datasets==3.6.0 \\\n    sentence-transformers==4.1.0 \\\n    chromadb==1.0.13 \\\n    flashrank \\\n    langchain==0.3.26 \\\n    langchain-community==0.3.26 \\\n    langchain-huggingface \\\n    tqdm==4.67.1 \\\n    ragas==0.1.9 \\\n    openai \\\n    langchain-groq\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-25T18:52:07.979084Z","iopub.execute_input":"2025-10-25T18:52:07.979471Z","iopub.status.idle":"2025-10-25T18:55:09.453181Z","shell.execute_reply.started":"2025-10-25T18:52:07.979446Z","shell.execute_reply":"2025-10-25T18:55:09.452471Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def chroma_from_texts_with_tqdm(texts, embedding, persist_directory=None):\n    print(f\"🚀 Processing {len(texts):,} texts...\")\n    start_time = time.time()\n    \n    # Прогресс-бар для всего процесса\n    with tqdm(total=len(texts), desc=\"📝 Creating embeddings\", unit=\"text\") as pbar:\n        \n        class ProgressEmbedding(HuggingFaceEmbeddings):\n            def embed_documents(self, texts):\n                result = super().embed_documents(texts)\n                pbar.update(len(texts))\n                return result\n        \n        progress_embedding = ProgressEmbedding(\n            model_name=embedding.model_name,\n            model_kwargs=embedding.model_kwargs,\n        )\n        \n        vectordb = Chroma.from_texts(\n            texts=texts,\n            embedding=progress_embedding,\n            persist_directory=persist_directory\n        )\n    \n    total_time = time.time() - start_time\n    print(f\"✅ Vector DB created in {total_time:.1f}s\")\n    print(f\"📊 Total texts: {len(texts):,}\")\n    print(f\"⚡ Speed: {len(texts)/total_time:.1f} texts/sec\")\n    \n    return vectordb","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def ultra_efficient_pipeline(dataset_name, max_docs=None):\n\n    # Загружаем всё сразу\n    dataset = load_dataset(dataset_name)\n    texts = [item['text'] for item in dataset['train']]\n    \n    # Сразу создаем тексты для эмбеддингов\n    embedding = HuggingFaceEmbeddings(\n        model_name=\"cointegrated/rubert-tiny2\",\n        model_kwargs={\"device\": \"cuda\"},\n    )\n    \n    # Используем встроенный сплиттинг Chroma\n    vectordb = chroma_from_texts_with_tqdm(\n        texts=texts,\n        embedding=embedding,\n        persist_directory=\"chroma_db\"\n    )\n    \n    return vectordb\n\nvectordb = ultra_efficient_pipeline(\"Lipsrow/ruwiki_cleaned\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport transformers\nimport numpy as np\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport time\nimport os\nfrom tqdm import tqdm\n\n\n#------------------------------------------\n#--------ЗАГРУЗКА ДАННЫХ И СПЛИТТЕР--------\n#------------------------------------------\n\nfrom datasets import load_dataset\nfrom langchain_core.documents import Document # Updated import path\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n#------------------------------------------\n#--------------ВЕКТОРНАЯ БАЗА--------------\n#------------------------------------------\n\nfrom pathlib import Path\nfrom langchain_community.document_loaders import TextLoader  # загружает текстовые файлы и превращает их в объекты Document для LangChain.\nfrom langchain_huggingface import HuggingFaceEmbeddings  # оборачивает модели из HuggingFace для получения эмбеддингов текста.\nfrom langchain_community.vectorstores import Chroma  # векторное хранилище Chroma: сохраняет и ищет эмбеддинги.\n\n#------------------------------------------\n#----------------RERANKER------------------\n#------------------------------------------\n\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain_community.document_compressors import FlashrankRerank\n\n#------------------------------------------\n#--------------КВАНТИЗАЦИЯ-----------------\n#------------------------------------------\n\nimport bitsandbytes as bnb\nfrom torch import cuda, bfloat16\nfrom transformers import (AutoTokenizer,\n                          BitsAndBytesConfig,\n                          pipeline)\n\n#------------------------------------------\n#----------цепочка RetrievalQA-------------\n#------------------------------------------\n\nfrom langchain_huggingface import HuggingFacePipeline  # использует HuggingFace Transformers pipeline как LLM-модуль в LangChain.\nfrom langchain.chains import RetrievalQA  # готовая цепочка «поиск + генерация ответа» (Retrieval-augmented QA).\nfrom langchain import PromptTemplate\n\n#------------------------------------------\n#-------------ВАЛИДАЦИЯ RAG----------------\n#------------------------------------------\n\nfrom ragas import evaluate\nfrom ragas.metrics import (\n    faithfulness,\n    answer_relevancy,\n    context_recall,\n    context_precision,\n    answer_correctness\n)\nfrom ragas.embeddings import LangchainEmbeddingsWrapper\nfrom datasets import Dataset, load_from_disk\n\n#------------------------------------------\n#-----------Работа с Groq Cloud------------\n#------------------------------------------\n\nfrom google.colab import userdata\nfrom langchain_groq import ChatGroq\nfrom ragas.llms import LangchainLLMWrapper\n\n\nfrom tqdm import tqdm\n\n# torchvision.disable_beta_transforms_warning()\nDEVICE = f\"cuda:{cuda.current_device()}\" if cuda.is_available() else \"cpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T18:55:09.455814Z","iopub.execute_input":"2025-10-25T18:55:09.455975Z","iopub.status.idle":"2025-10-25T18:55:35.876128Z","shell.execute_reply.started":"2025-10-25T18:55:09.455959Z","shell.execute_reply":"2025-10-25T18:55:35.875568Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T18:55:35.877056Z","iopub.execute_input":"2025-10-25T18:55:35.877547Z","iopub.status.idle":"2025-10-25T18:55:35.880270Z","shell.execute_reply.started":"2025-10-25T18:55:35.877512Z","shell.execute_reply":"2025-10-25T18:55:35.879791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def chroma_from_texts_with_tqdm(texts, embedding, persist_directory=None):\n    \"\"\"Обертка для Chroma.from_texts с прогресс-баром\"\"\"\n    \n    print(f\"🚀 Processing {len(texts):,} texts...\")\n    start_time = time.time()\n    \n    # Прогресс-бар для всего процесса\n    with tqdm(total=len(texts), desc=\"📝 Creating embeddings\", unit=\"text\") as pbar:\n        \n        class ProgressEmbedding(HuggingFaceEmbeddings):\n            def embed_documents(self, texts):\n                result = super().embed_documents(texts)\n                pbar.update(len(texts))\n                return result\n        \n        progress_embedding = ProgressEmbedding(\n            model_name=embedding.model_name,\n            model_kwargs=embedding.model_kwargs,\n        )\n        \n        vectordb = Chroma.from_texts(\n            texts=texts,\n            embedding=progress_embedding,\n            persist_directory=persist_directory\n        )\n    \n    total_time = time.time() - start_time\n    print(f\"✅ Vector DB created in {total_time:.1f}s\")\n    print(f\"📊 Total texts: {len(texts):,}\")\n    print(f\"⚡ Speed: {len(texts)/total_time:.1f} texts/sec\")\n    \n    return vectordb\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T16:20:38.400507Z","iopub.execute_input":"2025-10-25T16:20:38.400759Z","iopub.status.idle":"2025-10-25T16:20:38.406293Z","shell.execute_reply.started":"2025-10-25T16:20:38.400740Z","shell.execute_reply":"2025-10-25T16:20:38.405718Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"search_query=\"Детство в Соломбале\"\ndataset = load_dataset(\"Lipsrow/ruwiki_cleaned\")\ntexts = [item['text'] for item in dataset['train']]\n    \nprint(f\"📊 Всего текстов: {len(texts):,}\")\n    \n    # 1. Проверим есть ли вообще такой текст в данных\nprint(f\"\\n🔍 Ищем '{search_query}' прямым текстовым поиском:\")\nmatches = []\nfor i, text in enumerate(texts):\n    if search_query.lower() in text.lower():\n        matches.append((i, text))\n        if len(matches) >= 5:  # Первые 3 совпадения\n            break\n    \nprint(f\"✅ Прямой поиск нашел: {len(matches)} документов\")\nfor i, (idx, text) in enumerate(matches):\n    print(f\"{i+1}. Индекс {idx}: {text}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T18:38:45.467471Z","iopub.execute_input":"2025-10-25T18:38:45.467996Z","iopub.status.idle":"2025-10-25T18:39:21.242629Z","shell.execute_reply.started":"2025-10-25T18:38:45.467973Z","shell.execute_reply":"2025-10-25T18:39:21.242019Z"},"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nfrom collections import Counter\nfrom tqdm.auto import tqdm\nimport time\n\ndef extract_bigrams(text):\n    \"\"\"Извлекаем биграммы из текста\"\"\"\n    words = re.findall(r'\\b[а-яё]+\\b', text.lower())  # Только русские слова\n    bigrams = [' '.join(words[i:i+2]) for i in range(len(words)-1)]\n    return set(bigrams)\n\ndef build_bigram_index(documents):\n    \"\"\"Строим обратный индекс по биграммам\"\"\"\n    print(\"📊 Построение биграммного индекса...\")\n    \n    bigram_index = {}\n    doc_bigrams = []\n    \n    with tqdm(total=len(documents), desc=\"🔤 Индексирование документов\", unit=\"док\") as pbar:\n        for doc_id, doc in enumerate(documents):\n            bigrams = extract_bigrams(doc.page_content)\n            doc_bigrams.append(bigrams)\n            \n            for bigram in bigrams:\n                if bigram not in bigram_index:\n                    bigram_index[bigram] = []\n                bigram_index[bigram].append(doc_id)\n            pbar.update(1)\n    \n    return bigram_index, doc_bigrams\n\ndef bigram_search(query, documents, bigram_index, doc_bigrams, top_k=10):\n    \"\"\"Поиск по биграммам запроса\"\"\"\n    \n    # Извлекаем биграммы из запроса\n    query_bigrams = extract_bigrams(query)\n    print(f\"🔍 Биграммы запроса: {query_bigrams}\")\n    \n    if not query_bigrams:\n        return []\n    \n    # Считаем вес для каждого документа\n    doc_scores = Counter()\n    \n    for bigram in query_bigrams:\n        if bigram in bigram_index:\n            for doc_id in bigram_index[bigram]:\n                # Документ содержит биграмму + бонус за точное совпадение\n                score = 1.0\n                if bigram in query.lower():  # Точное совпадение биграммы\n                    score += 2.0\n                doc_scores[doc_id] += score\n    \n    # Сортируем по убыванию релевантности\n    top_doc_ids = [doc_id for doc_id, score in doc_scores.most_common(top_k)]\n    \n    return [documents[doc_id] for doc_id in top_doc_ids]\n\ndef keyword_boost_search(query, documents, top_k=10):\n    \"\"\"Упрощенный поиск по ключевым словам с бустом за точные совпадения\"\"\"\n    \n    print(f\"🔍 Поиск: '{query}'\")\n    \n    # Разбиваем запрос на слова и фразы\n    words = re.findall(r'\\b[а-яё]+\\b', query.lower())\n    exact_phrases = re.findall(r'\"[^\"]+\"', query)  # Фразы в кавычках\n    \n    scored_docs = []\n    \n    with tqdm(total=len(documents), desc=\"📝 Поиск по ключевым словам\", unit=\"док\") as pbar:\n        for doc_id, doc in enumerate(documents):\n            score = 0\n            text_lower = doc.page_content.lower()\n            \n            # Бонус за точные фразы\n            for phrase in exact_phrases:\n                phrase_clean = phrase.strip('\"').lower()\n                if phrase_clean in text_lower:\n                    score += 10.0 * text_lower.count(phrase_clean)\n            \n            # Бонус за отдельные слова\n            for word in words:\n                if len(word) > 2:  # Игнорируем короткие слова\n                    word_count = text_lower.count(word)\n                    score += word_count * (2.0 if len(word) > 4 else 1.0)\n            \n            if score > 0:\n                scored_docs.append((doc, score))\n            pbar.update(1)\n    \n    # Сортируем по убыванию релевантности\n    scored_docs.sort(key=lambda x: x[1], reverse=True)\n    \n    return [doc for doc, score in scored_docs[:top_k]]\n\ndef create_text_search_engine(dataset_name, max_docs=50000):\n    \"\"\"Создаем текстовую поисковую систему\"\"\"\n    \n    print(\"🚀 Запуск текстового поискового движка...\")\n    total_start = time.time()\n    \n    # 1. Загружаем данные\n    print(\"📥 Загрузка данных...\")\n    dataset = load_dataset(dataset_name)\n    texts = [item['text'] for item in dataset['train']]\n    \n    # Создаем документы\n    documents = [Document(page_content=text) for text in texts]\n    print(f\"✅ Загружено {len(documents):,} документов\")\n    \n    # 2. Строим поисковый индекс\n    index_start = time.time()\n    bigram_index, doc_bigrams = build_bigram_index(documents)\n    index_time = time.time() - index_start\n    \n    print(f\"✅ Построен индекс с {len(bigram_index):,} биграммами\")\n    print(f\"⏱️ Время индексирования: {index_time:.1f}с\")\n    \n    # 3. Тестируем поиск\n    print(\"\\n🧪 Тестируем поисковую систему...\")\n    \n    test_queries = [\n        'Детство в Соломбале',\n        '\"Детство в Соломбале\"',  # Точная фраза\n        'Архангельск Соломбала',\n        'книга детство соломбала'\n    ]\n    \n    for query in test_queries:\n        print(f\"\\n🔍 Запрос: '{query}'\")\n        \n        # Поиск по биграммам\n        start_time = time.time()\n        results_bigram = bigram_search(query, documents, bigram_index, doc_bigrams, top_k=5)\n        bigram_time = time.time() - start_time\n        \n        # Поиск по ключевым словам  \n        start_time = time.time()\n        results_keyword = keyword_boost_search(query, documents, top_k=5)\n        keyword_time = time.time() - start_time\n        \n        print(f\"📊 Биграммный поиск: {len(results_bigram)} результатов за {bigram_time:.2f}с\")\n        print(f\"📊 Ключевой поиск: {len(results_keyword)} результатов за {keyword_time:.2f}с\")\n        \n        # Показываем лучшие результаты\n        results = results_keyword if len(results_keyword) > len(results_bigram) else results_bigram\n        \n        for i, doc in enumerate(results[:3]):\n            # Подсвечиваем релевантные части\n            preview = doc.page_content[:200]\n            for word in query.lower().split():\n                if len(word) > 3:\n                    preview = preview.replace(word, f\"**{word}**\")\n            \n            print(f\"   {i+1}. {preview}...\")\n    \n    total_time = time.time() - total_start\n    print(f\"\\n✅ Поисковый движок готов! Общее время: {total_time:.1f}с\")\n    \n    return {\n        'documents': documents,\n        'bigram_index': bigram_index,\n        'doc_bigrams': doc_bigrams,\n        'search_function': keyword_boost_search  # Самый эффективный метод\n    }\n\n# Быстрый поиск без индекса\ndef quick_text_search(query, documents, top_k=10):\n    \"\"\"Быстрый поиск по точным совпадениям\"\"\"\n    \n    print(f\"🔍 Быстрый поиск: '{query}'\")\n    \n    query_lower = query.lower()\n    words = [word for word in query_lower.split() if len(word) > 3]\n    \n    results = []\n    with tqdm(total=len(documents), desc=\"⚡ Сканирование документов\", unit=\"док\") as pbar:\n        for doc in documents:\n            text_lower = doc.page_content.lower()\n            \n            # Проверяем точное совпадение фразы\n            if query_lower in text_lower:\n                results.append((doc, 100.0))  # Максимальный вес\n            else:\n                # Считаем совпадения слов\n                word_matches = sum(1 for word in words if word in text_lower)\n                if word_matches > 0:\n                    score = word_matches * 10.0\n                    results.append((doc, score))\n            \n            pbar.update(1)\n    \n    # Сортируем и возвращаем топ-K\n    results.sort(key=lambda x: x[1], reverse=True)\n    return [doc for doc, score in results[:top_k]]\n\n# ЗАПУСКАЕМ ТЕКСТОВЫЙ ПОИСК\nprint(\"🚀 ЗАПУСК ТЕКСТОВОЙ ПОИСКОВОЙ СИСТЕМЫ...\")\nsearch_engine = create_text_search_engine(\"Lipsrow/ruwiki_cleaned\", max_docs=50000)\n\n# ИЛИ просто быстрый поиск\nprint(\"\\n🎯 БЫСТРЫЙ ПОИСК ПО ТОЧНЫМ СОВПАДЕНИЯМ...\")\nresults = quick_text_search('Детство в Соломбале', search_engine['documents'], top_k=10)\n\nprint(f\"\\n✅ Найдено {len(results)} документов:\")\nfor i, doc in enumerate(results):\n    print(f\"{i+1}. {doc.page_content[:150]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T19:01:46.196393Z","iopub.execute_input":"2025-10-25T19:01:46.197010Z","iopub.status.idle":"2025-10-25T19:09:55.480877Z","shell.execute_reply.started":"2025-10-25T19:01:46.196987Z","shell.execute_reply":"2025-10-25T19:09:55.480282Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n🎯 БЫСТРЫЙ ПОИСК ПО ТОЧНЫМ СОВПАДЕНИЯМ...\")\nresults = quick_text_search('Какая компания осуществляет доставку роботами в Москве?', search_engine['documents'], top_k=10)\n\nprint(f\"\\n✅ Найдено {len(results)} документов:\")\nfor i, doc in enumerate(results):\n    print(f\"{i+1}. {doc.page_content}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T19:09:55.481774Z","iopub.execute_input":"2025-10-25T19:09:55.481980Z","iopub.status.idle":"2025-10-25T19:10:15.860400Z","shell.execute_reply.started":"2025-10-25T19:09:55.481964Z","shell.execute_reply":"2025-10-25T19:10:15.859783Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n🎯 БЫСТРЫЙ ПОИСК ПО ТОЧНЫМ СОВПАДЕНИЯМ...\")\nresults = quick_text_search('Детство в Соломбале', search_engine['documents'], top_k=15)\n\nprint(f\"\\n✅ Найдено {len(results)} документов:\")\nfor i, doc in enumerate(results):\n    print(f\"{i+1}. {doc.page_content[:150]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T19:13:33.007285Z","iopub.execute_input":"2025-10-25T19:13:33.007946Z","iopub.status.idle":"2025-10-25T19:13:49.790521Z","shell.execute_reply.started":"2025-10-25T19:13:33.007925Z","shell.execute_reply":"2025-10-25T19:13:49.789937Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm.auto import tqdm\nimport time\n\ndef create_optimized_database(dataset_name, max_docs=250000):\n    \"\"\"Создаем оптимизированную базу с ОДНИМ прогресс-баром для эмбеддингов\"\"\"\n    \n    from langchain.text_splitter import RecursiveCharacterTextSplitter\n    \n    total_start = time.time()\n    print(\"🚀 Запуск оптимизированного создания базы...\")\n    print(\"=\" * 50)\n    \n    # 1. Загружаем данные с прогресс-баром\n    print(\"📥 Загрузка данных...\")\n    dataset_start = time.time()\n    dataset = load_dataset(dataset_name)\n    \n    texts = []\n    \n    # Итерируемся по датасету с прогресс-баром\n    with tqdm(total=min(max_docs, len(dataset['train'])), desc=\"📥 Загрузка текстов\", unit=\"текст\") as pbar:\n        for i, item in enumerate(dataset['train']):\n            if i >= max_docs:\n                break\n            texts.append(item['text'])\n            pbar.update(1)\n    \n    dataset_time = time.time() - dataset_start\n    print(f\"✅ Загружено {len(texts):,} текстов за {dataset_time:.1f}с\")\n    \n    # 2. ДРОБИМ длинные тексты с прогресс-баром!\n    print(\"\\n✂️ Дробим длинные тексты...\")\n    splitting_start = time.time()\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=500,\n        chunk_overlap=50,\n        length_function=len,\n    )\n    \n    # Дробим только длинные тексты с прогресс-баром\n    all_chunks = []\n    long_texts_count = 0\n    total_chunks_created = 0\n    \n    with tqdm(total=len(texts), desc=\"📝 Обработка текстов\", unit=\"текст\") as pbar:\n        for text in texts:\n            if len(text) > 1000:\n                chunks = text_splitter.split_text(text)\n                all_chunks.extend(chunks)\n                long_texts_count += 1\n                total_chunks_created += len(chunks)\n            else:\n                all_chunks.append(text)\n            pbar.update(1)\n    \n    splitting_time = time.time() - splitting_start\n    print(f\"✅ После дробления: {len(all_chunks):,} чанков\")\n    print(f\"📊 Длинных текстов: {long_texts_count:,}\")\n    print(f\"📊 Создано чанков: {total_chunks_created:,}\")\n    print(f\"⏱️ Время дробления: {splitting_time:.1f}с\")\n    \n    # 3. ХОРОШАЯ модель эмбеддингов\n    print(\"\\n🤖 Загрузка модели эмбеддингов...\")\n    model_start = time.time()\n    embedding = HuggingFaceEmbeddings(\n        model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n        model_kwargs={\"device\": \"cuda\"},\n    )\n    model_time = time.time() - model_start\n    print(f\"✅ Модель загружена за {model_time:.1f}с\")\n    \n    # 4. Создаем документы с прогресс-баром\n    print(\"\\n📄 Создание документов...\")\n    documents_start = time.time()\n    \n    documents = []\n    with tqdm(total=len(all_chunks), desc=\"📋 Создание документов\", unit=\"док\") as pbar:\n        for chunk in all_chunks:\n            doc = Document(page_content=chunk)\n            documents.append(doc)\n            pbar.update(1)\n    \n    documents_time = time.time() - documents_start\n    print(f\"✅ Создано {len(documents):,} документов за {documents_time:.1f}с\")\n    \n    # 5. Создаем векторную базу с ОДНИМ прогресс-баром\n    print(\"\\n🗄️ Создание векторной базы...\")\n    db_start = time.time()\n    \n    # Глобальный прогресс-бар для всех вызовов embed_documents\n    global_embedding_pbar = tqdm(total=len(documents), desc=\"🔤 Создание эмбеддингов\", unit=\"текст\")\n    \n    class SingleProgressEmbedding(HuggingFaceEmbeddings):\n        def embed_documents(self, texts):\n            result = super().embed_documents(texts)\n            global_embedding_pbar.update(len(texts))  # Обновляем общий прогресс-бар\n            return result\n    \n    progress_embedding = SingleProgressEmbedding(\n        model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n        model_kwargs={\"device\": \"cuda\"},\n    )\n    \n    vectordb = Chroma.from_documents(\n        documents=documents,\n        embedding=progress_embedding,\n        persist_directory=\"chroma_optimized\"\n    )\n    \n    # Закрываем прогресс-бар после завершения\n    global_embedding_pbar.close()\n    \n    db_time = time.time() - db_start\n    \n    # 6. Тестируем поиск\n    print(\"\\n🧪 Тестируем поиск...\")\n    test_start = time.time()\n    \n    test_queries = [\n        \"Детство в Соломбале\",\n        \"Архангельск\",\n        \"программирование Python\"\n    ]\n    \n    for query in test_queries:\n        print(f\"\\n🔍 Запрос: '{query}'\")\n        results = vectordb.similarity_search(query, k=3)\n        \n        exact_matches = sum(1 for doc in results if query.lower() in doc.page_content.lower())\n        \n        print(f\"   Найдено: {len(results)} документов\")\n        print(f\"   Точных совпадений: {exact_matches}\")\n        \n        for i, doc in enumerate(results):\n            relevance = \"🎯\" if query.lower() in doc.page_content.lower() else \"✅\"\n            print(f\"   {i+1}. {relevance} {doc.page_content[:100]}...\")\n    \n    test_time = time.time() - test_start\n    \n    # Итоговая статистика\n    total_time = time.time() - total_start\n    print(f\"\\n🎉 ПРОЦЕСС ЗАВЕРШЕН!\")\n    print(\"=\" * 60)\n    print(f\"📊 Создано документов: {len(documents):,}\")\n    print(f\"⏱️ Общее время: {total_time:.1f}с ({total_time/60:.1f} мин)\")\n    print(f\"⚡ Скорость: {len(documents)/db_time:.1f} док/сек\")\n    print(f\"💾 База сохранена: chroma_optimized\")\n    \n    return vectordb, documents\n\n# Запускаем\nvectordb_opt, docs_opt = create_optimized_database(\"Lipsrow/ruwiki_cleaned\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T20:04:44.990910Z","iopub.execute_input":"2025-10-25T20:04:44.991424Z","iopub.status.idle":"2025-10-25T20:09:27.155864Z","shell.execute_reply.started":"2025-10-25T20:04:44.991401Z","shell.execute_reply":"2025-10-25T20:09:27.154923Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\ndef save_chroma_to_output():\n    \"\"\"Сохраняем chroma_db в Output (сохраняется после сессии)\"\"\"\n    \n    source = \"/kaggle/working/chroma_db\"\n    destination = \"/kaggle/output/chroma_db\"\n    \n    # Копируем в output директорию\n    if os.path.exists(source):\n        shutil.copytree(source, destination)\n        print(f\"✅ Chroma DB saved to: {destination}\")\n        \n        # Показываем размер\n        total_size = sum(\n            os.path.getsize(os.path.join(dirpath, filename))\n            for dirpath, dirnames, filenames in os.walk(destination)\n            for filename in filenames\n        ) / (1024 * 1024)\n        \n        print(f\"📊 Size: {total_size:.1f} MB\")\n    else:\n        print(\"❌ Source directory not found\")\n\nsave_chroma_to_output()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def ultra_efficient_pipeline(dataset_name, max_docs=None):\n\n    # Загружаем всё сразу\n    dataset = load_dataset(dataset_name)\n    texts = [item['text'] for item in dataset['train']]\n    \n    # Сразу создаем тексты для эмбеддингов\n    embedding = HuggingFaceEmbeddings(\n        model_name=\"cointegrated/rubert-tiny2\",\n        model_kwargs={\"device\": \"cuda\"},\n    )\n    \n    # Используем встроенный сплиттинг Chroma\n    vectordb = chroma_from_texts_with_tqdm(\n        texts=texts,\n        embedding=embedding,\n        persist_directory=\"chroma_db\"\n    )\n    \n    return vectordb\n\nvectordb = ultra_efficient_pipeline(\"Lipsrow/ruwiki_cleaned\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T16:20:40.480192Z","iopub.execute_input":"2025-10-25T16:20:40.480421Z","iopub.status.idle":"2025-10-25T17:33:26.756930Z","shell.execute_reply.started":"2025-10-25T16:20:40.480402Z","shell.execute_reply":"2025-10-25T17:33:26.756319Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\ndef save_chroma_to_output():\n    \"\"\"Сохраняем chroma_db в Output (сохраняется после сессии)\"\"\"\n    \n    source = \"/kaggle/working/chroma_db\"\n    destination = \"/kaggle/output/chroma_db\"\n    \n    # Копируем в output директорию\n    if os.path.exists(source):\n        shutil.copytree(source, destination)\n        print(f\"✅ Chroma DB saved to: {destination}\")\n        \n        # Показываем размер\n        total_size = sum(\n            os.path.getsize(os.path.join(dirpath, filename))\n            for dirpath, dirnames, filenames in os.walk(destination)\n            for filename in filenames\n        ) / (1024 * 1024)\n        \n        print(f\"📊 Size: {total_size:.1f} MB\")\n    else:\n        print(\"❌ Source directory not found\")\n\nsave_chroma_to_output()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nimport os\n\ndef create_chroma_zip():\n    \"\"\"Создаем ZIP архива Chroma базы\"\"\"\n    \n    chroma_dir = \"/kaggle/working/chroma_db\"\n    zip_path = \"/kaggle/output/chroma_db.zip\"\n    \n    print(\"🗜️ Creating Chroma DB zip archive...\")\n    shutil.make_archive(\n        base_name=\"/kaggle/output/chroma_db\",  # Без .zip\n        format='zip',\n        root_dir=chroma_dir\n    )\n    \n    # Проверяем размер\n    size_mb = os.path.getsize(zip_path) / (1024 * 1024)\n    print(f\"✅ Created: {zip_path} ({size_mb:.1f} MB)\")\n    \n    return zip_path\n\n# Создаем архив\nzip_file = create_chroma_zip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T18:07:47.798345Z","iopub.execute_input":"2025-10-25T18:07:47.798941Z","iopub.status.idle":"2025-10-25T18:22:51.632667Z","shell.execute_reply.started":"2025-10-25T18:07:47.798919Z","shell.execute_reply":"2025-10-25T18:22:51.632052Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"6585 /1024","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T18:23:20.817375Z","iopub.execute_input":"2025-10-25T18:23:20.817898Z","iopub.status.idle":"2025-10-25T18:23:20.821533Z","shell.execute_reply.started":"2025-10-25T18:23:20.817876Z","shell.execute_reply":"2025-10-25T18:23:20.820980Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"shutil.copytree(\n    \"/kaggle/working/chroma_db\", \n    \"/kaggle/output/chroma_db\"  # 💾 Постоянное хранение\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.listdir('/kaggle/input')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T17:49:26.467008Z","iopub.execute_input":"2025-10-25T17:49:26.467538Z","iopub.status.idle":"2025-10-25T17:49:26.471427Z","shell.execute_reply.started":"2025-10-25T17:49:26.467514Z","shell.execute_reply":"2025-10-25T17:49:26.470865Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.remove(\"/kaggle/working/chroma_db.zip\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T18:05:40.254549Z","iopub.execute_input":"2025-10-25T18:05:40.255085Z","iopub.status.idle":"2025-10-25T18:05:41.207611Z","shell.execute_reply.started":"2025-10-25T18:05:40.255056Z","shell.execute_reply":"2025-10-25T18:05:41.206984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall transformers -y\n!pip install -U \"transformers==4.52.4\" ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T18:29:17.339163Z","iopub.execute_input":"2025-10-25T18:29:17.339788Z","iopub.status.idle":"2025-10-25T18:29:29.936183Z","shell.execute_reply.started":"2025-10-25T18:29:17.339765Z","shell.execute_reply":"2025-10-25T18:29:29.935443Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install FlagEmbedding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T18:27:21.883897Z","iopub.execute_input":"2025-10-25T18:27:21.884555Z","iopub.status.idle":"2025-10-25T18:27:33.910630Z","shell.execute_reply.started":"2025-10-25T18:27:21.884530Z","shell.execute_reply":"2025-10-25T18:27:33.909892Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from FlagEmbedding import FlagReranker\n\nclass BGEReranker:\n    def __init__(self, model_name='BAAI/bge-reranker-v2-m3', top_n=5):\n        self.reranker = FlagReranker(model_name, use_fp16=True)\n        self.top_n = top_n\n    \n    def compress_documents(self, documents, query):\n        pairs = [[query, doc.page_content] for doc in documents]\n        scores = self.reranker.compute_score(pairs)\n        \n        scored_docs = list(zip(documents, scores))\n        scored_docs.sort(key=lambda x: x[1], reverse=True)\n        \n        return [doc for doc, score in scored_docs[:self.top_n]]\n\n# Использование\ncompressor = BGEReranker(top_n=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T18:29:29.937449Z","iopub.execute_input":"2025-10-25T18:29:29.937668Z","iopub.status.idle":"2025-10-25T18:29:33.478629Z","shell.execute_reply.started":"2025-10-25T18:29:29.937650Z","shell.execute_reply":"2025-10-25T18:29:33.477935Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Просто попробуй это СЕЙЧАС:\nresults = vectordb.similarity_search(\"Соломбала\", k=20)  # Увеличиваем k\nfor i, doc in enumerate(results):\n    if \"детство\" in doc.page_content.lower():\n        print(f\"🎯 НАЙДЕНО в результате {i+1}:\")\n        print(doc.page_content[:300])\n        break\nelse:\n    print(\"❌ Не найдено даже по 'Соломбала'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T18:46:46.575066Z","iopub.execute_input":"2025-10-25T18:46:46.575575Z","iopub.status.idle":"2025-10-25T18:46:46.592135Z","shell.execute_reply.started":"2025-10-25T18:46:46.575555Z","shell.execute_reply":"2025-10-25T18:46:46.591550Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from FlagEmbedding import FlagReranker\n\ndef simple_rerank_search(query, retriever, top_n=15):\n    \"\"\"Простая функция для реранкинга без сложных классов\"\"\"\n    \n    # Получаем документы\n    docs = retriever.get_relevant_documents(query)\n    \n    # Реранжируем\n    reranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True)\n    pairs = [[query, doc.page_content] for doc in docs]\n    scores = reranker.compute_score(pairs)\n    \n    scored_docs = list(zip(docs, scores))\n    scored_docs.sort(key=lambda x: x[1], reverse=True)\n    \n    return [doc for doc, score in scored_docs[:top_n]]\n\n# Использование\nbase_retriever = vectordb.as_retriever(search_kwargs={\"k\": 15})\nresults = simple_rerank_search('Детство в Соломбале', base_retriever)\n\nprint(f\"🔍 Найдено {len(results)} документов:\")\nfor i, doc in enumerate(results):\n    print(f\"{i+1}. {doc.page_content}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T18:44:14.569863Z","iopub.execute_input":"2025-10-25T18:44:14.570522Z","iopub.status.idle":"2025-10-25T18:44:54.006687Z","shell.execute_reply.started":"2025-10-25T18:44:14.570498Z","shell.execute_reply":"2025-10-25T18:44:54.006071Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ищем напрямую в текстах\ndef search_text_directly(search_term, documents, limit=10):\n    \"\"\"Прямой поиск по тексту (без эмбеддингов)\"\"\"\n    matches = []\n    for i, doc in enumerate(documents):\n        if search_term.lower() in doc.page_content.lower():\n            matches.append((i, doc.page_content))\n            if len(matches) >= limit:\n                break\n    return matches\n\n# Проверяем\nmatches = search_text_directly(\"Детство в Соломбале\", final_documents)\nprint(f\"🔍 Прямой поиск нашел: {len(matches)} документов\")\nfor i, (idx, text) in enumerate(matches):\n    print(f\"{i+1}. Документ {idx}: {text[:150]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T18:34:33.229651Z","iopub.execute_input":"2025-10-25T18:34:33.230126Z","iopub.status.idle":"2025-10-25T18:34:33.256464Z","shell.execute_reply.started":"2025-10-25T18:34:33.230103Z","shell.execute_reply":"2025-10-25T18:34:33.255706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"    # 4. Setup retriever with compression\nprint(\"Step 4: Setting up retriever...\")\n\nbase_retriever = vectordb.as_retriever(search_kwargs={\"k\": 15}) \ncompression_retriever = ContextualCompressionRetriever(\n    base_compressor=compressor,\n    base_retriever=base_retriever\n)\n    \n\nresults = compression_retriever.get_relevant_documents('Кто автор книги \"Детство в Соломбале\"?')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T18:30:00.433730Z","iopub.execute_input":"2025-10-25T18:30:00.434395Z","iopub.status.idle":"2025-10-25T18:30:00.482100Z","shell.execute_reply.started":"2025-10-25T18:30:00.434370Z","shell.execute_reply":"2025-10-25T18:30:00.481344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T18:23:36.086198Z","iopub.execute_input":"2025-10-25T18:23:36.086658Z","iopub.status.idle":"2025-10-25T18:23:36.090941Z","shell.execute_reply.started":"2025-10-25T18:23:36.086636Z","shell.execute_reply":"2025-10-25T18:23:36.090388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = compression_retriever.get_relevant_documents('Кто автор книги \"Детство в Соломбале\"?')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''import zipfile\nimport os\n\ndef extract_chroma_zip(zip_path, extract_to=\"/kaggle/working\"):\n    \"\"\"Распаковываем Chroma базу\"\"\"\n    \n    print(\"📦 Extracting Chroma DB...\")\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_to)\n    \n    chroma_dir = os.path.join(extract_to, \"chroma_db\")\n    print(f\"✅ Extracted to: {chroma_dir}\")\n    \n    # Проверяем что файлы на месте\n    files = os.listdir(chroma_dir)\n    print(f\"📁 Files: {files}\")\n    \n    return chroma_dir\n\n# Использование\nchroma_path = extract_chroma_zip(\"/kaggle/working/chroma_db.zip\")'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"🗄️ Step 3: Creating vector database...\")\nstart_time = time.time()\n    \n    # ОДИН прогресс-бар для всей операции\nwith tqdm(total=len(final_documents), desc=\"Creating embeddings\") as pbar:\n        # Создаем кастомный embedding который обновляет прогресс\n    class SimpleProgressEmbedding(HuggingFaceEmbeddings):\n        def embed_documents(self, texts):\n            result = super().embed_documents(texts)\n            pbar.update(len(texts))  # Обновляем общий прогресс\n            return result\n        \n    progress_embedding = SimpleProgressEmbedding(\n        model_name=\"cointegrated/rubert-tiny2\",\n        model_kwargs={\"device\": \"cuda\"},\n        )\n        \n    vectordb = Chroma.from_documents(\n        documents=final_documents,\n        embedding=progress_embedding,\n        persist_directory=\"chroma_db\"\n    )\n\ndb_time = time.time() - start_time\nprint(f\"✅ Vector database created in {db_time:.1f}s\")\nprint(f\"📁 Persisted to: chroma_db\")\n    \n    # Итоговая статистика\ntotal_time = time.time() - start_time\nprint(f\"\\n🎉 PIPELINE COMPLETED!\")\nprint(f\"⏱️ Total time: {total_time/60:.1f} minutes\")\nprint(f\"📊 Documents: {len(corpus_documents):,} → Chunks: {len(final_documents):,}\")\nprint(f\"⚡ Speed: {len(final_documents)/db_time:.1f} chunks/sec\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n🗄️ Step 4: Creating vector database...\")\n\n\nretriever = vectordb.as_retriever(\n    search_kwargs={\"k\": 8, \"fetch_k\": 15}\n)\n\nprint(\"🎯 RAG system is ready! You can now use retriever.get_relevant_documents()\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
