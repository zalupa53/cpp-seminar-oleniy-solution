{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91496,"databundleVersionId":11802066,"sourceType":"competition"},{"sourceId":13207212,"sourceType":"datasetVersion","datasetId":8370683}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nimport cv2 # OpenCV для работы с изображениями\nimport re\nimport requests\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torchvision.models import resnet18, ResNet18_Weights # Наша предобученная модель\n\nfrom collections import defaultdict # Удобный словарь, который не выдает ошибку, если ключа нет\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-01T17:57:50.637361Z","iopub.execute_input":"2025-10-01T17:57:50.637520Z","iopub.status.idle":"2025-10-01T17:58:01.349335Z","shell.execute_reply.started":"2025-10-01T17:57:50.637505Z","shell.execute_reply":"2025-10-01T17:58:01.348748Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"#https://www.kaggle.com/code/ayberkural/autoprice-ai-uncovering-car-value-insights","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Рекомендуем запускать код в среде выполения с cuda\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T17:58:08.307788Z","iopub.execute_input":"2025-10-01T17:58:08.308103Z","iopub.status.idle":"2025-10-01T17:58:08.373096Z","shell.execute_reply.started":"2025-10-01T17:58:08.308079Z","shell.execute_reply":"2025-10-01T17:58:08.372458Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def get_direct_file_link(mailru_file_url: str) -> str:\n    \"\"\"\n    Превращает ссылку на конкретный файл внутри папки cloud.mail.ru/public/XXX/YYYY/filename\n    в прямую ссылку на CDN.\n    \"\"\"\n    resp = requests.get(mailru_file_url)\n    if resp.status_code != 200:\n        raise RuntimeError(f\"Ошибка {resp.status_code} при запросе {mailru_file_url}\")\n    page = resp.text\n\n    match = re.search(r'dispatcher.*?weblink_get.*?url\":\"(.*?)\"', page)\n    if not match:\n        raise RuntimeError(\"Не удалось найти CDN ссылку в HTML\")\n    base_url = match.group(1)\n\n    # вычленяем /XXX/YYYY/filename\n    parts = mailru_file_url.split('/')[-3:]\n    return f\"{base_url}/{parts[0]}/{parts[1]}/{parts[2]}\"\n\ndef download_from_mailru(file_url: str, local_name: str):\n    direct = get_direct_file_link(file_url)\n    print(f\"Скачиваем {file_url} → {local_name}\")\n    os.system(f\"wget --content-disposition '{direct}' -O '{local_name}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T17:58:18.916224Z","iopub.execute_input":"2025-10-01T17:58:18.916633Z","iopub.status.idle":"2025-10-01T17:58:18.925698Z","shell.execute_reply.started":"2025-10-01T17:58:18.916597Z","shell.execute_reply":"2025-10-01T17:58:18.924689Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Ссылки на архивы с картинками\ntrain_link = \"https://cloud.mail.ru/public/2kaD/W4xWY9vgr/train_images.zip\"\ntest_link  = \"https://cloud.mail.ru/public/2kaD/W4xWY9vgr/test_images.zip\"\n\ntrain_zip = download_from_mailru(train_link, \"train_images.zip\")\ntest_zip  = download_from_mailru(test_link,  \"test_images.zip\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Распаковываем архивы в соответствующие папки\n\n!unzip -q train_images.zip -d train_images\n!unzip -q test_images.zip -d test_images","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TRAIN_PQ = \"/kaggle/input/task-2-autoprice-price-prediction-based-on-photos/train_dataset.parquet\"\nTEST_PQ  = \"/kaggle/input/task-2-autoprice-price-prediction-based-on-photos/test_dataset.parquet\"\nTRAIN_IMG_DIR = \"train_images\"\nTEST_IMG_DIR  = \"test_images\"\n\nTRAIN_CSV = \"train.csv\"\nTEST_CSV  = \"test.csv\"\nCKPT_NAME = \"resne18_IMG192\"\nCKPT_PATH = f\"{CKPT_NAME}.pth\"\nUSE_LOG_TARGET = True # Используем логарифмический маштаб таргета","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-01T17:57:08.013Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Гиперпараметры\n\nIMG_SIZE = 192         # Приводим все картинки к размеру 192x192\nN_IMAGES_PER_ITEM = 1  # Для бейзлайна берем только 1 картинку на объявление\nBATCH_SIZE = 16        # Сколько картинок обрабатывать за один шаг\nEPOCHS = 5             # Сколько раз прогонять все обучающие данные\nLR = 1e-4              # Скорость обучения (learning rate)\nWEIGHT_DECAY = 1e-5    # Параметр регуляризации для предотвращения переобучения\nVAL_SIZE = 0.2         # Какую долю данных отложить для валидации (20%)\nKEEP_RATIO = 1     # Для ускорения экспериментов можем взять не все данные, а только 50%\n\n# Фиксируем \"зерно\" случайности, чтобы наши эксперименты были повторяемыми\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-01T17:57:08.013Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Загружаем табличные данные\ntrain_df = pd.read_parquet(TRAIN_PQ)\ntest_df  = pd.read_parquet(TEST_PQ)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T17:25:50.327294Z","iopub.execute_input":"2025-10-01T17:25:50.327568Z","iopub.status.idle":"2025-10-01T17:25:50.897765Z","shell.execute_reply.started":"2025-10-01T17:25:50.327549Z","shell.execute_reply":"2025-10-01T17:25:50.897129Z"}},"outputs":[],"execution_count":135},{"cell_type":"code","source":"y = model.predict(test[feature_columns])\n\ns = pd.read_csv('/kaggle/input/task-2-autoprice-price-prediction-based-on-photos/sample_submission.csv')\n\ns.target = np.exp(y)\ns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T17:24:35.012651Z","iopub.execute_input":"2025-10-01T17:24:35.012964Z","iopub.status.idle":"2025-10-01T17:24:35.770524Z","shell.execute_reply.started":"2025-10-01T17:24:35.012938Z","shell.execute_reply":"2025-10-01T17:24:35.769669Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max(a)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"a = []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for r in train_df.itertuples(index=False):\n        iid = getattr(r, \"ID\")\n        paths = sorted(id_to_files.get(iid, []))\n        a.append(len(paths))\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_img_csv(df, img_dir, out_csv, n_images=1, keep_ratio=1.0, seed=42):\n\n    \"\"\"\n    Функция для создания CSV-файла, связывающего ID объявления с путями к его фотографиям.\n    \"\"\"\n\n    all_files = os.listdir(img_dir)\n    id_to_files = defaultdict(list)\n    for fname in all_files:\n        if fname.endswith(\".jpg\"):\n            iid = int(fname.split(\"_\")[0])  # предполагаем формат ID_xxx.jpg\n            id_to_files[iid].append(os.path.join(img_dir, fname))\n\n    rows = []\n    for r in df.itertuples(index=False):\n        iid = getattr(r, \"ID\")\n        paths = sorted(id_to_files.get(iid, []))\n        if len(paths) > n_images:\n            paths = paths[:n_images]\n\n        row = {\n            \"item_id\": iid,\n            \"paths\": \";\".join(paths)\n        }\n\n        # цена есть только в train\n        if \"price_TARGET\" in df.columns:\n            row[\"price\"] = getattr(r, \"price_TARGET\")\n        else:\n            row[\"price\"] = -1  # заглушка для test\n\n        rows.append(row)\n\n    # Создаем и сохраняем итоговый DataFrame\n    csv_df = pd.DataFrame(rows)\n\n    # Если нужно, оставляем только часть данных для быстрых экспериментов\n    if keep_ratio < 1.0:\n        csv_df = csv_df.sample(frac=keep_ratio, random_state=seed).reset_index(drop=True)\n\n    csv_df.to_csv(out_csv, index=False)\n    print(f\"CSV сохранён: {out_csv}, shape={csv_df.shape}\")\n    return csv_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Запускаем функцию для train и test выборок\n\ntrain_csv_df = build_img_csv(train_df, TRAIN_IMG_DIR, TRAIN_CSV)\ntest_csv_df  = build_img_csv(test_df,  TEST_IMG_DIR,  TEST_CSV)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Реализуем функцию для подсчета medianAPE в точности как в правилах соревнования\n\ndef median_absolute_percentage_error(y_true, y_pred):\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    mask = y_true > 0\n    ape = np.abs(y_true[mask] - y_pred[mask]) / (y_true[mask] + 1e-6)\n    return float(np.median(ape))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Создаем конвейер преобразований для ТРЕНИРОВОЧНЫХ данных\n\ntrain_tfms = A.Compose([\n    # Шаги предобработки (обязательные)\n    A.LongestMaxSize(IMG_SIZE), # Уменьшаем картинку по длинной стороне до IMG_SIZE\n    A.PadIfNeeded(IMG_SIZE, IMG_SIZE, border_mode=cv2.BORDER_CONSTANT), # Добавляем поля до квадрата\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), # Нормализуем \"магическими\" числами ImageNet\n\n    # Шаги аугментации (случайные)\n    A.HorizontalFlip(p=0.5), # Отражаем по горизонтали с вероятностью 50%\n    A.RandomBrightnessContrast(p=0.3), # Меняем яркость и контраст\n    A.ShiftScaleRotate(shift_limit=0.02, scale_limit=0.1, rotate_limit=10,\n                       border_mode=cv2.BORDER_CONSTANT, p=0.5), # Сдвигаем, масштабируем и поворачиваем\n\n    # Финальный шаг предобработки\n    ToTensorV2(), # Превращаем numpy-массив в тензор PyTorch\n])\n\n# Создаем конвейер преобразований для ВАЛИДАЦИОННЫХ данных (только обязательные шаги)\nvalid_tfms = A.Compose([\n    A.LongestMaxSize(IMG_SIZE),\n    A.PadIfNeeded(IMG_SIZE, IMG_SIZE, border_mode=cv2.BORDER_CONSTANT),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2(),\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def _safe_imread(p: str, fallback_hw=(IMG_SIZE, IMG_SIZE)) -> np.ndarray:\n    \"\"\"Безопасно читает картинку. Если файл битый или не существует, возвращает черный квадрат.\"\"\"\n    img = cv2.imread(p, cv2.IMREAD_COLOR)\n    if img is None:\n        # Если картинка не прочиталась, создаем \"пустышку\"\n        h, w = fallback_hw\n        img = np.zeros((h, w, 3), dtype=np.uint8)\n    else:\n        # OpenCV читает картинки в формате BGR, а нам нужен RGB\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return img\n\n\nclass CarsDataset(Dataset):\n    \"\"\"\n    Класс датасета для PyTorch.\n    \"\"\"\n    def __init__(self, df: pd.DataFrame, is_train: bool = True):\n        self.df = df.reset_index(drop=True)\n        # Выбираем нужный набор аугментаций в зависимости от того, обучающий это датасет или нет\n        self.tfms = train_tfms if is_train else valid_tfms\n        self.has_target = \"price\" in df.columns\n\n    def __len__(self):\n        \"\"\"Возвращает общее количество объектов в датасете.\"\"\"\n        return len(self.df)\n\n    def __getitem__(self, idx: int):\n        \"\"\"По индексу `idx` возвращает один готовый для обучения пример.\"\"\"\n        row = self.df.iloc[idx]\n        # Берем пути к картинкам из строки и очищаем от возможных пустых значений\n        paths = [p for p in str(row[\"paths\"]).split(\";\") if p and p.lower() != \"nan\"][:N_IMAGES_PER_ITEM]\n\n        imgs = []\n        for p in paths:\n            img = _safe_imread(p) # Читаем картинку\n            img = self.tfms(image=img)[\"image\"].contiguous() # Применяем аугментации\n            imgs.append(img)\n\n        # Если картинок меньше, чем N_IMAGES_PER_ITEM, добиваем \"пустыми\" тензорами\n        while len(imgs) < N_IMAGES_PER_ITEM:\n            imgs.append(torch.zeros(3, IMG_SIZE, IMG_SIZE))\n\n        # Собираем список тензоров в один тензор формы (N, C, H, W)\n        imgs = torch.stack(imgs, dim=0)\n\n        # Если это обучающие данные, возвращаем картинки и цену\n        if self.has_target:\n            y = torch.tensor(row[\"price\"], dtype=torch.float32)\n            # Если включен флаг, берем логарифм от цены. Это стабилизирует обучение.\n            if USE_LOG_TARGET:\n                y = torch.log1p(y)\n            return imgs, y\n        # Если это тестовые данные, возвращаем картинки и ID объявления\n        else:\n            return imgs, torch.tensor(row.get(\"item_id\", -1), dtype=torch.long)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"\n    Вспомогательная функция для DataLoader. Она умеет правильно собирать\n    пары (картинка, цена) или (картинка, ID) в один батч.\n    \"\"\"\n    imgs = torch.stack([b[0] for b in batch], dim=0)  # (B, N, C, H, W)\n    target = batch[0][1]\n\n    # Если таргет - это число с плавающей точкой (цена), собираем тензор цен\n    if torch.is_tensor(target) and target.dtype.is_floating_point:\n        y = torch.stack([b[1] for b in batch], dim=0)  # (B,)\n        return imgs, y\n    # Иначе это ID, собираем тензор ID\n    else:\n        ids = torch.stack([b[1] for b in batch], dim=0)\n        return imgs, ids\n\nclass ResNetRegressor(nn.Module):\n    \"\"\"\n    Наша модель: \"тушка\" ResNet18 + \"голова\" для регрессии.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        # Загружаем ResNet18 с весами, предобученными на ImageNet\n        backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n        in_features = backbone.fc.in_features # Узнаем размер эмбеддинга (у ResNet18 это 512)\n        # \"Отрезаем\" последний слой (fc)\n        self.backbone = nn.Sequential(*list(backbone.children())[:-1])\n        # Создаем нашу \"голову\" - линейный слой, который из 512 признаков делает 1 число (цену)\n\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # x имеет форму (Batch, N_images, Channels, Height, Width)\n        B, N, C, H, W = x.shape\n        # \"Распрямляем\" батч, чтобы обработать все картинки одним прогоном\n        x = x.view(B*N, C, H, W)\n        # Получаем эмбеддинги (векторы признаков)\n        feats = self.backbone(x)          # (B*N, 512, 1, 1)\n        # Возвращаем эмбеддингам форму, связанную с батчем\n        feats = feats.view(B, N, -1)      # (B, N, 512)\n        # Усредняем эмбеддинги по всем картинкам одного объявления\n        feats = feats.mean(dim=1)\n        # Прогоняем усредненный эмбеддинг через \"голову\"\n        out = self.head(feats)            # (B, 1)\n        return out.squeeze(1)             # (B,) - возвращаем вектор предсказаний","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_epoch(model, loader, optimizer=None, device=\"cuda\"):\n    \"\"\"Прогоняет одну эпоху обучения или валидации.\"\"\"\n    is_train = optimizer is not None  # Если передан optimizer - это обучение\n    model.train(is_train) # Переключаем модель в режим train или eval\n\n    losses = []\n    preds_log_all, y_log_all = [], []\n\n    # tqdm оборачивает итератор, чтобы показывать красивую полоску прогресса\n    pbar = tqdm(loader, desc=\"Train\" if is_train else \"Valid\", leave=False)\n    for imgs, y_log in pbar:\n        # Переносим данные на GPU\n        imgs = imgs.to(device, non_blocking=True)\n        y_log = y_log.view(-1).to(device)\n\n        # Получаем предсказания модели\n        preds_log = model(imgs)\n        # Считаем ошибку. L1Loss (MAE) хорошо работает для регрессии, особенно с логарифмом.\n        loss = nn.L1Loss()(preds_log, y_log)\n\n        if is_train:\n            optimizer.zero_grad(set_to_none=True) # Обнуляем градиенты\n            loss.backward()                       # Считаем градиенты\n            optimizer.step()                      # Обновляем веса модели\n\n        # Собираем статистику\n        losses.append(loss.item())\n        preds_log_all.append(preds_log.detach().cpu())\n        y_log_all.append(y_log.detach().cpu())\n        pbar.set_postfix({\"loss\": f\"{loss.item():.3f}\"}) # Показываем текущую ошибку в прогресс-баре\n\n    # Считаем итоговые метрики за всю эпоху\n    preds_log_all = torch.cat(preds_log_all).numpy()\n    y_log_all     = torch.cat(y_log_all).numpy()\n\n    # Если использовали логарифм, возвращаем предсказания и таргет в исходный масштаб\n    preds  = np.expm1(preds_log_all)\n    y_true = np.expm1(y_log_all)\n\n    medape = median_absolute_percentage_error(y_true, preds) * 100.0\n    return float(medape), float(np.mean(losses))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = {\n    \"epoch\": [],\n    \"tr_loss\": [],\n    \"tr_medape\": [],\n    \"val_loss\": [],\n    \"val_medape\": [],\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(TRAIN_CSV)\ntrn_df, val_df = train_test_split(df, test_size=VAL_SIZE, random_state=SEED, shuffle=True)\n\ntrain_ds = CarsDataset(trn_df, is_train=True)\nvalid_ds = CarsDataset(val_df, is_train=False)\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n                          pin_memory=True, num_workers=4, drop_last=True,\n                          collate_fn=collate_fn)\nvalid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False,\n                          pin_memory=True, num_workers=4, drop_last=False,\n                          collate_fn=collate_fn)\n\nmodel = ResNetRegressor().to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n\nbest_val_medape = float(\"inf\")\n\n# Главный цикл обучения\nfor epoch in range(1, EPOCHS + 1):\n    print(f\"\\nЭпоха {epoch}/{EPOCHS}\")\n    # Запускаем эпоху обучения\n    tr_medape, tr_loss = run_epoch(model, train_loader, optimizer, device=device)\n    # Запускаем эпоху валидации (optimizer=None)\n    val_medape, val_loss = run_epoch(model, valid_loader, optimizer=None, device=device)\n\n    print(f\"Train loss: {tr_loss:.3f} | medianAPE: {tr_medape:.2f}%\")\n    print(f\"Valid loss: {val_loss:.3f} | medianAPE: {val_medape:.2f}%\")\n\n    # Сохраняем историю\n    history[\"epoch\"].append(epoch)\n    history[\"tr_loss\"].append(tr_loss)\n    history[\"tr_medape\"].append(tr_medape)\n    history[\"val_loss\"].append(val_loss)\n    history[\"val_medape\"].append(val_medape)\n\n    # Сохраняем модель (чекпоинт), только если она показала лучший результат на валидации\n    if val_medape < best_val_medape:\n        best_val_medape = val_medape\n        torch.save({\n            \"model\": model.state_dict(),\n            \"optimizer\": optimizer.state_dict(),\n            \"epoch\": epoch,\n            \"val_medape\": val_medape\n        }, CKPT_PATH)\n        print(f\"✓ Модель сохранена в эпоху {epoch} -> {CKPT_PATH}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Визуализация истории обучения\n\nplt.figure()\nplt.plot(history[\"epoch\"], history[\"tr_medape\"], label=\"train medAPE\")\nplt.plot(history[\"epoch\"], history[\"val_medape\"], label=\"valid medAPE\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Median APE, %\")\nplt.title(\"Median APE vs Epoch\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.savefig(f\"medape_vs_epoch_{CKPT_NAME}.png\", dpi=150)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Сохраняем историю в CSV для дальнейшего анализа\n\ncsv_name = f\"{CKPT_NAME}.csv\"\ndf_hist = pd.DataFrame(history)\ndf_hist.to_csv(csv_name, index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n#cb\nimport numpy as np\n\ndef create_dummy_columns(df):\n    \"\"\"\n    Создает dummy-колонки для колонок со списками строк,\n    игнорируя значения None в списках.\n    \n    Parameters:\n    df (pd.DataFrame): Исходный DataFrame\n    \n    Returns:\n    pd.DataFrame: DataFrame с добавленными dummy-колонками\n    \"\"\"\n    df_result = df.copy()\n    d_cols= []\n    for column in df.columns:\n        # Проверяем, что в колонке есть списки\n        if df[column].apply(lambda x: isinstance(x, np.ndarray) ).any() or column in ['aktivnaya_bezopasnost_mult',\n'audiosistema_mult',\n'shini_i_diski_mult',\n'electroprivod_mult',\n'fary_mult',\n'multimedia_navigacia_mult',\n'obogrev_mult',\n'pamyat_nastroek_mult',\n'podushki_bezopasnosti_mult',\n'pomosh_pri_vozhdenii_mult',\n'protivoygonnaya_sistema_mult',\n'salon_mult',\n'upravlenie_klimatom_mult']:\n            print(column)\n            \n            # Собираем все уникальные значения из всех списков (исключая None)\n            unique_values = set()\n            for item_list in df[column]:\n                if isinstance(item_list, np.ndarray):\n                    for item in item_list:\n                        if item is not None:\n                            unique_values.add(item)\n            \n            # Создаем dummy-колонки для каждого уникального значения\n            for value in unique_values:\n                dummy_col_name = f\"{column}_{value}\"\n                \n                # Создаем dummy-колонку: 1 если значение есть в списке, 0 если нет\n                df_result[dummy_col_name] = df[column].apply(\n                    lambda x: 1 if isinstance(x, np.ndarray) and value in x else 0\n                )\n            d_cols.append(column)\n    df_result.drop(columns = d_cols,inplace = True)\n    \n    return df_result\n\ndf = df.fillna('No')\n\ndf = create_dummy_columns(train_df)\n\ntest_df = test_df.fillna('No')\n\ntest = create_dummy_columns(test_df)\nfrom catboost import CatBoostRegressor\ndf['price_TARGET'] = np.log(df['price_TARGET'])\n\ncat_features = ['owners_count', 'equipment', 'body_type', 'drive_type', 'engine_type',\n       'doors_number', 'color', 'pts', 'audiosistema', 'diski',\n       'electropodemniki', 'fary', 'salon', 'upravlenie_klimatom',\n       'usilitel_rul', 'steering_wheel','crashes_count',\n       'aktivnaya_bezopasnost_mult_Курсовая устойчивость',\n       'aktivnaya_bezopasnost_mult_Блок. дифференциала',\n       'aktivnaya_bezopasnost_mult_Экстренное торможение',\n       'aktivnaya_bezopasnost_mult_Распред. тормозных усилий',\n       'aktivnaya_bezopasnost_mult_Антипробуксовка',\n       'aktivnaya_bezopasnost_mult_Обнаружение пешеходов',\n       'aktivnaya_bezopasnost_mult_Антиблокировка тормозов',\n       'audiosistema_mult_Cабвуфер',\n       'shini_i_diski_mult_Зимние шины в комплекте',\n       'electroprivod_mult_Задних сидений',\n       'electroprivod_mult_Передних сидений', 'electroprivod_mult_Зеркал',\n       'electroprivod_mult_Складывания зеркал',\n       'electroprivod_mult_Рулевой колонки', 'fary_mult_Противотуманные',\n       'fary_mult_Адаптивное освещение', 'fary_mult_Омыватели фар',\n       'multimedia_navigacia_mult_MP3',\n       'multimedia_navigacia_mult_Управление на руле',\n       'multimedia_navigacia_mult_TV', 'multimedia_navigacia_mult_CD привод',\n       'multimedia_navigacia_mult_Радио',\n       'multimedia_navigacia_mult_GPS-навигатор',\n       'multimedia_navigacia_mult_Экран', 'multimedia_navigacia_mult_USB',\n       'multimedia_navigacia_mult_Bluetooth', 'multimedia_navigacia_mult_AUX',\n       'obogrev_mult_Заднего стекла', 'obogrev_mult_Задних сидений',\n       'obogrev_mult_Руля', 'obogrev_mult_Передних сидений',\n       'obogrev_mult_Зеркал', 'pamyat_nastroek_mult_Задних сидений',\n       'pamyat_nastroek_mult_Сиденья водителя',\n       'pamyat_nastroek_mult_Рулевой колонки', 'pamyat_nastroek_mult_Зеркал',\n       'podushki_bezopasnosti_mult_Фронтальная для водителя',\n       'podushki_bezopasnosti_mult_Шторки',\n       'podushki_bezopasnosti_mult_Боковые передние',\n       'podushki_bezopasnosti_mult_Боковые задние',\n       'podushki_bezopasnosti_mult_Коленные',\n       'pomosh_pri_vozhdenii_mult_Контроль слепых зон',\n       'pomosh_pri_vozhdenii_mult_Парктроник задний',\n       'pomosh_pri_vozhdenii_mult_Монохромный экран бортового компьютера',\n       'pomosh_pri_vozhdenii_mult_Автопарковщик',\n       'pomosh_pri_vozhdenii_mult_Камера заднего вида',\n       'pomosh_pri_vozhdenii_mult_Круиз-контроль',\n       'pomosh_pri_vozhdenii_mult_Датчик света',\n       'pomosh_pri_vozhdenii_mult_Парктроник передний',\n       'pomosh_pri_vozhdenii_mult_Датчик дождя',\n       'protivoygonnaya_sistema_mult_Центральный замок',\n       'protivoygonnaya_sistema_mult_Сигнализация',\n       'protivoygonnaya_sistema_mult_Спутник',\n       'protivoygonnaya_sistema_mult_Иммобилайзер', 'salon_mult_Люк',\n       'salon_mult_Кожаный руль',\n       'upravlenie_klimatom_mult_Атермальное остекление',\n       'upravlenie_klimatom_mult_Управление на руле']\n\nfeature_columns = [ 'equipment', 'body_type', 'drive_type', 'engine_type',\n       'doors_number', 'color', 'pts', 'audiosistema', 'diski',\n       'electropodemniki', 'fary', 'salon', 'upravlenie_klimatom',\n       'usilitel_rul', 'steering_wheel', 'crashes_count', 'owners_count',\n       'mileage', 'latitude', 'longitude',\n       'aktivnaya_bezopasnost_mult_Курсовая устойчивость',\n       'aktivnaya_bezopasnost_mult_Блок. дифференциала',\n       'aktivnaya_bezopasnost_mult_Экстренное торможение',\n       'aktivnaya_bezopasnost_mult_Распред. тормозных усилий',\n       'aktivnaya_bezopasnost_mult_Антипробуксовка',\n       'aktivnaya_bezopasnost_mult_Обнаружение пешеходов',\n       'aktivnaya_bezopasnost_mult_Антиблокировка тормозов',\n       'audiosistema_mult_Cабвуфер',\n       'shini_i_diski_mult_Зимние шины в комплекте',\n       'electroprivod_mult_Задних сидений',\n       'electroprivod_mult_Передних сидений', 'electroprivod_mult_Зеркал',\n       'electroprivod_mult_Складывания зеркал',\n       'electroprivod_mult_Рулевой колонки', 'fary_mult_Противотуманные',\n       'fary_mult_Адаптивное освещение', 'fary_mult_Омыватели фар',\n       'multimedia_navigacia_mult_MP3',\n       'multimedia_navigacia_mult_Управление на руле',\n       'multimedia_navigacia_mult_TV', 'multimedia_navigacia_mult_CD привод',\n       'multimedia_navigacia_mult_Радио',\n       'multimedia_navigacia_mult_GPS-навигатор',\n       'multimedia_navigacia_mult_Экран', 'multimedia_navigacia_mult_USB',\n       'multimedia_navigacia_mult_Bluetooth', 'multimedia_navigacia_mult_AUX',\n       'obogrev_mult_Заднего стекла', 'obogrev_mult_Задних сидений',\n       'obogrev_mult_Руля', 'obogrev_mult_Передних сидений',\n       'obogrev_mult_Зеркал', 'pamyat_nastroek_mult_Задних сидений',\n       'pamyat_nastroek_mult_Сиденья водителя',\n       'pamyat_nastroek_mult_Рулевой колонки', 'pamyat_nastroek_mult_Зеркал',\n       'podushki_bezopasnosti_mult_Фронтальная для водителя',\n       'podushki_bezopasnosti_mult_Шторки',\n       'podushki_bezopasnosti_mult_Боковые передние',\n       'podushki_bezopasnosti_mult_Боковые задние',\n       'podushki_bezopasnosti_mult_Коленные',\n       'pomosh_pri_vozhdenii_mult_Контроль слепых зон',\n       'pomosh_pri_vozhdenii_mult_Парктроник задний',\n       'pomosh_pri_vozhdenii_mult_Монохромный экран бортового компьютера',\n       'pomosh_pri_vozhdenii_mult_Автопарковщик',\n       'pomosh_pri_vozhdenii_mult_Камера заднего вида',\n       'pomosh_pri_vozhdenii_mult_Круиз-контроль',\n       'pomosh_pri_vozhdenii_mult_Датчик света',\n       'pomosh_pri_vozhdenii_mult_Парктроник передний',\n       'pomosh_pri_vozhdenii_mult_Датчик дождя',\n       'protivoygonnaya_sistema_mult_Центральный замок',\n       'protivoygonnaya_sistema_mult_Сигнализация',\n       'protivoygonnaya_sistema_mult_Спутник',\n       'protivoygonnaya_sistema_mult_Иммобилайзер', 'salon_mult_Люк',\n       'salon_mult_Кожаный руль',\n       'upravlenie_klimatom_mult_Атермальное остекление',\n       'upravlenie_klimatom_mult_Управление на руле']\n\ntarget_column = 'price_TARGET'\n\ndef preprocess_data(df, target_column):\n    \"\"\"\n    Предобработка данных: обработка None и пропущенных значений\n    \"\"\"\n    df_processed = df.copy()\n    \n    # Заменяем None на np.nan\n    df_processed = df_processed.replace([None, 'None', 'null', 'NULL'], np.nan)\n    \n    # Для числовых колонок заполняем медианой\n    numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n    for col in numeric_cols:\n        if col != target_column:  # Не заполняем таргет\n            df_processed[col] = df_processed[col].fillna(df_processed[col].median())\n    \n    # Для категориальных колонок заполняем модой\n    categorical_cols = df_processed.select_dtypes(include=['object']).columns\n    for col in categorical_cols:\n        df_processed[col] = df_processed[col].fillna(df_processed[col].mode()[0] if not df_processed[col].mode().empty else 'missing')\n    \n    # Для таргета удаляем строки с пропусками\n    if target_column in df_processed.columns:\n        df_processed = df_processed.dropna(subset=[target_column])\n    \n    return df_processed\ndf = preprocess_data(df, target_column)\n\ntest = preprocess_data(test, target_column)\n\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_absolute_percentage_error\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef time_based_train_test_split(df, test_size=0.1):\n    \n    # Определяем индекс разделения\n    split_idx = int(len(df) * (1 - test_size))\n    \n    # Разделяем данные\n    train_df = df.iloc[:split_idx]\n    test_df = df.iloc[split_idx:]\n    \n    return train_df, test_df\n\n\ntrain_df, test_df = time_based_train_test_split(df, 0.1)\n    \nprint(f\"Train size: {len(train_df)}\")\nprint(f\"Test size: {len(test_df)}\")\n    \nX_train = train_df[feature_columns]\ny_train = train_df[target_column]\nX_test = test_df[feature_columns]\ny_test = test_df[target_column]\n    \n    # Инициализация CatBoost с оптимизацией MAPE\nmodel = CatBoostRegressor(\n        loss_function='MAPE',  # Оптимизируем MAPE напрямую\n        iterations=2000,\n        random_state=42,\n        verbose=100,  # Выводим прогресс каждые 100 итераций\n        cat_features=cat_features  # Автоматическое определение категориальных features\n)\n    \nmodel.fit(\n        X_train, y_train,\n        eval_set=(X_test, y_test),\n        early_stopping_rounds=50,\n        use_best_model=True\n)\n    \n    # Предсказания\ny_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)\n    \n    # Метрики\nmape_train = mean_absolute_percentage_error(y_train, y_pred_train) * 100\nmape_test = mean_absolute_percentage_error(y_test, y_pred_test) * 100\n    \nprint(f\"\\nРезультаты:\")\nprint(f\"MAPE на train: {mape_train:.2f}%\")\nprint(f\"MAPE на test: {mape_test:.2f}%\")\n    \n\nmape_train = mean_absolute_percentage_error(np.exp(y_train), np.exp(y_pred_train))* 100\nmape_test = mean_absolute_percentage_error(np.exp(y_test), np.exp(y_pred_test)) * 100\n    \nprint(f\"\\nРезультаты:\")\nprint(f\"MAPE на train: {mape_train:.2f}%\")\nprint(f\"MAPE на test: {mape_test:.2f}%\")\n    \n\nX_train = df[feature_columns]\ny_train = df[target_column]\n    \n    # Инициализация CatBoost с оптимизацией MAPE\nmodel = CatBoostRegressor(\n        loss_function='MAPE',  # Оптимизируем MAPE напрямую\n        iterations=1500,\n        random_state=42,\n        verbose=100,  # Выводим прогресс каждые 100 итераций\n        cat_features=cat_features  # Автоматическое определение категориальных features\n)\n    \nmodel.fit(\n        X_train, y_train,\n        early_stopping_rounds=50,\n        use_best_model=True\n)\n    \n\ndef median_ape(y_true, y_pred):\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    mask = y_true > 0\n    ape = np.abs(y_true[mask] - y_pred[mask]) / (y_true[mask] + 1e-6)\n    return float(np.median(ape))\n\n1 / (1 + 0.44)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T17:39:26.867904Z","iopub.execute_input":"2025-10-01T17:39:26.868837Z","iopub.status.idle":"2025-10-01T17:39:26.875277Z","shell.execute_reply.started":"2025-10-01T17:39:26.868805Z","shell.execute_reply":"2025-10-01T17:39:26.874319Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Инференс на тестовых данных\n\ntest_df = pd.read_csv(TEST_CSV)\ntest_ds = CarsDataset(test_df, is_train=False)\ntest_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n                         num_workers=8, pin_memory=True, drop_last=False,\n                         collate_fn=collate_fn)\n\n# Загружаем лучшую сохраненную модель\nckpt = torch.load(CKPT_PATH, map_location=device)\nmodel.load_state_dict(ckpt[\"model\"])\nmodel.eval() # Переводим модель в режим предсказания\n\npreds_all, ids_all = [], []\n# torch.no_grad() отключает расчет градиентов, что ускоряет инференс и экономит память\nwith torch.no_grad():\n    for imgs, ids in tqdm(test_loader, desc=\"Test\", leave=False):\n        imgs = imgs.to(device, non_blocking=True)\n        preds = model(imgs) # Получаем предсказания в лог-масштабе\n\n        # Если нужно, возвращаем в исходный масштаб\n        if USE_LOG_TARGET:\n            preds = torch.expm1(preds)\n\n        preds_all.append(preds.cpu())\n        ids_all.append(ids.cpu())\n\n# Собираем все предсказания и ID в единые массивы\npreds_all = torch.cat(preds_all).squeeze().numpy()\nids_all   = torch.cat(ids_all).squeeze().numpy()\n\n# Формирование файла для сабмита\nsubmission_df = pd.DataFrame({\n    \"ID\": ids_all,\n    \"price_TARGET\": preds_all\n})\n\nsubmission_df.to_csv(\"submission.csv\", index=False)\nsubmission_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Убедимся, что наш файл имеет ту же структуру и размер, что и образец\n# Это простой, но очень полезный санити-чек\n\nsample = pd.read_csv(\"sample_submission.csv\")\nassert list(submission_df.columns) == [\"ID\", \"target\"]\nassert len(submission_df) == len(sample)\n\nprint(\"Размер сабмита:\", submission_df.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
