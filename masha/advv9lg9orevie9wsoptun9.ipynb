{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13395226,"sourceType":"datasetVersion","datasetId":8500280},{"sourceId":13436221,"sourceType":"datasetVersion","datasetId":8528352}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#add optuna","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall transformers -y\n!pip install -U \"transformers==4.56.0\" ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T17:51:45.496775Z","iopub.execute_input":"2025-11-10T17:51:45.497057Z","iopub.status.idle":"2025-11-10T17:52:02.405476Z","shell.execute_reply.started":"2025-11-10T17:51:45.497036Z","shell.execute_reply":"2025-11-10T17:52:02.404416Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install h3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T17:52:02.407205Z","iopub.execute_input":"2025-11-10T17:52:02.407569Z","iopub.status.idle":"2025-11-10T17:52:05.924242Z","shell.execute_reply.started":"2025-11-10T17:52:02.407547Z","shell.execute_reply":"2025-11-10T17:52:05.923459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T17:52:05.925130Z","iopub.execute_input":"2025-11-10T17:52:05.925500Z","iopub.status.idle":"2025-11-10T17:52:15.489493Z","shell.execute_reply.started":"2025-11-10T17:52:05.925476Z","shell.execute_reply":"2025-11-10T17:52:15.488795Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport os\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.decomposition import PCA\n\nimport re\nimport time\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T17:52:15.491425Z","iopub.execute_input":"2025-11-10T17:52:15.491786Z","iopub.status.idle":"2025-11-10T17:52:16.328952Z","shell.execute_reply.started":"2025-11-10T17:52:15.491768Z","shell.execute_reply":"2025-11-10T17:52:16.328375Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from h3.api import basic_str as h3\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntqdm.pandas()\n\ntrain_data = pd.read_csv(\"/kaggle/input/cnewwwww/train.tsv\", sep=\"\\t\")\ntest_data = pd.read_csv(\"/kaggle/input/cnewwwww/test.tsv\", sep=\"\\t\")\nreviews_data = pd.read_csv(\"/kaggle/input/cnewwwww/reviews.tsv\", sep=\"\\t\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T17:52:16.329611Z","iopub.execute_input":"2025-11-10T17:52:16.329972Z","iopub.status.idle":"2025-11-10T17:52:23.342200Z","shell.execute_reply.started":"2025-11-10T17:52:16.329953Z","shell.execute_reply":"2025-11-10T17:52:23.341343Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.isna().sum().sort_values()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T17:52:23.343107Z","iopub.execute_input":"2025-11-10T17:52:23.343336Z","iopub.status.idle":"2025-11-10T17:52:23.402890Z","shell.execute_reply.started":"2025-11-10T17:52:23.343319Z","shell.execute_reply":"2025-11-10T17:52:23.402186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = train_data[train_data['target'] >= 1]\n\ndef extract_coords(coord_str, coord_type):\n    comma_pos = coord_str.find(\",\")\n    if coord_type == \"lng\":\n        return float(coord_str[1:comma_pos])\n    elif coord_type == \"lat\":\n        return float(coord_str[comma_pos+2:-1])\n\ntrain_data['lng'] = train_data['coordinates'].apply(lambda x: extract_coords(x, \"lng\"))\ntrain_data['lat'] = train_data['coordinates'].apply(lambda x: extract_coords(x, \"lat\"))\ntest_data['lng'] = test_data['coordinates'].apply(lambda x: extract_coords(x, \"lng\"))\ntest_data['lat'] = test_data['coordinates'].apply(lambda x: extract_coords(x, \"lat\"))\n\nfor df in (train_data, test_data):\n    df[\"lat_rad\"] = np.radians(df[\"lat\"])\n    df[\"lng_rad\"] = np.radians(df[\"lng\"])\n    df[\"sin_lat\"] = np.sin(df[\"lat_rad\"])\n    df[\"cos_lat\"] = np.cos(df[\"lat_rad\"])\n    df[\"sin_lng\"] = np.sin(df[\"lng_rad\"])\n    df[\"cos_lng\"] = np.cos(df[\"lng_rad\"])\n    df[\"lat_lng_ratio\"] = df[\"lat\"] / df[\"lng\"]\n    df[\"lng_lat_ratio\"] = df[\"lng\"] / df[\"lat\"]\n    df[\"coord_sum\"] = df[\"lat\"] + df[\"lng\"]\n    df[\"coord_product\"] = df[\"lat\"] * df[\"lng\"]\n\ndef add_h3(df, levels=(7, 8, 9)):\n    for lvl in levels:\n        df[f\"h3_{lvl}\"] = [h3.latlng_to_cell(df[\"lat\"].iloc[i], df[\"lng\"].iloc[i], lvl) for i in range(len(df))]\n    return df\ntrain_data = add_h3(train_data)\ntest_data = add_h3(test_data)\n\nfor df in (train_data, test_data):\n    for lvl in (7, 8, 9):\n        col = f\"h3_{lvl}\"\n        df[f\"{col}_density\"] = df[col].map(df[col].value_counts())\n\nimport json\nfrom sklearn.neighbors import BallTree\nwith open('/kaggle/input/metrodata/metro.msk.json', 'r', encoding='utf-8') as f:\n    metro_data = json.load(f)\nmetro_stations = []\nfor line in metro_data:\n    for station in line['stations']:\n        metro_stations.append({\n            'name': station['name'],\n            'lat': station['lat'],\n            'lng': station['lng']\n        })\nmetro_df = pd.DataFrame(metro_stations)\nmetro_coords_rad = np.radians(metro_df[['lat', 'lng']].values)\nmetro_tree = BallTree(metro_coords_rad, metric='haversine')\n\ndef find_nearest_metro(lat, lng, k=3):\n    point_rad = np.radians([[lat, lng]])\n    distances, indices = metro_tree.query(point_rad, k=k)\n    distances_km = distances * 6371\n    return distances_km[0], indices[0]\n\ntrain_index = train_data.index\ntest_index = test_data.index\n\ntrain_metro_features = []\nfor idx, row in train_data.iterrows():\n    distances, indices = find_nearest_metro(row['lat'], row['lng'], k=3)\n    train_metro_features.append({\n        'metro_dist_1': distances[0],\n        'metro_dist_2': distances[1],\n        'metro_dist_3': distances[2]\n    })\ntrain_metro_df = pd.DataFrame(train_metro_features, index=train_index)\ntrain_data = pd.concat([train_data, train_metro_df], axis=1)\n\ntest_metro_features = []\nfor idx, row in test_data.iterrows():\n    distances, indices = find_nearest_metro(row['lat'], row['lng'], k=3)\n    test_metro_features.append({\n        'metro_dist_1': distances[0],\n        'metro_dist_2': distances[1],\n        'metro_dist_3': distances[2]\n    })\ntest_metro_df = pd.DataFrame(test_metro_features, index=test_index)\ntest_data = pd.concat([test_data, test_metro_df], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T17:52:23.403732Z","iopub.execute_input":"2025-11-10T17:52:23.404010Z","iopub.status.idle":"2025-11-10T17:52:32.723142Z","shell.execute_reply.started":"2025-11-10T17:52:23.403986Z","shell.execute_reply":"2025-11-10T17:52:32.722499Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import polars as pl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T17:52:32.723964Z","iopub.execute_input":"2025-11-10T17:52:32.724230Z","iopub.status.idle":"2025-11-10T17:52:33.241723Z","shell.execute_reply.started":"2025-11-10T17:52:32.724207Z","shell.execute_reply":"2025-11-10T17:52:33.241068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.neighbors import BallTree","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T17:52:33.242471Z","iopub.execute_input":"2025-11-10T17:52:33.242730Z","iopub.status.idle":"2025-11-10T17:52:33.247092Z","shell.execute_reply.started":"2025-11-10T17:52:33.242710Z","shell.execute_reply":"2025-11-10T17:52:33.246317Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_spatial_features_train_test(train_df, test_df, category_col='category', lat_col='lat', lon_col='lng', target_col='target'):\n\n    if isinstance(train_df, pl.DataFrame):\n        train_pl = train_df\n    else:\n        train_pl = pl.from_pandas(train_df)\n        \n    if isinstance(test_df, pl.DataFrame):\n        test_pl = test_df\n    else:\n        test_pl = pl.from_pandas(test_df)\n    \n    # 1. Создаем BallTree на train данных\n    train_lat = train_pl[lat_col].to_numpy()\n    train_lon = train_pl[lon_col].to_numpy()\n    train_category = train_pl[category_col].to_numpy()\n    train_target = train_pl[target_col].to_numpy()\n    \n    train_coords_radians = np.deg2rad(np.column_stack([train_lat, train_lon]))\n    tree = BallTree(train_coords_radians, metric='haversine')\n    \n    radius_300m = 300 / 6371000\n    radius_1000m = 1000 / 6371000\n    \n    def calculate_features(df_coords, df_categories, is_train=False):\n        mean_target_300m = []\n        mean_target_1000m = []\n        \n        for idx in tqdm(range(len(df_coords))):\n            current_category = df_categories[idx]\n            current_coord = np.deg2rad([df_coords[idx]])\n            \n            indices_300m = tree.query_radius(current_coord, r=radius_300m)[0]\n            indices_1000m = tree.query_radius(current_coord, r=radius_1000m)[0]\n            \n            if is_train:\n                same_category_300m = [\n                    i for i in indices_300m \n                    if i != idx and train_category[i] == current_category\n                ]\n                same_category_1000m = [\n                    i for i in indices_1000m \n                    if i != idx and train_category[i] == current_category\n                ]\n            else:\n                same_category_300m = [\n                    i for i in indices_300m \n                    if train_category[i] == current_category\n                ]\n                same_category_1000m = [\n                    i for i in indices_1000m \n                    if train_category[i] == current_category\n                ]\n            \n            if len(same_category_300m) > 0:\n                mean_300 = np.mean(train_target[same_category_300m])\n            else:\n                mean_300 = np.nan\n                \n            if len(same_category_1000m) > 0:\n                mean_1000 = np.mean(train_target[same_category_1000m])\n            else:\n                mean_1000 = np.nan\n            \n            mean_target_300m.append(mean_300)\n            mean_target_1000m.append(mean_1000)\n        \n        return mean_target_300m, mean_target_1000m\n    \n    train_coords = np.column_stack([train_lat, train_lon])\n    train_300m, train_1000m = calculate_features(train_coords, train_category, is_train=True)\n    \n    test_lat = test_pl[lat_col].to_numpy()\n    test_lon = test_pl[lon_col].to_numpy()\n    test_category = test_pl[category_col].to_numpy()\n    test_coords = np.column_stack([test_lat, test_lon])\n    test_300m, test_1000m = calculate_features(test_coords, test_category, is_train=False)\n    \n    train_result = train_pl.with_columns([\n        pl.Series(f'mean_{target_col}_same_category_300m', train_300m),\n        pl.Series(f'mean_{target_col}_same_category_1000m', train_1000m)\n    ])\n    \n    test_result = test_pl.with_columns([\n        pl.Series(f'mean_{target_col}_same_category_300m', test_300m),\n        pl.Series(f'mean_{target_col}_same_category_1000m', test_1000m)\n    ])\n    \n    return train_result.to_pandas(), test_result.to_pandas()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T17:52:33.250048Z","iopub.execute_input":"2025-11-10T17:52:33.250290Z","iopub.status.idle":"2025-11-10T17:52:33.269990Z","shell.execute_reply.started":"2025-11-10T17:52:33.250249Z","shell.execute_reply":"2025-11-10T17:52:33.269371Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data,test_data =  create_spatial_features_train_test(\n    train_data, \n    test_data,\n    category_col='category',\n    lat_col='lat', \n    lon_col='lng',\n    target_col='target'\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T17:52:33.270744Z","iopub.execute_input":"2025-11-10T17:52:33.270995Z","iopub.status.idle":"2025-11-10T17:52:49.306974Z","shell.execute_reply.started":"2025-11-10T17:52:33.270970Z","shell.execute_reply":"2025-11-10T17:52:49.306265Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data['address'] = train_data['address'].fillna(\"\")\ntest_data['address'] = test_data['address'].fillna(\"\")\n\nfor df in (train_data, test_data):\n    df[\"name_len\"] = df[\"name\"].str.len()\n    df[\"name_words\"] = df[\"name\"].str.split().str.len()\n    df[\"addr_len\"] = df[\"address\"].str.len()\n    df[\"name_log_len\"] = np.log1p(df[\"name_len\"])\n    df[\"addr_log_len\"] = np.log1p(df[\"addr_len\"])\n\ncombined = pd.concat([train_data[[\"id\", \"name\", \"address\", \"category\"]], test_data[[\"id\", \"name\", \"address\", \"category\"]]])\naddr_count = combined[\"address\"].value_counts().to_dict()\nname_count = combined[\"name\"].value_counts().to_dict()\ncat_count = combined[\"category\"].value_counts().to_dict()\nrev_count = reviews_data.groupby(\"id\")[\"text\"].size().to_dict()\n\nfor df in (train_data, test_data):\n    df[\"addr_count\"] = df[\"address\"].map(addr_count)\n    df[\"name_count\"] = df[\"name\"].map(name_count)\n    df[\"rev_count\"] = df[\"id\"].map(rev_count)\n    df[\"cat_count\"] = df[\"category\"].map(cat_count)\n    df[\"seti\"] = (df[\"name_count\"] > 1).astype(int)\n\nword_pat = re.compile(r\"\\w+\", re.I)\n\ndef get_text_features(text):\n    if not isinstance(text, str):\n        return pd.Series([0, 0, 0, 0, 0])\n    words = word_pat.findall(text.lower())\n    word_cnt = len(words)\n    unique_cnt = len(set(words))\n    return pd.Series([\n        len(text),\n        word_cnt,\n        unique_cnt,\n        unique_cnt / max(word_cnt, 1),\n        np.mean([len(w) for w in words]) if words else 0\n    ])\n\ntext_feats = pd.DataFrame([get_text_features(t) for t in tqdm(reviews_data[\"text\"])])\ntext_feats.columns = [\"text_len\", \"word_cnt\", \"unique_cnt\", \"lex_div\", \"avg_word_len\"]\ntext_feats[\"id\"] = reviews_data[\"id\"]\ntext_feats[\"text_log_len\"] = np.log1p(text_feats[\"text_len\"])\n\ntext_stats = reviews_data.groupby(\"id\")[\"text\"].agg(\n    rev_len_mean=lambda x: np.mean(x.str.len()),\n    rev_len_std=lambda x: np.std(x.str.len()),\n    rev_len_log=lambda x: np.log1p(np.mean(x.str.len()))\n).reset_index()\n\ntext_agg = text_feats.groupby(\"id\").agg({\n    \"text_len\": [\"mean\", \"std\"],\n    \"word_cnt\": [\"mean\", \"std\"], \n    \"unique_cnt\": \"mean\",\n    \"lex_div\": \"mean\",\n    \"avg_word_len\": \"mean\",\n    \"text_log_len\": \"mean\"\n})\n\ntext_agg.columns = [f\"{a}_{b}\" for a, b in text_agg.columns]\ntext_agg = text_agg.reset_index()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T17:52:49.308015Z","iopub.execute_input":"2025-11-10T17:52:49.308316Z","iopub.status.idle":"2025-11-10T17:53:56.334328Z","shell.execute_reply.started":"2025-11-10T17:52:49.308290Z","shell.execute_reply":"2025-11-10T17:53:56.333289Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfor df_name, df in [('train', train_data), ('test', test_data)]:\n    merged = df.merge(text_stats, on=\"id\", how=\"left\")\n    for col in text_stats.columns:\n        if col != \"id\":\n            merged[col] = merged[col].fillna(0)\n    \n    if df_name == 'train':\n        train_data = merged\n    else:\n        test_data = merged\n\ntrain_data = train_data.reset_index(drop=True)\ntest_data = test_data.reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T17:53:56.335549Z","iopub.execute_input":"2025-11-10T17:53:56.335847Z","iopub.status.idle":"2025-11-10T17:53:56.576429Z","shell.execute_reply.started":"2025-11-10T17:53:56.335830Z","shell.execute_reply":"2025-11-10T17:53:56.575845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data['rev_count'] = train_data['rev_count'].fillna(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T17:53:56.577137Z","iopub.execute_input":"2025-11-10T17:53:56.577409Z","iopub.status.idle":"2025-11-10T17:53:56.582891Z","shell.execute_reply.started":"2025-11-10T17:53:56.577386Z","shell.execute_reply":"2025-11-10T17:53:56.582086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data['rev_count'] = test_data['rev_count'].fillna(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T17:53:56.583513Z","iopub.execute_input":"2025-11-10T17:53:56.583752Z","iopub.status.idle":"2025-11-10T17:53:56.595889Z","shell.execute_reply.started":"2025-11-10T17:53:56.583730Z","shell.execute_reply":"2025-11-10T17:53:56.595169Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sentiment_model_name = \"tabularisai/multilingual-sentiment-analysis\"\nbatch_size = 32\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntokenizer = AutoTokenizer.from_pretrained(sentiment_model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(sentiment_model_name).to(device)\nmodel.eval() \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T17:53:56.596678Z","iopub.execute_input":"2025-11-10T17:53:56.596902Z","iopub.status.idle":"2025-11-10T17:54:14.063307Z","shell.execute_reply.started":"2025-11-10T17:53:56.596888Z","shell.execute_reply":"2025-11-10T17:54:14.062647Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntexts = reviews_data[\"text\"].astype(str).tolist()\nreview_ids = reviews_data[\"id\"].values\n\npredicted_classes = []\nconfidence_scores = []\n\nwith torch.no_grad():\n    for i in tqdm(range(0, len(texts), batch_size)):\n        batch = texts[i:i+batch_size]\n        inputs = tokenizer(batch, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n        outputs = model(**inputs)\n        \n        predictions = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n        predicted_classes.extend(predictions)\n        \n        probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1).cpu().numpy()\n        confidence_scores.extend(probabilities.max(axis=1))\n\nsentiment_df = pd.DataFrame({\n    \"id\": review_ids, \n    \"sentiment_class\": predicted_classes,\n    \"confidence\": confidence_scores\n})\n\naggregated_sentiment = (\n    sentiment_df.groupby(\"id\")[\"sentiment_class\"]\n    .agg([\n        (\"very_positive_count\", lambda x: np.sum(x == 4)),  # Very Positive\n        (\"positive_count\", lambda x: np.sum(x == 3)),       # Positive  \n        (\"neutral_count\", lambda x: np.sum(x == 2)),        # Neutral\n        (\"negative_count\", lambda x: np.sum(x == 1)),       # Negative\n        (\"very_negative_count\", lambda x: np.sum(x == 0)),  # Very Negative\n        (\"total_count\", \"count\"),\n        (\"mean_sentiment\", \"mean\")  \n    ])\n    .reset_index()\n)\n\naggregated_sentiment[\"positive_ratio\"] = (\n    aggregated_sentiment[\"very_positive_count\"] + aggregated_sentiment[\"positive_count\"]\n) / aggregated_sentiment[\"total_count\"].clip(lower=1)\n\naggregated_sentiment[\"negative_ratio\"] = (\n    aggregated_sentiment[\"very_negative_count\"] + aggregated_sentiment[\"negative_count\"] \n) / aggregated_sentiment[\"total_count\"].clip(lower=1)\n\naggregated_sentiment[\"neutral_ratio\"] = (\n    aggregated_sentiment[\"neutral_count\"] \n) / aggregated_sentiment[\"total_count\"].clip(lower=1)\n\ntrain_data = train_data.merge(aggregated_sentiment, on=\"id\", how=\"left\")\ntest_data = test_data.merge(aggregated_sentiment, on=\"id\", how=\"left\")\n\nsentiment_columns = [\n    \"very_positive_count\", \"positive_count\", \"neutral_count\", \n    \"negative_count\", \"very_negative_count\", \"total_count\",\n    \"mean_sentiment\", \"positive_ratio\", \"negative_ratio\", \n    \"neutral_ratio\", \n]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T17:54:14.064077Z","iopub.execute_input":"2025-11-10T17:54:14.064697Z","iopub.status.idle":"2025-11-10T18:16:38.145476Z","shell.execute_reply.started":"2025-11-10T17:54:14.064675Z","shell.execute_reply":"2025-11-10T18:16:38.144807Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nembedding_model_name = \"cointegrated/rubert-tiny2\"\n\ngrouped_reviews = (\n    reviews_data.groupby(\"id\")[\"text\"]\n    .apply(lambda x: \" \".join(x.tolist()[:20])[:2048])\n    .reset_index()\n    .rename(columns={\"text\": \"combined_text\"})\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T18:16:38.146378Z","iopub.execute_input":"2025-11-10T18:16:38.146641Z","iopub.status.idle":"2025-11-10T18:16:38.877210Z","shell.execute_reply.started":"2025-11-10T18:16:38.146623Z","shell.execute_reply":"2025-11-10T18:16:38.876574Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T18:16:38.877932Z","iopub.execute_input":"2025-11-10T18:16:38.878153Z","iopub.status.idle":"2025-11-10T18:16:41.931008Z","shell.execute_reply.started":"2025-11-10T18:16:38.878135Z","shell.execute_reply":"2025-11-10T18:16:41.930413Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embedding_model = SentenceTransformer(embedding_model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T18:16:41.931827Z","iopub.execute_input":"2025-11-10T18:16:41.932476Z","iopub.status.idle":"2025-11-10T18:16:45.243514Z","shell.execute_reply.started":"2025-11-10T18:16:41.932452Z","shell.execute_reply":"2025-11-10T18:16:45.242979Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text_list = grouped_reviews[\"combined_text\"].tolist()\nembeddings = embedding_model.encode(\n    text_list,\n    batch_size=batch_size,\n    show_progress_bar=True,\n    convert_to_numpy=True,\n    normalize_embeddings=False\n)\n\npca = PCA(n_components=64, random_state=42)\nreduced_embeddings = pca.fit_transform(embeddings)\n\nembedding_features = [f\"embedding_{i}\" for i in range(64)]\nembedding_df = pd.DataFrame(reduced_embeddings, columns=embedding_features)\nembedding_df[\"id\"] = grouped_reviews[\"id\"].values\n\ntrain_data = train_data.merge(embedding_df, on=\"id\", how=\"left\")\ntest_data = test_data.merge(embedding_df, on=\"id\", how=\"left\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T19:04:23.029578Z","iopub.execute_input":"2025-11-10T19:04:23.029890Z","iopub.status.idle":"2025-11-10T19:05:24.722347Z","shell.execute_reply.started":"2025-11-10T19:04:23.029871Z","shell.execute_reply":"2025-11-10T19:05:24.721490Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.isna().sum().sort_values()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T18:17:46.491502Z","iopub.execute_input":"2025-11-10T18:17:46.491776Z","iopub.status.idle":"2025-11-10T18:17:46.552195Z","shell.execute_reply.started":"2025-11-10T18:17:46.491749Z","shell.execute_reply":"2025-11-10T18:17:46.551623Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data[embedding_features] = train_data[embedding_features].fillna(0)\ntest_data[embedding_features] = test_data[embedding_features].fillna(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T19:05:24.723498Z","iopub.execute_input":"2025-11-10T19:05:24.723752Z","iopub.status.idle":"2025-11-10T19:05:24.750014Z","shell.execute_reply.started":"2025-11-10T19:05:24.723734Z","shell.execute_reply":"2025-11-10T19:05:24.748612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"categorical_features = ['name', 'category']\ntarget_column = 'target'\nfeature_columns = [col for col in train_data.columns if col not in [target_column, 'id', 'coordinates', 'address', 'h3_7', 'h3_8', 'h3_9',\n                                                                   'h3_7_density', 'h3_8_density', 'h3_9_density']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T19:05:24.750531Z","iopub.status.idle":"2025-11-10T19:05:24.750794Z","shell.execute_reply.started":"2025-11-10T19:05:24.750676Z","shell.execute_reply":"2025-11-10T19:05:24.750687Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#adv val","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T18:17:46.583512Z","iopub.execute_input":"2025-11-10T18:17:46.583793Z","iopub.status.idle":"2025-11-10T18:17:46.595285Z","shell.execute_reply.started":"2025-11-10T18:17:46.583764Z","shell.execute_reply":"2025-11-10T18:17:46.594703Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom catboost import CatBoostClassifier, CatBoostRegressor, Pool\n\nNFOLD = 5\nSEED = 42\nTOP_N_IMP = 30\nEARLY_STOPPING = 200  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T19:05:24.752055Z","iopub.status.idle":"2025-11-10T19:05:24.752305Z","shell.execute_reply.started":"2025-11-10T19:05:24.752188Z","shell.execute_reply":"2025-11-10T19:05:24.752200Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cat_feats = categorical_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T19:05:24.752925Z","iopub.status.idle":"2025-11-10T19:05:24.753228Z","shell.execute_reply.started":"2025-11-10T19:05:24.753055Z","shell.execute_reply":"2025-11-10T19:05:24.753071Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_tr_df = train_data[feature_columns]\nX_te_df = test_data[feature_columns]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T19:05:24.754493Z","iopub.status.idle":"2025-11-10T19:05:24.754996Z","shell.execute_reply.started":"2025-11-10T19:05:24.754801Z","shell.execute_reply":"2025-11-10T19:05:24.754820Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for c in cat_feats:\n    if c in X_tr_df.columns:\n        X_tr_df[c] = X_tr_df[c].astype('object')\n    if c in X_te_df.columns:\n        X_te_df[c] = X_te_df[c].astype('object')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T19:05:24.756137Z","iopub.status.idle":"2025-11-10T19:05:24.756373Z","shell.execute_reply.started":"2025-11-10T19:05:24.756266Z","shell.execute_reply":"2025-11-10T19:05:24.756276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T19:05:24.757743Z","iopub.status.idle":"2025-11-10T19:05:24.757958Z","shell.execute_reply.started":"2025-11-10T19:05:24.757860Z","shell.execute_reply":"2025-11-10T19:05:24.757869Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_train = X_tr_df.shape[0]\nn_test  = X_te_df.shape[0]\n\n# ---------------- Adversarial Validation ----------------\n# is_test: train=0, test=1, P(y=1)=P(is_test)\n\nX_av = pd.concat([X_tr_df, X_te_df], axis=0, ignore_index=True)\ny_av = np.concatenate([np.zeros(n_train, dtype=int), np.ones(n_test, dtype=int)])\n\nskf_av = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n\nav_params = dict(\n    loss_function='Logloss',\n    eval_metric='AUC',\n    learning_rate=0.05,\n    depth=6,\n    l2_leaf_reg=3.0,\n    iterations=1000,\n    od_type='Iter',\n    od_wait=EARLY_STOPPING,\n    random_seed=SEED,\n    verbose=100,\n    auto_class_weights='Balanced', task_type = 'GPU'\n)\n\noof_p_test = np.zeros(n_train + n_test, dtype=float)\nfold_aucs_av = []\nfold_imps = []\n\nfor fold, (tr_idx, va_idx) in enumerate(skf_av.split(X_av, y_av), 1):\n    tr_pool = Pool(X_av.iloc[tr_idx], label=y_av[tr_idx], cat_features=cat_feats)\n    va_pool = Pool(X_av.iloc[va_idx], label=y_av[va_idx], cat_features=cat_feats)\n\n    model_av = CatBoostClassifier(**av_params)\n    model_av.fit(tr_pool, eval_set=va_pool, use_best_model=True)\n\n    oof_p_test[va_idx] = model_av.predict_proba(va_pool)[:, 1]\n    fold_auc = roc_auc_score(y_av[va_idx], oof_p_test[va_idx])\n    fold_aucs_av.append(fold_auc)\n\n    fold_imps.append(model_av.get_feature_importance(type='FeatureImportance'))\n    print(f\"[AV] Fold {fold}/{NFOLD} AUC={fold_auc:.6f}\")\n\noof_auc_av = roc_auc_score(y_av, oof_p_test)\nprint(f\"[AV] OOF AUC: {oof_auc_av:.6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T19:05:24.758819Z","iopub.status.idle":"2025-11-10T19:05:24.759043Z","shell.execute_reply.started":"2025-11-10T19:05:24.758936Z","shell.execute_reply":"2025-11-10T19:05:24.758946Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------- Plots ----------------\np_train = oof_p_test[:n_train]\np_test  = oof_p_test[n_train:]\n\nplt.figure(figsize=(8,5))\nbins = np.linspace(0, 1, 50)\nplt.hist(p_train, bins=bins, alpha=0.6, label='train (is_test=0)')\nplt.hist(p_test,  bins=bins, alpha=0.6, label='test (is_test=1)')\nplt.axvline(0.5, color='k', linestyle='--', linewidth=1)\nplt.title(f'Adversarial OOF P(test) | AUC={oof_auc_av:.4f}')\nplt.xlabel('P(sample is from test)')\nplt.ylabel('Count')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\navg_imp = np.mean(np.stack(fold_imps, axis=0), axis=0)\nord_idx = np.argsort(avg_imp)[::-1][:TOP_N_IMP]\nplt.figure(figsize=(9, max(4.5, 0.35*len(ord_idx))))\nplt.barh(np.array(feature_columns)[ord_idx][::-1], avg_imp[ord_idx][::-1], color='#4e79a7')\nplt.title(f'Adversarial feature importance (top {TOP_N_IMP})')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T19:05:24.759927Z","iopub.status.idle":"2025-11-10T19:05:24.760180Z","shell.execute_reply.started":"2025-11-10T19:05:24.760065Z","shell.execute_reply":"2025-11-10T19:05:24.760078Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_av","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"weights.max()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T18:33:28.493531Z","iopub.execute_input":"2025-11-10T18:33:28.494243Z","iopub.status.idle":"2025-11-10T18:33:28.499121Z","shell.execute_reply.started":"2025-11-10T18:33:28.494217Z","shell.execute_reply":"2025-11-10T18:33:28.498397Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"weights = p_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T19:05:24.761400Z","iopub.status.idle":"2025-11-10T19:05:24.761690Z","shell.execute_reply.started":"2025-11-10T19:05:24.761548Z","shell.execute_reply":"2025-11-10T19:05:24.761557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# CatBoost Hyperparameter Tuning (Classifier & Regressor)\n# - IO: polars, моделирование: pandas + CatBoost\n# - Tuning: Optuna + (Stratified)KFold CV + early stopping\n# - GPU/CPU-aware пространство поиска\n# - Возвращает лучшие параметры, лучшие модели CV-ансамбля, OOF и предсказания на тесте (опционально)\n# =========================================================\nimport os\nimport gc\nimport math\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport polars as pl\nimport numpy as np\nimport pandas as pd\n\nimport optuna\nfrom optuna.samplers import TPESampler\nfrom optuna.pruners import MedianPruner\nfrom optuna.importance import get_param_importances\n\nfrom catboost import CatBoostClassifier, CatBoostRegressor, Pool\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import roc_auc_score, log_loss, mean_squared_error\n\n# --------------- Config (правьте под себя) ---------------\nSEED = 42\nNFOLD = 5\nEARLY_STOPPING = 200\nN_TRIALS_CLS = 60\nN_TRIALS_REG = 60\nTIMEOUT_SEC = None  # например 3600\n\nUSE_GPU = False  # True, если хотите принудительно использовать GPU\nN_THREADS = 0    # 0 = все доступные\n\n# Явно укажите:\nTARGET_CLASS = 'target_cls'  # таргет для классификации\nTARGET_REG   = 'target_reg'  # таргет для регрессии\nCAT_COLS = ['cat_col_1', 'cat_col_2']  # список категориальных фич по именам\nID_COL = None  # опционально\n\n# Если FEATURES=None — возьмём все столбцы кроме TARGET/ID\nFEATURES_CLS = None\nFEATURES_REG = None\n\n# --------------- Utils ---------------\ndef set_seed(seed=42):\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\nset_seed(SEED)\n\ndef detect_gpu():\n    if USE_GPU:\n        return True\n    # простая эвристика: проверим переменную окружения\n    return False\n\nGPU_AVAILABLE = detect_gpu()\n\ndef to_pandas_and_prepare(df_pl: pl.DataFrame, features: list, cat_cols: list):\n    df_pd = df_pl.select(features).to_pandas()\n    # CatBoost: object dtype для категориальных\n    cat_cols = [c for c in cat_cols if c in features]\n    for c in cat_cols:\n        df_pd[c] = df_pd[c].astype('object')\n    cat_idx = [i for i, c in enumerate(features) if c in cat_cols]\n    return df_pd, cat_idx\n\ndef get_features(df_pl: pl.DataFrame, target: str, id_col: str | None, explicit_features: list | None):\n    if explicit_features is not None:\n        return explicit_features\n    exc = {target}\n    if id_col and id_col in df_pl.columns:\n        exc.add(id_col)\n    return [c for c in df_pl.columns if c not in exc]\n\ndef kfold_generator_classification(y, n_splits=5, seed=42):\n    return StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed).split(np.zeros_like(y), y)\n\ndef kfold_generator_regression(X, n_splits=5, seed=42):\n    return KFold(n_splits=n_splits, shuffle=True, random_state=seed).split(X)\n\n# --------------- Hyperparameter spaces ---------------\ndef suggest_common_params(trial, for_gpu: bool):\n    params = {\n        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-3, 100.0, log=True),\n        \"random_strength\": trial.suggest_float(\"random_strength\", 0.0, 2.0),\n        \"rsm\": trial.suggest_float(\"rsm\", 0.5, 1.0),  # column sampling\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 64),\n        \"leaf_estimation_method\": trial.suggest_categorical(\"leaf_estimation_method\", [\"Newton\", \"Gradient\"]),\n        \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\"]),\n        \"iterations\": 10000,\n        \"od_type\": \"Iter\",\n        \"od_wait\": EARLY_STOPPING,\n        \"random_seed\": SEED,\n        \"thread_count\": N_THREADS,\n        \"task_type\": \"GPU\" if for_gpu else \"CPU\",\n        \"allow_writing_files\": False,\n        \"verbose\": 0,\n    }\n    if params[\"bootstrap_type\"] == \"Bayesian\":\n        params[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0.0, 1.0)\n    else:  # Bernoulli\n        params[\"subsample\"] = trial.suggest_float(\"subsample\", 0.5, 1.0)\n\n    # grow_policy зависит от устройства\n    if for_gpu:\n        # на GPU разрешён Lossguide\n        grow = trial.suggest_categorical(\"grow_policy\", [\"SymmetricTree\", \"Lossguide\"])\n        params[\"grow_policy\"] = grow\n        if grow == \"Lossguide\":\n            params[\"max_leaves\"] = trial.suggest_int(\"max_leaves\", 31, 512)\n        # можно ограничить bins, если нужно\n        params[\"border_count\"] = trial.suggest_int(\"border_count\", 32, 128)\n    else:\n        params[\"grow_policy\"] = \"SymmetricTree\"\n        params[\"border_count\"] = trial.suggest_int(\"border_count\", 32, 255)\n\n    return params\n\n# --------------- CV evaluation helpers ---------------\ndef cv_score_classifier(params, X, y, cat_idx, n_splits=5, seed=42):\n    scores = []\n    best_iters = []\n    for fold, (tr_idx, va_idx) in enumerate(kfold_generator_classification(y, n_splits, seed), 1):\n        tr_pool = Pool(X.iloc[tr_idx], label=y[tr_idx], cat_features=cat_idx)\n        va_pool = Pool(X.iloc[va_idx], label=y[va_idx], cat_features=cat_idx)\n\n        model = CatBoostClassifier(\n            loss_function=\"Logloss\",\n            eval_metric=\"AUC\",\n            **params\n        )\n        model.fit(tr_pool, eval_set=va_pool, use_best_model=True)\n        pred = model.predict_proba(va_pool)[:, 1]\n        auc = roc_auc_score(y[va_idx], pred)\n        scores.append(auc)\n        best_iters.append(model.get_best_iteration())\n    return float(np.mean(scores)), int(np.mean(best_iters))\n\ndef cv_score_regressor(params, X, y, cat_idx, n_splits=5, seed=42):\n    rmses = []\n    best_iters = []\n    for fold, (tr_idx, va_idx) in enumerate(kfold_generator_regression(X, n_splits, seed), 1):\n        tr_pool = Pool(X.iloc[tr_idx], label=y[tr_idx], cat_features=cat_idx)\n        va_pool = Pool(X.iloc[va_idx], label=y[va_idx], cat_features=cat_idx)\n\n        model = CatBoostRegressor(\n            loss_function=\"RMSE\",\n            eval_metric=\"RMSE\",\n            **params\n        )\n        model.fit(tr_pool, eval_set=va_pool, use_best_model=True)\n        pred = model.predict(va_pool)\n        rmse = mean_squared_error(y[va_idx], pred, squared=False)\n        rmses.append(rmse)\n        best_iters.append(model.get_best_iteration())\n    return float(np.mean(rmses)), int(np.mean(best_iters))\n\n# --------------- Optuna objectives ---------------\ndef objective_classifier(trial, X, y, cat_idx):\n    params = suggest_common_params(trial, GPU_AVAILABLE)\n    # Доп. параметры, специфичные для классификации\n    params[\"auto_class_weights\"] = trial.suggest_categorical(\"auto_class_weights\", [None, \"Balanced\"])\n\n    # CV оценка\n    scores = []\n    best_iters = []\n    # Pruning по фолдам (report + prune)\n    for step, (tr_idx, va_idx) in enumerate(kfold_generator_classification(y, NFOLD, SEED)):\n        tr_pool = Pool(X.iloc[tr_idx], label=y[tr_idx], cat_features=cat_idx)\n        va_pool = Pool(X.iloc[va_idx], label=y[va_idx], cat_features=cat_idx)\n\n        model = CatBoostClassifier(\n            loss_function=\"Logloss\",\n            eval_metric=\"AUC\",\n            **params\n        )\n        model.fit(tr_pool, eval_set=va_pool, use_best_model=True)\n        pred = model.predict_proba(va_pool)[:, 1]\n        auc = roc_auc_score(y[va_idx], pred)\n        scores.append(auc)\n        best_iters.append(model.get_best_iteration())\n\n        # сообщаем промежуточное значение и проверяем на prune\n        trial.report(np.mean(scores), step=step)\n        if trial.should_prune():\n            raise optuna.exceptions.TrialPruned()\n\n    return float(np.mean(scores))\n\ndef objective_regressor(trial, X, y, cat_idx):\n    params = suggest_common_params(trial, GPU_AVAILABLE)\n    # CV оценка\n    rmses = []\n    for step, (tr_idx, va_idx) in enumerate(kfold_generator_regression(X, NFOLD, SEED)):\n        tr_pool = Pool(X.iloc[tr_idx], label=y[tr_idx], cat_features=cat_idx)\n        va_pool = Pool(X.iloc[va_idx], label=y[va_idx], cat_features=cat_idx)\n\n        model = CatBoostRegressor(\n            loss_function=\"RMSE\",\n            eval_metric=\"RMSE\",\n            **params\n        )\n        model.fit(tr_pool, eval_set=va_pool, use_best_model=True)\n        pred = model.predict(va_pool)\n        rmse = mean_squared_error(y[va_idx], pred, squared=False)\n        rmses.append(rmse)\n\n        trial.report(np.mean(rmses), step=step)\n        if trial.should_prune():\n            raise optuna.exceptions.TrialPruned()\n\n    # minimize\n    return float(np.mean(rmses))\n\n# --------------- Tuning runners ---------------\ndef tune_classifier(X, y, cat_idx, n_trials=N_TRIALS_CLS, timeout=TIMEOUT_SEC):\n    study = optuna.create_study(\n        direction=\"maximize\",\n        sampler=TPESampler(seed=SEED),\n        pruner=MedianPruner(n_warmup_steps=2),\n        study_name=\"catboost_classifier_tuning\"\n    )\n    study.optimize(lambda tr: objective_classifier(tr, X, y, cat_idx),\n                   n_trials=n_trials, timeout=timeout, show_progress_bar=True)\n    return study\n\ndef tune_regressor(X, y, cat_idx, n_trials=N_TRIALS_REG, timeout=TIMEOUT_SEC):\n    study = optuna.create_study(\n        direction=\"minimize\",\n        sampler=TPESampler(seed=SEED),\n        pruner=MedianPruner(n_warmup_steps=2),\n        study_name=\"catboost_regressor_tuning\"\n    )\n    study.optimize(lambda tr: objective_regressor(tr, X, y, cat_idx),\n                   n_trials=n_trials, timeout=timeout, show_progress_bar=True)\n    return study\n\n# --------------- Fit best models with CV and (optionally) predict test ---------------\ndef fit_cv_ensemble_classifier(X, y, cat_idx, best_params, n_splits=NFOLD, seed=SEED, X_test=None):\n    oof = np.zeros(len(X), dtype=float)\n    models = []\n    test_pred = np.zeros(len(X_test), dtype=float) if X_test is not None else None\n    fold_scores = []\n    for fold, (tr_idx, va_idx) in enumerate(kfold_generator_classification(y, n_splits, seed), 1):\n        tr_pool = Pool(X.iloc[tr_idx], label=y[tr_idx], cat_features=cat_idx)\n        va_pool = Pool(X.iloc[va_idx], label=y[va_idx], cat_features=cat_idx)\n        te_pool = Pool(X_test, cat_features=cat_idx) if X_test is not None else None\n\n        model = CatBoostClassifier(\n            loss_function=\"Logloss\",\n            eval_metric=\"AUC\",\n            **best_params\n        )\n        model.fit(tr_pool, eval_set=va_pool, use_best_model=True)\n        oof[va_idx] = model.predict_proba(va_pool)[:, 1]\n        fold_auc = roc_auc_score(y[va_idx], oof[va_idx])\n        fold_scores.append(fold_auc)\n        models.append(model)\n\n        if te_pool is not None:\n            test_pred += model.predict_proba(te_pool)[:, 1] / n_splits\n\n    oof_metric = roc_auc_score(y, oof)\n    return models, oof, oof_metric, fold_scores, test_pred\n\ndef fit_cv_ensemble_regressor(X, y, cat_idx, best_params, n_splits=NFOLD, seed=SEED, X_test=None):\n    oof = np.zeros(len(X), dtype=float)\n    models = []\n    test_pred = np.zeros(len(X_test), dtype=float) if X_test is not None else None\n    fold_metrics = []\n    for fold, (tr_idx, va_idx) in enumerate(kfold_generator_regression(X, n_splits, seed), 1):\n        tr_pool = Pool(X.iloc[tr_idx], label=y[tr_idx], cat_features=cat_idx)\n        va_pool = Pool(X.iloc[va_idx], label=y[va_idx], cat_features=cat_idx)\n        te_pool = Pool(X_test, cat_features=cat_idx) if X_test is not None else None\n\n        model = CatBoostRegressor(\n            loss_function=\"RMSE\",\n            eval_metric=\"RMSE\",\n            **best_params\n        )\n        model.fit(tr_pool, eval_set=va_pool, use_best_model=True)\n        oof[va_idx] = model.predict(va_pool)\n        fold_rmse = mean_squared_error(y[va_idx], oof[va_idx], squared=False)\n        fold_metrics.append(fold_rmse)\n        models.append(model)\n\n        if te_pool is not None:\n            test_pred += model.predict(te_pool) / n_splits\n\n    oof_metric = mean_squared_error(y, oof, squared=False)\n    return models, oof, oof_metric, fold_metrics, test_pred\n\n# --------------- EXAMPLE USAGE ---------------\n# Пример для классификации\n# 1) загрузка\n# train_cls.csv должен содержать TARGET_CLASS, фичи + (опц.) ID_COL\ntry:\n    train_cls_pl = pl.read_csv(\"train_cls.csv\")\n    assert TARGET_CLASS in train_cls_pl.columns\n    feats_cls = get_features(train_cls_pl, TARGET_CLASS, ID_COL, FEATURES_CLS)\n    # каст категорий к строкам\n    cast_cols = [pl.col(c).cast(pl.Utf8) for c in CAT_COLS if c in train_cls_pl.columns]\n    if cast_cols:\n        train_cls_pl = train_cls_pl.with_columns(cast_cols)\n\n    X_cls, cat_idx_cls = to_pandas_and_prepare(train_cls_pl, feats_cls, CAT_COLS)\n    y_cls = train_cls_pl[TARGET_CLASS].to_numpy()\n\n    # 2) тюнинг\n    study_cls = tune_classifier(X_cls, y_cls, cat_idx_cls, n_trials=N_TRIALS_CLS, timeout=TIMEOUT_SEC)\n    best_params_cls = study_cls.best_params\n    # фиксируем общие не-тюнимые параметры (ES, итерации и т.д.), чтобы были воспроизводимы\n    best_params_cls.update({\n        \"iterations\": 10000,\n        \"od_type\": \"Iter\",\n        \"od_wait\": EARLY_STOPPING,\n        \"random_seed\": SEED,\n        \"thread_count\": N_THREADS,\n        \"task_type\": \"GPU\" if GPU_AVAILABLE else \"CPU\",\n        \"allow_writing_files\": False,\n        \"verbose\": 0\n    })\n    print(\"Best (Classifier) score:\", study_cls.best_value)\n    print(\"Best (Classifier) params:\", best_params_cls)\n\n    # 3) финальное CV-обучение и (опц.) предсказание на тесте\n    X_test_cls_pl = pl.read_csv(\"test_cls.csv\") if os.path.exists(\"test_cls.csv\") else None\n    X_test_cls_pd = None\n    if X_test_cls_pl is not None:\n        # согласуем типы\n        if cast_cols:\n            X_test_cls_pl = X_test_cls_pl.with_columns([pl.col(c).cast(pl.Utf8) for c in CAT_COLS if c in X_test_cls_pl.columns])\n        X_test_cls_pd, _ = to_pandas_and_prepare(X_test_cls_pl, feats_cls, CAT_COLS)\n\n    models_cls, oof_cls, oof_auc_cls, fold_aucs_cls, test_pred_cls = fit_cv_ensemble_classifier(\n        X_cls, y_cls, cat_idx_cls, best_params_cls, n_splits=NFOLD, seed=SEED, X_test=X_test_cls_pd\n    )\n    print(f\"[Classifier] OOF AUC: {oof_auc_cls:.6f} | per-fold: {', '.join(f'{x:.5f}' for x in fold_aucs_cls)}\")\n    if test_pred_cls is not None:\n        print(\"Test preds (classifier) shape:\", test_pred_cls.shape)\n\nexcept Exception as e:\n    print(\"Classification part skipped or failed:\", e)\n\ngc.collect()\n\n# Пример для регрессии\ntry:\n    train_reg_pl = pl.read_csv(\"train_reg.csv\")\n    assert TARGET_REG in train_reg_pl.columns\n    feats_reg = get_features(train_reg_pl, TARGET_REG, ID_COL, FEATURES_REG)\n    cast_cols = [pl.col(c).cast(pl.Utf8) for c in CAT_COLS if c in train_reg_pl.columns]\n    if cast_cols:\n        train_reg_pl = train_reg_pl.with_columns(cast_cols)\n\n    X_reg, cat_idx_reg = to_pandas_and_prepare(train_reg_pl, feats_reg, CAT_COLS)\n    y_reg = train_reg_pl[TARGET_REG].to_numpy()\n\n    study_reg = tune_regressor(X_reg, y_reg, cat_idx_reg, n_trials=N_TRIALS_REG, timeout=TIMEOUT_SEC)\n    best_params_reg = study_reg.best_params\n    best_params_reg.update({\n        \"iterations\": 10000,\n        \"od_type\": \"Iter\",\n        \"od_wait\": EARLY_STOPPING,\n        \"random_seed\": SEED,\n        \"thread_count\": N_THREADS,\n        \"task_type\": \"GPU\" if GPU_AVAILABLE else \"CPU\",\n        \"allow_writing_files\": False,\n        \"verbose\": 0\n    })\n    print(\"Best (Regressor) score (RMSE):\", study_reg.best_value)\n    print(\"Best (Regressor) params:\", best_params_reg)\n\n    X_test_reg_pl = pl.read_csv(\"test_reg.csv\") if os.path.exists(\"test_reg.csv\") else None\n    X_test_reg_pd = None\n    if X_test_reg_pl is not None:\n        if cast_cols:\n            X_test_reg_pl = X_test_reg_pl.with_columns([pl.col(c).cast(pl.Utf8) for c in CAT_COLS if c in X_test_reg_pl.columns])\n        X_test_reg_pd, _ = to_pandas_and_prepare(X_test_reg_pl, feats_reg, CAT_COLS)\n\n    models_reg, oof_reg, oof_rmse_reg, fold_rmses_reg, test_pred_reg = fit_cv_ensemble_regressor(\n        X_reg, y_reg, cat_idx_reg, best_params_reg, n_splits=NFOLD, seed=SEED, X_test=X_test_reg_pd\n    )\n    print(f\"[Regressor] OOF RMSE: {oof_rmse_reg:.6f} | per-fold: {', '.join(f'{x:.5f}' for x in fold_rmses_reg)}\")\n    if test_pred_reg is not None:\n        print(\"Test preds (regressor) shape:\", test_pred_reg.shape)\n\nexcept Exception as e:\n    print(\"Regression part skipped or failed:\", e)\n\n# --------------- (Опционально) важность гиперпараметров по Optuna ---------------\ndef print_param_importance(study, title=\"Param importance\"):\n    try:\n        imp = get_param_importances(study)\n        print(title)\n        for k, v in imp.items():\n            print(f\"- {k}: {v:.4f}\")\n    except Exception as e:\n        print(\"Param importance unavailable:\", e)\n\ntry:\n    print_param_importance(study_cls, \"Classifier param importance\")\nexcept:\n    pass\n\ntry:\n    print_param_importance(study_reg, \"Regressor param importance\")\nexcept:\n    pass","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#https://www.predictiveresearchsolutions.com/post/data-science-tips-feature-selection-using-boruta-in-python","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import KBinsDiscretizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T18:33:32.386175Z","iopub.execute_input":"2025-11-10T18:33:32.386850Z","iopub.status.idle":"2025-11-10T18:33:32.390111Z","shell.execute_reply.started":"2025-11-10T18:33:32.386827Z","shell.execute_reply":"2025-11-10T18:33:32.389394Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X, y = train_data[feature_columns], train_data[target_column]\n\nn_folds = 5\nn_bins = 20\ndiscretizer = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='quantile', random_state=42)\ny_binned = discretizer.fit_transform(y.values.reshape(-1, 1)).ravel().astype(int)\n\nstratified_kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\ncatboost_scores = []\n\nfor fold, (train_indices, valid_indices) in enumerate(stratified_kfold.split(X, y_binned), 1):\n    X_train, X_valid = X.iloc[train_indices].copy(), X.iloc[valid_indices].copy()\n    y_train, y_valid = y.iloc[train_indices], y.iloc[valid_indices]\n    sample_weights_train = weights[train_indices]\n    sample_weights_valid = weights[valid_indices]\n\n    train_pool = Pool(\n        data=X_train,\n        label=y_train,\n        cat_features=categorical_features,\n        weight=sample_weights_train\n    )\n    \n    valid_pool = Pool(\n        data=X_valid,\n        label=y_valid,\n        cat_features=categorical_features,\n        weight=sample_weights_valid\n    )\n\n    catboost_model = CatBoostRegressor(\n        iterations=3000,\n        depth=8,\n        loss_function='MAE',\n        eval_metric='MAE',\n        random_seed=42 + fold,\n        task_type='GPU',\n        early_stopping_rounds=100\n    )\n\n    catboost_model.fit(\n        train_pool,\n        eval_set=valid_pool,\n        use_best_model=True, \n        verbose=200\n    )\n    catboost_predictions = catboost_model.predict(X_valid)\n    catboost_fold_mae = mean_absolute_error(y_valid, catboost_predictions)\n    catboost_scores.append(catboost_fold_mae)\n\n    catboost_model.save_model(f\"cb_f{fold}.cbm\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T19:05:24.762684Z","iopub.status.idle":"2025-11-10T19:05:24.762889Z","shell.execute_reply.started":"2025-11-10T19:05:24.762791Z","shell.execute_reply":"2025-11-10T19:05:24.762800Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.mean(catboost_scores)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T19:05:37.951348Z","iopub.execute_input":"2025-11-10T19:05:37.952034Z","iopub.status.idle":"2025-11-10T19:05:37.957053Z","shell.execute_reply.started":"2025-11-10T19:05:37.952001Z","shell.execute_reply":"2025-11-10T19:05:37.956189Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"1/ (1+np.mean(catboost_scores))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T19:05:39.831825Z","iopub.execute_input":"2025-11-10T19:05:39.832294Z","iopub.status.idle":"2025-11-10T19:05:39.837356Z","shell.execute_reply.started":"2025-11-10T19:05:39.832270Z","shell.execute_reply":"2025-11-10T19:05:39.836797Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.mean(catboost_scores)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T18:41:33.942632Z","iopub.execute_input":"2025-11-10T18:41:33.943171Z","iopub.status.idle":"2025-11-10T18:41:33.949764Z","shell.execute_reply.started":"2025-11-10T18:41:33.943142Z","shell.execute_reply":"2025-11-10T18:41:33.948983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"1/ (1+np.mean(catboost_scores))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T18:42:23.461927Z","iopub.execute_input":"2025-11-10T18:42:23.462619Z","iopub.status.idle":"2025-11-10T18:42:23.467364Z","shell.execute_reply.started":"2025-11-10T18:42:23.462570Z","shell.execute_reply":"2025-11-10T18:42:23.466560Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"catboost_test_predictions=[]\n\nfor fold in [1,2,4]: #2,4\n    catboost_model = CatBoostRegressor()\n    catboost_model.load_model(f\"/kaggle/working/cb_f{fold}.cbm\")\n    catboost_test_predictions.append(catboost_model.predict(test_data[feature_columns]))\n    \ncatboost_mean_predictions = np.mean(catboost_test_predictions, axis=0)\nfinal_predictions =  catboost_mean_predictions \nsubmission = test_data[['id']]\nsubmission[\"target\"] = final_predictions\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T19:03:00.928332Z","iopub.execute_input":"2025-11-10T19:03:00.929097Z","iopub.status.idle":"2025-11-10T19:03:01.249741Z","shell.execute_reply.started":"2025-11-10T19:03:00.929073Z","shell.execute_reply":"2025-11-10T19:03:01.248924Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
