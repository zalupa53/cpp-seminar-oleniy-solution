{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install faiss-cpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T12:59:46.296233Z","iopub.execute_input":"2025-11-17T12:59:46.296891Z","iopub.status.idle":"2025-11-17T12:59:49.375708Z","shell.execute_reply.started":"2025-11-17T12:59:46.296857Z","shell.execute_reply":"2025-11-17T12:59:49.374809Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.13.0)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip uninstall transformers -y\n!pip install -U \"transformers==4.56.0\" ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T12:59:49.535887Z","iopub.execute_input":"2025-11-17T12:59:49.536740Z","iopub.status.idle":"2025-11-17T13:00:00.983612Z","shell.execute_reply.started":"2025-11-17T12:59:49.536705Z","shell.execute_reply":"2025-11-17T13:00:00.982868Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Found existing installation: transformers 4.56.0\nUninstalling transformers-4.56.0:\n  Successfully uninstalled transformers-4.56.0\nCollecting transformers==4.56.0\n  Using cached transformers-4.56.0-py3-none-any.whl.metadata (40 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.0) (3.20.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.0) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.0) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.0) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.0) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.0) (2.32.5)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.0) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.0) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.56.0) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.0) (2025.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.0) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.0) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.56.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.56.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.56.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.56.0) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.56.0) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.56.0) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.56.0) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.56.0) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.56.0) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.56.0) (2025.10.5)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.56.0) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.56.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.56.0) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.56.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.56.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.56.0) (2024.2.0)\nUsing cached transformers-4.56.0-py3-none-any.whl (11.6 MB)\nInstalling collected packages: transformers\nSuccessfully installed transformers-4.56.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nimport numpy as np\nfrom tqdm import tqdm\nimport os, zipfile, torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T13:00:01.914312Z","iopub.execute_input":"2025-11-17T13:00:01.914641Z","iopub.status.idle":"2025-11-17T13:00:34.109086Z","shell.execute_reply.started":"2025-11-17T13:00:01.914610Z","shell.execute_reply":"2025-11-17T13:00:34.108501Z"}},"outputs":[{"name":"stderr","text":"2025-11-17 13:00:14.204855: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763384414.406498      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763384414.461953      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":5},{"cell_type":"code","source":"# === Константы ===\nDATASET_NAME = \"Lipsrow/ruwiki_cleaned\"\nMODEL_EMB = \"sergeyzh/LaBSE-ru-turbo\"\n# === 1. Загрузка и подготовка данных ===\nprint(\"Загружаем датасет...\")\nds = load_dataset(DATASET_NAME, split=\"train\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T13:00:40.818649Z","iopub.execute_input":"2025-11-17T13:00:40.819308Z","iopub.status.idle":"2025-11-17T13:01:34.501287Z","shell.execute_reply.started":"2025-11-17T13:00:40.819280Z","shell.execute_reply":"2025-11-17T13:01:34.500739Z"}},"outputs":[{"name":"stdout","text":"Загружаем датасет...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/24.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aba9e87602ac4129abe781c544545c57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ruwiki.json:   0%|          | 0.00/7.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4b1f3271d144b14998d41cbe0ecc075"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/500000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c94f94f10aac48a9ab23d5d8ff126c9f"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"!pip install llama-cpp-python\n\nfrom llama_cpp import Llama\n\nllm = Llama.from_pretrained(\n\trepo_id=\"IlyaGusev/saiga_yandexgpt_8b_gguf\",\n\tfilename=\"saiga_yandexgpt_8b.BF16.gguf\",\n    n_gpu_layers=-1, \n    device=\"cuda\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T13:01:49.792706Z","iopub.execute_input":"2025-11-17T13:01:49.793387Z","iopub.status.idle":"2025-11-17T13:06:10.694770Z","shell.execute_reply.started":"2025-11-17T13:01:49.793363Z","shell.execute_reply":"2025-11-17T13:06:10.693822Z"}},"outputs":[{"name":"stdout","text":"Collecting llama-cpp-python\n  Downloading llama_cpp_python-0.3.16.tar.gz (50.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.15.0)\nRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (1.26.4)\nCollecting diskcache>=5.6.1 (from llama-cpp-python)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.20.0->llama-cpp-python) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\nDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.16-cp311-cp311-linux_x86_64.whl size=4503256 sha256=4f8fd1b1d18c469aa8c3cb09afdf02aeb1eab6e19f1e2ca11c18b45eb114ebff\n  Stored in directory: /root/.cache/pip/wheels/d8/5b/e5/a7d4b5765da347d314e8155197440c9995a962f8e4a5f52b23\nSuccessfully built llama-cpp-python\nInstalling collected packages: diskcache, llama-cpp-python\nSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.3.16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"./saiga_yandexgpt_8b.BF16.gguf:   0%|          | 0.00/16.1G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc56abc6a7d546eea4ba4713cdcd5927"}},"metadata":{}},{"name":"stderr","text":"llama_model_loader: loaded meta data with 33 key-value pairs and 291 tensors from /root/.cache/huggingface/hub/models--IlyaGusev--saiga_yandexgpt_8b_gguf/snapshots/e902caf817aed403060016db9e35e21f59d1ec49/./saiga_yandexgpt_8b.BF16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Saiga_Yandexgpt_8B_Sft_M4_D19_Hf\nllama_model_loader: - kv   3:                       general.organization str              = Models\nllama_model_loader: - kv   4:                         general.size_label str              = 8.0B\nllama_model_loader: - kv   5:                          llama.block_count u32              = 32\nllama_model_loader: - kv   6:                       llama.context_length u32              = 32768\nllama_model_loader: - kv   7:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   8:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv   9:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  10:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\nllama_model_loader: - kv  12:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  13:                 llama.attention.key_length u32              = 128\nllama_model_loader: - kv  14:               llama.attention.value_length u32              = 128\nllama_model_loader: - kv  15:                          general.file_type u32              = 32\nllama_model_loader: - kv  16:                           llama.vocab_size u32              = 129024\nllama_model_loader: - kv  17:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  18:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = llama3\nllama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,129024]  = [\"<unk>\", \"<s>\", \"</s>\", \"\\n\", \"[SEP]\"...\nllama_model_loader: - kv  21:                      tokenizer.ggml.scores arr[f32,129024]  = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,129024]  = [3, 3, 3, 1, 1, 1, 1, 6, 6, 6, 6, 6, ...\nException ignored on calling ctypes callback function: <function llama_log_callback at 0x786cf633a660>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/llama_cpp/_logger.py\", line 39, in llama_log_callback\n    print(text.decode(\"utf-8\"), end=\"\", flush=True, file=sys.stderr)\n          ^^^^^^^^^^^^^^^^^^^^\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xe2 in position 128: invalid continuation byte\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 128009\nllama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\nllama_model_loader: - kv  31:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - kv  32:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type bf16:  226 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = BF16\nprint_info: file size   = 14.97 GiB (16.00 BPW) \ninit_tokenizer: initializing tokenizer for type 1\nload: control token: 128006 '<|start_header_id|>' is not marked as EOG\nload: control token:      2 '</s>' is not marked as EOG\nload: control token:      0 '<unk>' is not marked as EOG\nload: control token:      1 '<s>' is not marked as EOG\nload: control token: 128007 '<|end_header_id|>' is not marked as EOG\nload: printing all EOG tokens:\nload:   - 128009 ('<|eot_id|>')\nload: special tokens cache size = 6\nload: token to piece cache size = 1.2035 MB\nprint_info: arch             = llama\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 32768\nprint_info: n_embd           = 4096\nprint_info: n_layer          = 32\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 14336\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 0\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 500000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 32768\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 8B\nprint_info: model params     = 8.04 B\nprint_info: general.name     = Saiga_Yandexgpt_8B_Sft_M4_D19_Hf\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 129024\nprint_info: n_merges         = 0\nprint_info: BOS token        = 1 '<s>'\nprint_info: EOS token        = 128009 '<|eot_id|>'\nprint_info: EOT token        = 128009 '<|eot_id|>'\nprint_info: UNK token        = 0 '<unk>'\nprint_info: PAD token        = 0 '<unk>'\nprint_info: LF token         = 17 '<0x0A>'\nprint_info: EOG token        = 128009 '<|eot_id|>'\nprint_info: max token length = 48\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: layer   0 assigned to device CPU, is_swa = 0\nload_tensors: layer   1 assigned to device CPU, is_swa = 0\nload_tensors: layer   2 assigned to device CPU, is_swa = 0\nload_tensors: layer   3 assigned to device CPU, is_swa = 0\nload_tensors: layer   4 assigned to device CPU, is_swa = 0\nload_tensors: layer   5 assigned to device CPU, is_swa = 0\nload_tensors: layer   6 assigned to device CPU, is_swa = 0\nload_tensors: layer   7 assigned to device CPU, is_swa = 0\nload_tensors: layer   8 assigned to device CPU, is_swa = 0\nload_tensors: layer   9 assigned to device CPU, is_swa = 0\nload_tensors: layer  10 assigned to device CPU, is_swa = 0\nload_tensors: layer  11 assigned to device CPU, is_swa = 0\nload_tensors: layer  12 assigned to device CPU, is_swa = 0\nload_tensors: layer  13 assigned to device CPU, is_swa = 0\nload_tensors: layer  14 assigned to device CPU, is_swa = 0\nload_tensors: layer  15 assigned to device CPU, is_swa = 0\nload_tensors: layer  16 assigned to device CPU, is_swa = 0\nload_tensors: layer  17 assigned to device CPU, is_swa = 0\nload_tensors: layer  18 assigned to device CPU, is_swa = 0\nload_tensors: layer  19 assigned to device CPU, is_swa = 0\nload_tensors: layer  20 assigned to device CPU, is_swa = 0\nload_tensors: layer  21 assigned to device CPU, is_swa = 0\nload_tensors: layer  22 assigned to device CPU, is_swa = 0\nload_tensors: layer  23 assigned to device CPU, is_swa = 0\nload_tensors: layer  24 assigned to device CPU, is_swa = 0\nload_tensors: layer  25 assigned to device CPU, is_swa = 0\nload_tensors: layer  26 assigned to device CPU, is_swa = 0\nload_tensors: layer  27 assigned to device CPU, is_swa = 0\nload_tensors: layer  28 assigned to device CPU, is_swa = 0\nload_tensors: layer  29 assigned to device CPU, is_swa = 0\nload_tensors: layer  30 assigned to device CPU, is_swa = 0\nload_tensors: layer  31 assigned to device CPU, is_swa = 0\nload_tensors: layer  32 assigned to device CPU, is_swa = 0\nload_tensors: tensor 'token_embd.weight' (bf16) (and 290 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\nload_tensors:   CPU_Mapped model buffer size = 15329.02 MiB\n.........................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 512\nllama_context: n_ctx_per_seq = 512\nllama_context: n_batch       = 512\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: kv_unified    = false\nllama_context: freq_base     = 500000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (512) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\nset_abort_callback: call\nllama_context:        CPU  output buffer size =     0.49 MiB\ncreate_memory: n_ctx = 512 (padded)\nllama_kv_cache_unified: layer   0: dev = CPU\nllama_kv_cache_unified: layer   1: dev = CPU\nllama_kv_cache_unified: layer   2: dev = CPU\nllama_kv_cache_unified: layer   3: dev = CPU\nllama_kv_cache_unified: layer   4: dev = CPU\nllama_kv_cache_unified: layer   5: dev = CPU\nllama_kv_cache_unified: layer   6: dev = CPU\nllama_kv_cache_unified: layer   7: dev = CPU\nllama_kv_cache_unified: layer   8: dev = CPU\nllama_kv_cache_unified: layer   9: dev = CPU\nllama_kv_cache_unified: layer  10: dev = CPU\nllama_kv_cache_unified: layer  11: dev = CPU\nllama_kv_cache_unified: layer  12: dev = CPU\nllama_kv_cache_unified: layer  13: dev = CPU\nllama_kv_cache_unified: layer  14: dev = CPU\nllama_kv_cache_unified: layer  15: dev = CPU\nllama_kv_cache_unified: layer  16: dev = CPU\nllama_kv_cache_unified: layer  17: dev = CPU\nllama_kv_cache_unified: layer  18: dev = CPU\nllama_kv_cache_unified: layer  19: dev = CPU\nllama_kv_cache_unified: layer  20: dev = CPU\nllama_kv_cache_unified: layer  21: dev = CPU\nllama_kv_cache_unified: layer  22: dev = CPU\nllama_kv_cache_unified: layer  23: dev = CPU\nllama_kv_cache_unified: layer  24: dev = CPU\nllama_kv_cache_unified: layer  25: dev = CPU\nllama_kv_cache_unified: layer  26: dev = CPU\nllama_kv_cache_unified: layer  27: dev = CPU\nllama_kv_cache_unified: layer  28: dev = CPU\nllama_kv_cache_unified: layer  29: dev = CPU\nllama_kv_cache_unified: layer  30: dev = CPU\nllama_kv_cache_unified: layer  31: dev = CPU\nllama_kv_cache_unified:        CPU KV buffer size =    64.00 MiB\nllama_kv_cache_unified: size =   64.00 MiB (   512 cells,  32 layers,  1/1 seqs), K (f16):   32.00 MiB, V (f16):   32.00 MiB\nllama_context: enumerating backends\nllama_context: backend_ptrs.size() = 1\nllama_context: max_nodes = 2328\nllama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\ngraph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\ngraph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\ngraph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\nllama_context:        CPU compute buffer size =   260.00 MiB\nllama_context: graph nodes  = 1126\nllama_context: graph splits = 1\nCPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \nModel metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '129024', 'general.file_type': '32', 'llama.attention.value_length': '128', 'llama.attention.key_length': '128', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'llama3', 'llama.context_length': '32768', 'general.name': 'Saiga_Yandexgpt_8B_Sft_M4_D19_Hf', 'general.organization': 'Models', 'general.type': 'model', 'general.size_label': '8.0B', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\nAvailable chat formats from metadata: chat_template.default\nGuessed chat format: llama-3\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"llm.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T13:29:18.354785Z","iopub.status.idle":"2025-11-17T13:29:18.354997Z","shell.execute_reply.started":"2025-11-17T13:29:18.354894Z","shell.execute_reply":"2025-11-17T13:29:18.354904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"texts = []\nMAX_WORDS = 200\nfor text in tqdm(ds['text'], desc=\"Извлечение первых абзацев\"):\n    first_paragraph = text.strip().split(\"\\n\")[0]\n    words = first_paragraph.split()\n    if len(words) > MAX_WORDS:\n        first_paragraph = \" \".join(words[:MAX_WORDS])\n    texts.append(first_paragraph)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T13:06:10.697270Z","iopub.execute_input":"2025-11-17T13:06:10.697838Z","iopub.status.idle":"2025-11-17T13:06:36.019050Z","shell.execute_reply.started":"2025-11-17T13:06:10.697814Z","shell.execute_reply":"2025-11-17T13:06:36.018389Z"}},"outputs":[{"name":"stderr","text":"Извлечение первых абзацев: 100%|██████████| 500000/500000 [00:25<00:00, 19760.42it/s]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import os, json\nfrom tqdm import tqdm\n\nout_path = \"texts.jsonl\"\n\nwith open(out_path, \"w\", encoding=\"utf-8\") as f:\n    for t in tqdm(texts, desc=\"write-texts\"):\n        para = t  # если уже первый абзац – можно убрать\n        if not para:\n            continue\n        f.write(json.dumps({\"text\": para}, ensure_ascii=False) + \"\\n\")\n\nprint(\"saved:\", out_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T13:10:57.607466Z","iopub.execute_input":"2025-11-17T13:10:57.608151Z","iopub.status.idle":"2025-11-17T13:11:02.329851Z","shell.execute_reply.started":"2025-11-17T13:10:57.608127Z","shell.execute_reply":"2025-11-17T13:11:02.329017Z"}},"outputs":[{"name":"stderr","text":"write-texts: 100%|██████████| 500000/500000 [00:04<00:00, 106080.76it/s]","output_type":"stream"},{"name":"stdout","text":"saved: texts.jsonl\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"#restart","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"texts = []\nwith open(\"/kaggle/working/texts.jsonl\", \"r\", encoding=\"utf-8\") as f:\n    for line in tqdm(f, desc=\"read-texts\"):\n        obj = json.loads(line)\n        texts.append(obj.get(\"text\", \"\"))\nprint(\"loaded:\", len(texts))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T13:11:16.647804Z","iopub.execute_input":"2025-11-17T13:11:16.648075Z","iopub.status.idle":"2025-11-17T13:11:19.765977Z","shell.execute_reply.started":"2025-11-17T13:11:16.648056Z","shell.execute_reply":"2025-11-17T13:11:19.765320Z"}},"outputs":[{"name":"stderr","text":"read-texts: 500000it [00:03, 164122.26it/s]","output_type":"stream"},{"name":"stdout","text":"loaded: 500000\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":" !nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T13:29:46.453061Z","iopub.execute_input":"2025-11-17T13:29:46.453697Z","iopub.status.idle":"2025-11-17T13:29:46.724411Z","shell.execute_reply.started":"2025-11-17T13:29:46.453663Z","shell.execute_reply":"2025-11-17T13:29:46.723675Z"}},"outputs":[{"name":"stdout","text":"Mon Nov 17 13:29:46 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   57C    P0             37W /  250W |    2571MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"!pip install -U --force-reinstall --no-cache-dir llama-cpp-python-cu121","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T13:30:10.349457Z","iopub.execute_input":"2025-11-17T13:30:10.349768Z","iopub.status.idle":"2025-11-17T13:30:12.025929Z","shell.execute_reply.started":"2025-11-17T13:30:10.349740Z","shell.execute_reply":"2025-11-17T13:30:12.024954Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement llama-cpp-python-cu121 (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for llama-cpp-python-cu121\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"device = 'cuda'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T13:11:51.413065Z","iopub.execute_input":"2025-11-17T13:11:51.413578Z","iopub.status.idle":"2025-11-17T13:11:51.417095Z","shell.execute_reply.started":"2025-11-17T13:11:51.413552Z","shell.execute_reply":"2025-11-17T13:11:51.416372Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# === 2. Создание эмбеддингов ===\nprint(\"Создаём эмбеддинги...\")\nembedder = SentenceTransformer(MODEL_EMB).to(device)\nembeddings = []\n\nfor i in tqdm(range(0, len(texts), 256), desc=\"Векторизация\"):\n    batch = texts[i:i+256]\n    vecs = embedder.encode(batch, convert_to_numpy=True, show_progress_bar=False, device = device,normalize_embeddings=True)\n    embeddings.append(vecs)\n\nembeddings = np.vstack(embeddings).astype(\"float32\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-17T13:11:55.542049Z","iopub.execute_input":"2025-11-17T13:11:55.542326Z","iopub.status.idle":"2025-11-17T13:29:18.354419Z","shell.execute_reply.started":"2025-11-17T13:11:55.542308Z","shell.execute_reply":"2025-11-17T13:29:18.353392Z"}},"outputs":[{"name":"stdout","text":"Создаём эмбеддинги...\n","output_type":"stream"},{"name":"stderr","text":"Векторизация:  49%|████▉     | 955/1954 [17:19<18:07,  1.09s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/585789334.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Векторизация\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mvecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_to_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnormalize_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0;31m# fixes for #522 and #487 to avoid oom problems on gpu with large datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_numpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0mall_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":17},{"cell_type":"code","source":"embeddings = np.vstack(embeddings).astype(\"float32\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T13:31:22.197483Z","iopub.execute_input":"2025-11-17T13:31:22.197766Z","iopub.status.idle":"2025-11-17T13:31:24.394345Z","shell.execute_reply.started":"2025-11-17T13:31:22.197745Z","shell.execute_reply":"2025-11-17T13:31:24.393741Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"print(\"Создаём FAISS индекс...\")\ndim = embeddings.shape[1]\nindex = faiss.IndexFlatIP(dim)\nindex.add(embeddings)\n#faiss.write_index(index, DB_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T13:31:25.004795Z","iopub.execute_input":"2025-11-17T13:31:25.005060Z","iopub.status.idle":"2025-11-17T13:31:25.740051Z","shell.execute_reply.started":"2025-11-17T13:31:25.005040Z","shell.execute_reply":"2025-11-17T13:31:25.739167Z"}},"outputs":[{"name":"stdout","text":"Создаём FAISS индекс...\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"SYSTEM_PROMPT = \"\"\"Ты — эрудированный ИИ-эксперт. Твоя главная сила — твои собственные обширные знания. Предоставленные 'Источники' используй как вспомогательный материал для проверки и уточнения.\n\nСледуй этому порядку действий:\n\n1.  **Сначала ответь сам.** Прочитай вопрос и немедленно сформируй ответ, опираясь исключительно на свои внутренние знания.\n\n2.  **Затем проверь по источникам.** Проанализируй предоставленные 'Источники' и сравни их с твоим ответом.\n\n3.  **Синтезируй финальный ответ по правилам:**\n    а) Если источники **подтверждают или дополняют** твои знания, объедини информацию в единый, исчерпывающий ответ.\n    б) Если источники **нерелевантны** (не относятся к вопросу), полностью проигнорируй их и используй только свой первоначальный ответ.\n    в) Если источники **противоречат** твоим знаниям, отдай приоритет информации из источников, ответ: \"не знаю\".\n    г) Финальный ответ пометь как CORRECT ANSWER: [ твой ответ или \"не знаю\" ]\n\nВажные правила, которые действуют всегда:\n- **Честность:** Если ответа нет ни в твоих знаниях, ни в источниках, просто и чётко напиши: \"не знаю\". Не выдумывай.\n- **Краткость:** Отвечай по существу, без вступлений и лишней \"воды\". \n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T13:31:29.490487Z","iopub.execute_input":"2025-11-17T13:31:29.491066Z","iopub.status.idle":"2025-11-17T13:31:29.495242Z","shell.execute_reply.started":"2025-11-17T13:31:29.491039Z","shell.execute_reply":"2025-11-17T13:31:29.494581Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# === Вспомогательные функции ===\ndef apply_chat_template(messages_list, tokenizer):\n    \"\"\"Форматирование списка сообщений в текст через шаблон Qwen.\"\"\"\n    texts = []\n    for messages in messages_list:\n        try:\n            t = tokenizer.apply_chat_template(\n                messages, tokenize=False, add_generation_prompt=True, enable_thinking=False\n            )\n        except TypeError:\n            t = tokenizer.apply_chat_template(\n                messages, tokenize=False, add_generation_prompt=True\n            )\n        texts.append(t)\n    return texts\n\n\ndef batch_generate_chat(messages_list, tokenizer, model, max_new_tokens=200, max_length=4096):\n    \"\"\"Быстрая генерация на GPU без pipeline с использованием структуры сообщений.\"\"\"\n    if not messages_list:\n        return []\n    \n    texts = apply_chat_template(messages_list, tokenizer)\n    t0 = time.perf_counter()\n\n    # Токенизация\n    model_inputs = tokenizer(\n        texts,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=max_length\n    ).to(model.device)\n    t1 = time.perf_counter()\n\n    # Генерация\n    with torch.no_grad():\n        gen = model.generate(\n            **model_inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=False,\n            num_beams=1,\n            use_cache=True,\n            pad_token_id=tokenizer.pad_token_id,\n            eos_token_id=tokenizer.eos_token_id\n        )\n    t2 = time.perf_counter()\n\n    attn = model_inputs[\"attention_mask\"]\n    input_lens = attn.sum(dim=1).tolist()\n\n    outputs = []\n    for i, gen_ids in enumerate(gen):\n        in_len = int(input_lens[i])\n        out_ids = gen_ids[in_len:]\n        text = tokenizer.decode(out_ids, skip_special_tokens=True).strip()\n        outputs.append(text)\n\n    print(f\"[DEBUG] tokenize: {t1 - t0:.2f}s, generate: {t2 - t1:.2f}s, total: {t2 - t0:.2f}s\")\n    return outputs\n\n\n\ndef fast_rag_batch(queries, rag_system_prompt, model, top_k=3, max_tokens=512, temperature=0.2):\n    # 1) Эмбеддинг вопросов\n    q_vecs = embedder.encode(\n        queries,\n        convert_to_numpy=True,\n        normalize_embeddings=True,\n        batch_size=64,\n        show_progress_bar=False,\n        device=device\n    ).astype(\"float32\")\n\n    # 2) Поиск ближайшего контекста\n    _, idxs = index.search(q_vecs, top_k)\n\n    # 3) Генерация через llama-cpp (chat)\n    answers = []\n    for question, ids in tqdm(zip(queries, idxs)):\n        ctx = \"\\n\\n\".join([f\"Статья {i}: {texts[i]}\" for i in ids])\n        texts_prepared = f\"\\n\\nИсточники:\\n---\\n{ctx}\\n---\\n\"\n        messages = [\n            {\"role\": \"system\", \"content\": rag_system_prompt },\n            {\"role\": \"user\", \"content\": question},\n        ]\n        out = model.create_chat_completion(\n            messages=messages,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            stream=False\n        )\n        answers.append(out[\"choices\"][0][\"message\"][\"content\"].strip())\n    return answers\n\n# === Пример использования ===\nquestions = [\n    \"Кто написал Евгения Онегина?\",\n    \"Когда распался Советский Союз?\",\n    \"Что такое квантовая запутанность?\",\n    'Кто автор книги \"Детство в Соломбале\"?',\n    \n    'Как зовут человека, который написал «Детство в Соломбале»?',\n    'Кто написал книгу «Детство в Соломбале»?',\n    'В каком году начал поддерживаться сеттинг Eberron?',\n    'Какое окрашивание приобретает безводный метанол при растворении небольшого количества медного купороса?',\n    \n    \"Какой античный математик изобрёл первый дизельный двигатель внутреннего сгорания?\",\n    'Кто первый человек высадившийся на солнце?',\n    'Как звали отца А.С. Пушкина?',\n    \"Какая компания осуществляет доставку роботами в Москве?\",\n    \n    \"Кто имеет крупнейшую доставку роботами на территории Москвы?\",\n    \"В Москве есть доставка роботами, какая компания выполняет её?\"\n]\n\nanswers = fast_rag_batch(questions,SYSTEM_PROMPT,llm, top_k=3)\n\nprint(\"\\n=== ОТВЕТЫ ===\")\nfor q, a in zip(questions, answers):\n    print(f\"\\n❓ {q}\\n💬 {a}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T13:36:54.622302Z","iopub.execute_input":"2025-11-17T13:36:54.623082Z","iopub.status.idle":"2025-11-17T13:50:07.001484Z","shell.execute_reply.started":"2025-11-17T13:36:54.623051Z","shell.execute_reply":"2025-11-17T13:50:07.000797Z"}},"outputs":[{"name":"stderr","text":"0it [00:00, ?it/s]llama_perf_context_print:        load time =   43301.84 ms\nllama_perf_context_print: prompt eval time =  101565.89 ms /   634 tokens (  160.20 ms per token,     6.24 tokens per second)\nllama_perf_context_print:        eval time =   65498.89 ms /    84 runs   (  779.75 ms per token,     1.28 tokens per second)\nllama_perf_context_print:       total time =  108909.40 ms /   718 tokens\nllama_perf_context_print:    graphs reused =         81\n1it [01:48, 108.92s/it]Llama.generate: 305 prefix-match hit, remaining 12 prompt tokens to eval\nllama_perf_context_print:        load time =   43301.84 ms\nllama_perf_context_print: prompt eval time =    1770.43 ms /    12 tokens (  147.54 ms per token,     6.78 tokens per second)\nllama_perf_context_print:        eval time =   24930.79 ms /    32 runs   (  779.09 ms per token,     1.28 tokens per second)\nllama_perf_context_print:       total time =   26743.64 ms /    44 tokens\nllama_perf_context_print:    graphs reused =         30\n2it [02:15, 60.58s/it] Llama.generate: 305 prefix-match hit, remaining 13 prompt tokens to eval\nllama_perf_context_print:        load time =   43301.84 ms\nllama_perf_context_print: prompt eval time =    1987.31 ms /    13 tokens (  152.87 ms per token,     6.54 tokens per second)\nllama_perf_context_print:        eval time =   93397.67 ms /   120 runs   (  778.31 ms per token,     1.28 tokens per second)\nllama_perf_context_print:       total time =   95543.36 ms /   133 tokens\nllama_perf_context_print:    graphs reused =        115\n3it [03:51, 76.55s/it]Llama.generate: 305 prefix-match hit, remaining 16 prompt tokens to eval\nllama_perf_context_print:        load time =   43301.84 ms\nllama_perf_context_print: prompt eval time =    2437.39 ms /    16 tokens (  152.34 ms per token,     6.56 tokens per second)\nllama_perf_context_print:        eval time =   32665.19 ms /    42 runs   (  777.74 ms per token,     1.29 tokens per second)\nllama_perf_context_print:       total time =   35156.34 ms /    58 tokens\nllama_perf_context_print:    graphs reused =         40\n4it [04:26, 60.21s/it]Llama.generate: 305 prefix-match hit, remaining 22 prompt tokens to eval\nllama_perf_context_print:        load time =   43301.84 ms\nllama_perf_context_print: prompt eval time =    3109.13 ms /    22 tokens (  141.32 ms per token,     7.08 tokens per second)\nllama_perf_context_print:        eval time =   25531.10 ms /    33 runs   (  773.67 ms per token,     1.29 tokens per second)\nllama_perf_context_print:       total time =   28682.13 ms /    55 tokens\nllama_perf_context_print:    graphs reused =         31\n5it [04:55, 48.84s/it]Llama.generate: 305 prefix-match hit, remaining 16 prompt tokens to eval\nllama_perf_context_print:        load time =   43301.84 ms\nllama_perf_context_print: prompt eval time =    2385.74 ms /    16 tokens (  149.11 ms per token,     6.71 tokens per second)\nllama_perf_context_print:        eval time =   29568.46 ms /    38 runs   (  778.12 ms per token,     1.29 tokens per second)\nllama_perf_context_print:       total time =   32002.74 ms /    54 tokens\nllama_perf_context_print:    graphs reused =         36\n6it [05:27, 43.12s/it]Llama.generate: 305 prefix-match hit, remaining 18 prompt tokens to eval\nllama_perf_context_print:        load time =   43301.84 ms\nllama_perf_context_print: prompt eval time =    2543.35 ms /    18 tokens (  141.30 ms per token,     7.08 tokens per second)\nllama_perf_context_print:        eval time =   30996.89 ms /    40 runs   (  774.92 ms per token,     1.29 tokens per second)\nllama_perf_context_print:       total time =   33591.42 ms /    58 tokens\nllama_perf_context_print:    graphs reused =         38\n7it [06:00, 40.01s/it]Llama.generate: 305 prefix-match hit, remaining 24 prompt tokens to eval\nllama_perf_context_print:        load time =   43301.84 ms\nllama_perf_context_print: prompt eval time =    3371.97 ms /    24 tokens (  140.50 ms per token,     7.12 tokens per second)\nllama_perf_context_print:        eval time =  125709.24 ms /   162 runs   (  775.98 ms per token,     1.29 tokens per second)\nllama_perf_context_print:       total time =  129297.20 ms /   186 tokens\nllama_perf_context_print:    graphs reused =        156\n8it [08:09, 68.43s/it]Llama.generate: 305 prefix-match hit, remaining 21 prompt tokens to eval\nllama_perf_context_print:        load time =   43301.84 ms\nllama_perf_context_print: prompt eval time =    2992.56 ms /    21 tokens (  142.50 ms per token,     7.02 tokens per second)\nllama_perf_context_print:        eval time =   34312.99 ms /    44 runs   (  779.84 ms per token,     1.28 tokens per second)\nllama_perf_context_print:       total time =   37361.22 ms /    65 tokens\nllama_perf_context_print:    graphs reused =         42\n9it [08:47, 58.72s/it]Llama.generate: 305 prefix-match hit, remaining 17 prompt tokens to eval\nllama_perf_context_print:        load time =   43301.84 ms\nllama_perf_context_print: prompt eval time =    2611.14 ms /    17 tokens (  153.60 ms per token,     6.51 tokens per second)\nllama_perf_context_print:        eval time =   49371.33 ms /    63 runs   (  783.67 ms per token,     1.28 tokens per second)\nllama_perf_context_print:       total time =   52062.95 ms /    80 tokens\nllama_perf_context_print:    graphs reused =         60\n10it [09:39, 56.67s/it]Llama.generate: 305 prefix-match hit, remaining 15 prompt tokens to eval\nllama_perf_context_print:        load time =   43301.84 ms\nllama_perf_context_print: prompt eval time =    2240.62 ms /    15 tokens (  149.37 ms per token,     6.69 tokens per second)\nllama_perf_context_print:        eval time =   49444.97 ms /    63 runs   (  784.84 ms per token,     1.27 tokens per second)\nllama_perf_context_print:       total time =   51767.02 ms /    78 tokens\nllama_perf_context_print:    graphs reused =         61\n11it [10:31, 55.17s/it]Llama.generate: 305 prefix-match hit, remaining 15 prompt tokens to eval\nllama_perf_context_print:        load time =   43301.84 ms\nllama_perf_context_print: prompt eval time =    2158.81 ms /    15 tokens (  143.92 ms per token,     6.95 tokens per second)\nllama_perf_context_print:        eval time =   27940.94 ms /    36 runs   (  776.14 ms per token,     1.29 tokens per second)\nllama_perf_context_print:       total time =   30146.51 ms /    51 tokens\nllama_perf_context_print:    graphs reused =         34\n12it [11:01, 47.56s/it]Llama.generate: 305 prefix-match hit, remaining 17 prompt tokens to eval\nllama_perf_context_print:        load time =   43301.84 ms\nllama_perf_context_print: prompt eval time =    2460.31 ms /    17 tokens (  144.72 ms per token,     6.91 tokens per second)\nllama_perf_context_print:        eval time =   84823.28 ms /   109 runs   (  778.20 ms per token,     1.29 tokens per second)\nllama_perf_context_print:       total time =   87427.03 ms /   126 tokens\nllama_perf_context_print:    graphs reused =        105\n13it [12:28, 59.64s/it]Llama.generate: 305 prefix-match hit, remaining 18 prompt tokens to eval\nllama_perf_context_print:        load time =   43301.84 ms\nllama_perf_context_print: prompt eval time =    2608.02 ms /    18 tokens (  144.89 ms per token,     6.90 tokens per second)\nllama_perf_context_print:        eval time =   40428.95 ms /    52 runs   (  777.48 ms per token,     1.29 tokens per second)\nllama_perf_context_print:       total time =   43103.12 ms /    70 tokens\nllama_perf_context_print:    graphs reused =         50\n14it [13:11, 56.56s/it]","output_type":"stream"},{"name":"stdout","text":"\n=== ОТВЕТЫ ===\n\n❓ Кто написал Евгения Онегина?\n💬 1. **Сначала ответь сам.**\n   Автор романа \"Евгений Онегин\" — Александр Сергеевич Пушкин.\n\n2. **Затем проверь по источникам.**\n   Источники подтверждают, что автором \"Евгения Онегина\" является Александр Сергеевич Пушкин.\n\n3. **Синтезируй финальный ответ по правилам.**\n   а) Источники подтверждают мои знания.\n\n**CORRECT ANSWER:** Александр Сергеевич Пушкин.\n\n\n❓ Когда распался Советский Союз?\n💬 Советский Союз распался 26 декабря 1991 года.\n\nCORRECT ANSWER: 26 декабря 1991 года\n\n\n❓ Что такое квантовая запутанность?\n💬 **Квантовая запутанность** — это феномен в квантовой механике, при котором состояние двух или более частиц становится взаимосвязанным таким образом, что изменение состояния одной частицы мгновенно влияет на состояние другой, независимо от расстояния между ними. Этот эффект не может быть объяснен классической физикой и является одним из ключевых аспектов квантовой механики.\n\nCORRECT ANSWER: Квантовая запутанность — это феномен, при котором состояние двух или более частиц становится взаимосвязанным, так что изменение состояния одной частицы мгновенно влияет на состояние другой, независимо от расстояния между ними, что противоречит классической физике.\n\n\n❓ Кто автор книги \"Детство в Соломбале\"?\n💬 **CORRECT ANSWER: [не знаю]**\n\nМои знания не содержат информации об авторе книги \"Детство в Соломбале\". Возможно, это название является ошибочным или относится к малоизвестному произведению.\n\n\n❓ Как зовут человека, который написал «Детство в Соломбале»?\n💬 CORRECT ANSWER: Николай Жданов\n\nИсточники подтверждают, что автором произведения \"Детство в Солобалье\" является Николай Жданов, советский писатель и журналист.\n\n\n❓ Кто написал книгу «Детство в Соломбале»?\n💬 CORRECT ANSWER: [не знаю]\n\nМои знания не включают информацию о книге \"Детство в Соломбале\". Возможно, это произведение не является широко известным или не было опубликовано.\n\n\n❓ В каком году начал поддерживаться сеттинг Eberron?\n💬 CORRECT ANSWER: Ebberon начал поддерживаться в 2004 году, когда был выпущен первый официальный материал по этому сеттингу в рамках Dungeons & Dragons 3.5.\n\n\n❓ Какое окрашивание приобретает безводный метанол при растворении небольшого количества медного купороса?\n💬 **CORRECT ANSWER:** [синий]\n\n**Объяснение:**\n\n1. **Мой ответ:** Безводный метанол приобретает синий цвет при растворении небольшого количества медного купороса.\n\n2. **Проверка по источникам:**\n   - Метанол (CH₃OH) является бесцветной жидкостью.\n   - Медный купорос (CuSO₄·5H₂O) при растворении в метаноле образует комплекс с медью, который имеет синий цвет.\n\n3. **Синтез финального ответа:**\n   - Источники подтверждают, что при растворении медного купороса в безводном метаноле образуется синий комплекс.\n\nТаким образом, безводный метанол приобретает синий цвет при растворении небольшого количества медного купороса.\n\n\n❓ Какой античный математик изобрёл первый дизельный двигатель внутреннего сгорания?\n💬 CORRECT ANSWER: [не знаю]\n\nАнтичный математик не мог изобрести дизельный двигатель внутреннего сгорания, так как это изобретение относится к XIX веку и связано с работами Рудольфа Дизеля.\n\n\n❓ Кто первый человек высадившийся на солнце?\n💬 CORRECT ANSWER: [не знаю]\n\nЯ не знаю ни одного случая, когда человек высаживался на Солнце. Это физически невозможно из-за экстремальных условий: температура поверхности Солнца составляет около 5500°C, а плотность плазмы настолько высока, что человек не сможет выжить там даже короткое время.\n\n\n❓ Как звали отца А.С. Пушкина?\n💬 **Ответ:** Сергей Львович Пушкин\n\n**Проверка по источникам:**\n\n1. **Пушкин, Сергей Львович** (1770–1848) — отец Александра Сергеевича Пушкина.\n\n**Синтез финального ответа:**\n\nCORRECT ANSWER: Сергей Львович Пушкин\n\n\n❓ Какая компания осуществляет доставку роботами в Москве?\n💬 CORRECT ANSWER: [не знаю]\n\nМои знания не содержат информации о компаниях, осуществляющих доставку роботами в Москве. Возможно, это новая или малоизвестная услуга.\n\n\n❓ Кто имеет крупнейшую доставку роботами на территории Москвы?\n💬 CORRECT ANSWER: [не знаю]\n\nЯ не располагаю информацией о крупнейших службах доставки роботами в Москве. Эта тема относительно новая и может быть недостаточно освещена в общедоступных источниках.\n\nИсточники:\n1. \"Роботы в логистике: как они меняют рынок доставки\" - статья на сайте Logistic.ru\n2. \"Москва становится лидером в использовании роботов для доставки\" - статья на сайте Moscow.ru\n\nЭти источники не содержат информации о конкретных компаниях, предоставляющих крупнейшую доставку роботами в Москве.\n\n\n❓ В Москве есть доставка роботами, какая компания выполняет её?\n💬 CORRECT ANSWER: [не знаю]\n\nПосле анализа предоставленных источников, я не обнаружил информации о компании, которая бы осуществляла доставку роботами в Москве. Возможно, эта услуга предоставляется небольшими стартапами или экспериментальными проектами, которые не получили широкого распространения.\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
