{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Без catboost никуда"
      ],
      "metadata": {
        "id": "YE6ux_PCigul"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oscQLac-Azz",
        "outputId": "a9bd07b1-9190-45b9-cb53-19a370a742b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "Ysc-hnuiXU9W"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import zipfile\n",
        "import os\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from datasets import Dataset\n",
        "from sklearn.neighbors import KDTree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import warnings\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "import requests\n",
        "import geopandas as gpd\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "igSDLGYRTSoI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def haversine(lat1, lon1, lat2=55.7558, lon2=37.6176):\n",
        "\n",
        "    R = 6371.0\n",
        "\n",
        "    lat1_rad = math.radians(lat1)\n",
        "    lon1_rad = math.radians(lon1)\n",
        "    lat2_rad = math.radians(lat2)\n",
        "    lon2_rad = math.radians(lon2)\n",
        "\n",
        "    dlat = lat2_rad - lat1_rad\n",
        "    dlon = lon2_rad - lon1_rad\n",
        "\n",
        "    a = math.sin(dlat / 2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dlon / 2)**2\n",
        "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
        "\n",
        "    distance = R * c\n",
        "    return distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "0cjCXGdIBSaT",
        "outputId": "e3bc14ba-876b-4fad-b0b8-c4490263bc30"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bea6d086-b667-4c9c-9d49-67855d0346a6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bea6d086-b667-4c9c-9d49-67855d0346a6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"quezaltendes\",\"key\":\"34423f987893d29a84a13fe36bc78630\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "Bg1Wqt6vBV6-"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44UoMjBqBXXs",
        "outputId": "7b459850-3687-48cc-cfcc-f278ef5a84bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/quezaltendes/dewlei\n",
            "License(s): unknown\n",
            "dewlei.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d quezaltendes/dewlei"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVliDKnIBZEh",
        "outputId": "babd02d8-5b58-4818-f29d-ec6448b22e3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "def unzip_basic(zip_path, extract_to=None):\n",
        "\n",
        "    if extract_to is None:\n",
        "        extract_to = os.path.dirname(zip_path) or '.'\n",
        "\n",
        "    os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "        print(extract_to)\n",
        "\n",
        "\n",
        "unzip_basic('/content/dewlei.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWi4d2GiY3eY"
      },
      "source": [
        "## все выше - загрузка датасета и импорт библиотек!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "7ShLqoNAX-gW"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('/content/train.tsv (1)/train.tsv', sep='\\t')\n",
        "test_data = pd.read_csv('/content/test.tsv (1)/test.tsv', sep='\\t')\n",
        "reviews = pd.read_csv('/content/reviews.txv/reviews.tsv', sep='\\t')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "0dabGfQrBIWQ"
      },
      "outputs": [],
      "source": [
        "test_for_inference = pd.read_csv('/content/test.tsv (1)/test.tsv', sep='\\t')\n",
        "train_for_inference = pd.read_csv('/content/train.tsv (1)/train.tsv', sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "XoA727cG5PQa"
      },
      "outputs": [],
      "source": [
        "y_train = train_data['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "YAms88b3m3Kp",
        "outputId": "11a491c0-2d89-437e-91c9-d3c9fcceba38"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "address\n",
              "Кировоградская ул., 13А, Москва            59\n",
              "Ходынский бул., 4, Москва                  58\n",
              "просп. Мира, 211, корп. 2, Москва          52\n",
              "площадь Киевского Вокзала, 2, Москва       51\n",
              "Дмитровское ш., 163А, Москва               49\n",
              "                                           ..\n",
              "Заречная ул., 6, д. Рузино                  1\n",
              "Советская ул., 7, село Атепцево             1\n",
              "Шелепихинская наб., 42, корп. 1, Москва     1\n",
              "Кленовый бул., 28, Москва                   1\n",
              "Поклонная ул., 3, корп. 2, Москва           1\n",
              "Name: count, Length: 22155, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>address</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Кировоградская ул., 13А, Москва</th>\n",
              "      <td>59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ходынский бул., 4, Москва</th>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>просп. Мира, 211, корп. 2, Москва</th>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>площадь Киевского Вокзала, 2, Москва</th>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Дмитровское ш., 163А, Москва</th>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Заречная ул., 6, д. Рузино</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Советская ул., 7, село Атепцево</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Шелепихинская наб., 42, корп. 1, Москва</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Кленовый бул., 28, Москва</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Поклонная ул., 3, корп. 2, Москва</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>22155 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "train_data['address'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr_imr-y_7J7"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF оказался полезнее BERT, прогнал еще через SVD"
      ],
      "metadata": {
        "id": "ZgLKaKmxlZaA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "Qno4fGp-W7dn"
      },
      "outputs": [],
      "source": [
        "\n",
        "def extract_text_features(reviews_df):\n",
        "\n",
        "\n",
        "    reviews_agg = reviews_df.groupby('id')['text'].agg(' '.join).reset_index()\n",
        "\n",
        "\n",
        "    tfidf = TfidfVectorizer(\n",
        "        max_features=5000,\n",
        "        ngram_range=(1, 3),\n",
        "        stop_words=['и', 'в', 'на', 'с', 'у', 'к', 'по', 'для', 'это', 'то', 'так']\n",
        "    )\n",
        "    tfidf_matrix = tfidf.fit_transform(reviews_agg['text'])\n",
        "\n",
        "\n",
        "    svd = TruncatedSVD(n_components=100, random_state=42)\n",
        "    tfidf_reduced = svd.fit_transform(tfidf_matrix)\n",
        "\n",
        "\n",
        "\n",
        "    return reviews_agg, tfidf_reduced,\n",
        "reviews_agg, tfidf_reduced =extract_text_features(reviews)\n",
        "import pandas as pd\n",
        "\n",
        "tfidf_array = tfidf_reduced\n",
        "tfidf_df = pd.DataFrame(tfidf_array, index=reviews_agg.index)\n",
        "\n",
        "tfidf_df.columns = [f'tfidf_{i}' for i in range(tfidf_df.shape[1])]\n",
        "\n",
        "result = pd.concat([reviews_agg, tfidf_df], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "tabDLz9eQk9m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "a0eb8529-1d9a-4abe-fc41-277425f60e2d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import torch\\nfrom transformers import AutoTokenizer, AutoModel\\nimport numpy as np\\nimport pandas as pd\\nfrom tqdm import tqdm\\n\\ndef extract_bert_features(reviews_df, model_name=\"deeppavlov/rubert-base-cased\"):\\n\\n\\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n\\n    reviews_agg = reviews_df.groupby(\"id\")[\"text\"].agg(\" \".join).reset_index()\\n\\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\\n    model = AutoModel.from_pretrained(model_name).to(device)\\n    model.eval()\\n\\n    embeddings = []\\n\\n    for text in tqdm(reviews_agg[\"text\"], desc=\"Extracting BERT embeddings\"):\\n        inputs = tokenizer(\\n            text,\\n            return_tensors=\"pt\",\\n            truncation=True,\\n            max_length=128,\\n            padding=\"max_length\"\\n        ).to(device)\\n\\n        with torch.no_grad():\\n            outputs = model(**inputs)\\n            emb = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\\n\\n        embeddings.append(emb[0])\\n\\n    bert_array = np.vstack(embeddings)\\n    bert_df = pd.DataFrame(bert_array, columns=[f\"bert_{i}\" for i in range(bert_array.shape[1])])\\n\\n    result = pd.concat([reviews_agg, bert_df], axis=1)\\n    return reviews_agg, bert_df, result'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "'''import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "def extract_bert_features(reviews_df, model_name=\"deeppavlov/rubert-base-cased\"):\n",
        "\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    reviews_agg = reviews_df.groupby(\"id\")[\"text\"].agg(\" \".join).reset_index()\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    embeddings = []\n",
        "\n",
        "    for text in tqdm(reviews_agg[\"text\"], desc=\"Extracting BERT embeddings\"):\n",
        "        inputs = tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            padding=\"max_length\"\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            emb = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "\n",
        "        embeddings.append(emb[0])\n",
        "\n",
        "    bert_array = np.vstack(embeddings)\n",
        "    bert_df = pd.DataFrame(bert_array, columns=[f\"bert_{i}\" for i in range(bert_array.shape[1])])\n",
        "\n",
        "    result = pd.concat([reviews_agg, bert_df], axis=1)\n",
        "    return reviews_agg, bert_df, result'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "S6wmCiXVQp6_",
        "outputId": "92da3a2c-d2ac-416e-87ff-ee95ad579f1d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"reviews_agg_bert, bert_df, result_bert = extract_bert_features(reviews)\\n\\nfull_features = result.merge(result_bert.drop(columns=['text']), on='id', how='left')\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "'''reviews_agg_bert, bert_df, result_bert = extract_bert_features(reviews)\n",
        "\n",
        "full_features = result.merge(result_bert.drop(columns=['text']), on='id', how='left')'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "lrhq-S_XC6Ff"
      },
      "outputs": [],
      "source": [
        "reviews_agg, tfidf_reduced = extract_text_features(reviews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "bMPrbBafC95g"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "tfidf_array = tfidf_reduced\n",
        "tfidf_df = pd.DataFrame(tfidf_array, index=reviews_agg.index)\n",
        "\n",
        "tfidf_df.columns = [f'tfidf_{i}' for i in range(tfidf_df.shape[1])]\n",
        "\n",
        "result = pd.concat([reviews_agg, tfidf_df], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "8srVbgQGkqDD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "66f49dcf-044b-45a5-8459-7eae436d11c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nassert 'id' in result.columns and 'id' in result_bert.columns\\n\\ntext_features_full = (\\n    result\\n    .drop(columns=['text'], errors='ignore')\\n    .merge(\\n        result_bert.drop(columns=['text'], errors='ignore'),\\n        on='id',\\n        how='outer'\\n    )\\n)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "'''\n",
        "assert 'id' in result.columns and 'id' in result_bert.columns\n",
        "\n",
        "text_features_full = (\n",
        "    result\n",
        "    .drop(columns=['text'], errors='ignore')\n",
        "    .merge(\n",
        "        result_bert.drop(columns=['text'], errors='ignore'),\n",
        "        on='id',\n",
        "        how='outer'\n",
        "    )\n",
        ")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpfBfoneQizy"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Работает с основной частью данных, развертываем coordinates"
      ],
      "metadata": {
        "id": "ydJHM_tJlljG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RUZx-YrplkAK"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "gYiZo5QPfzU-"
      },
      "outputs": [],
      "source": [
        "train_data = train_data[train_data['target'] > 0.999999]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "T-QEAF6UA4EM"
      },
      "outputs": [],
      "source": [
        "train_data[['longitude', 'latitude']] = train_data['coordinates'].str.strip('[]').str.split(',', expand=True).astype(float)\n",
        "test_data[['longitude', 'latitude']] = test_data['coordinates'].str.strip('[]').str.split(',', expand=True).astype(float)\n",
        "\n",
        "train_data['distance_to_moscow_center_km'] = train_data.apply(\n",
        "    lambda row: haversine(row['latitude'], row['longitude']), axis=1\n",
        ")\n",
        "test_data['distance_to_moscow_center_km'] = test_data.apply(\n",
        "    lambda row: haversine(row['latitude'], row['longitude']), axis=1\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FX_VzsATls3V"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Берем абсолютно все из текста, мне это в итоге помогло"
      ],
      "metadata": {
        "id": "zF09bYUjlt1C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "LrJX8JRsnj72"
      },
      "outputs": [],
      "source": [
        "\n",
        "def add_review_features(main_data, reviews_file='/content/reviews.txv/reviews.tsv'):\n",
        "\n",
        "    try:\n",
        "        reviews = pd.read_csv(reviews_file, sep='\\t')\n",
        "\n",
        "        reviews['text_length'] = reviews['text'].str.len().fillna(0)\n",
        "        reviews['word_count'] = reviews['text'].str.split().str.len().fillna(0)\n",
        "        reviews['char_count'] = reviews['text'].str.replace(' ', '').str.len().fillna(0)\n",
        "\n",
        "        reviews['exclamation_count'] = reviews['text'].str.count('!').fillna(0)\n",
        "        reviews['question_count'] = reviews['text'].str.count('\\?').fillna(0)\n",
        "        reviews['capital_ratio'] = reviews['text'].apply(\n",
        "            lambda x: sum(1 for char in str(x) if char.isupper()) / len(str(x)) if len(str(x)) > 0 else 0\n",
        "        )\n",
        "\n",
        "        reviews['comma_count'] = reviews['text'].str.count(',').fillna(0)\n",
        "        reviews['dot_count'] = reviews['text'].str.count('\\.').fillna(0)\n",
        "        reviews['ellipsis_count'] = reviews['text'].str.count('\\.\\.\\.').fillna(0)\n",
        "        reviews['line_breaks_count'] = reviews['text'].str.count('\\n').fillna(0)\n",
        "        reviews['has_quotes'] = reviews['text'].str.contains('\"|\\'').fillna(0).astype(int)\n",
        "\n",
        "        def simple_sentiment_analysis(text):\n",
        "            if pd.isna(text):\n",
        "                return 0\n",
        "            text = str(text).lower()\n",
        "            positive_words = ['хорош', 'отличн', 'прекрасн', 'рекоменд', 'супер',\n",
        "                            'класс', 'любим', 'удовольств', 'замечательн', 'восхитительн',\n",
        "                            'быстро', 'вежлив', 'чист', 'комфортн', 'вкусн', 'спасибо',\n",
        "                            'благодар', 'совету', 'порадова', 'понрави']\n",
        "            negative_words = ['плох', 'ужасн', 'кошмар', 'разочарован', 'не рекоменд',\n",
        "                            'отвратительн', 'груб', 'грязн', 'дорог', 'долго',\n",
        "                            'медленно', 'невкусн', 'шумн', 'тесн', 'обман', 'жаль',\n",
        "                            'напугал', 'отврат', 'противн', 'увол']\n",
        "\n",
        "            pos_count = sum(1 for word in positive_words if word in text)\n",
        "            neg_count = sum(1 for word in negative_words if word in text)\n",
        "\n",
        "            total = pos_count + neg_count\n",
        "            if total > 0:\n",
        "                return (pos_count - neg_count) / total\n",
        "            return 0\n",
        "\n",
        "        reviews['sentiment'] = reviews['text'].apply(simple_sentiment_analysis)\n",
        "\n",
        "\n",
        "        def get_sentiment_categories(text):\n",
        "            if pd.isna(text):\n",
        "                return 0, 0, 0\n",
        "            text = str(text).lower()\n",
        "\n",
        "\n",
        "            service_words = ['обслуживан', 'персонал', 'сотрудник', 'администратор', 'официант']\n",
        "            quality_words = ['качеств', 'уровен', 'стандарт', 'профессионал']\n",
        "            price_words = ['цен', 'стоим', 'дорог', 'дешев', 'выгодн']\n",
        "            atmosphere_words = ['атмосфер', 'интерьер', 'уютн', 'комфорт', 'обстановк']\n",
        "\n",
        "            service_count = sum(1 for word in service_words if word in text)\n",
        "            quality_count = sum(1 for word in quality_words if word in text)\n",
        "            price_count = sum(1 for word in price_words if word in text)\n",
        "            atmosphere_count = sum(1 for word in atmosphere_words if word in text)\n",
        "\n",
        "            total_mentions = service_count + quality_count + price_count + atmosphere_count\n",
        "            if total_mentions > 0:\n",
        "                return service_count/total_mentions, quality_count/total_mentions, price_count/total_mentions\n",
        "            return 0, 0, 0\n",
        "\n",
        "        reviews[['service_mention_ratio', 'quality_mention_ratio', 'price_mention_ratio']] = \\\n",
        "            reviews['text'].apply(lambda x: pd.Series(get_sentiment_categories(x)))\n",
        "\n",
        "        reviews['unique_words_ratio'] = reviews['text'].apply(\n",
        "            lambda x: len(set(str(x).split())) / len(str(x).split()) if len(str(x).split()) > 0 else 0\n",
        "        )\n",
        "\n",
        "        reviews['avg_word_length'] = reviews['char_count'] / reviews['word_count'].replace(0, 1)\n",
        "        reviews['has_emoji'] = reviews['text'].str.contains('[^\\x00-\\x7F]').fillna(0).astype(int)\n",
        "\n",
        "        reviews['long_words_ratio'] = reviews['text'].apply(\n",
        "            lambda x: sum(1 for word in str(x).split() if len(word) > 6) / len(str(x).split())\n",
        "            if len(str(x).split()) > 0 else 0\n",
        "        )\n",
        "\n",
        "        reviews['sentence_count'] = reviews['text'].str.count('[.!?]+').fillna(0)\n",
        "        reviews['avg_sentence_length'] = reviews['word_count'] / reviews['sentence_count'].replace(0, 1)\n",
        "\n",
        "        review_stats = reviews.groupby('id').agg({\n",
        "            'text': 'count',\n",
        "            'text_length': ['mean', 'std', 'max', 'min'],\n",
        "            'word_count': ['mean', 'std', 'max', 'min'],\n",
        "            'char_count': ['mean', 'std'],\n",
        "            'exclamation_count': ['mean', 'sum', 'max'],\n",
        "            'question_count': ['mean', 'sum', 'max'],\n",
        "            'capital_ratio': 'mean',\n",
        "            'comma_count': 'mean',\n",
        "            'dot_count': 'mean',\n",
        "            'ellipsis_count': 'sum',\n",
        "            'line_breaks_count': 'mean',\n",
        "            'has_quotes': 'mean',\n",
        "            'sentiment': ['mean', 'std', 'min', 'max', lambda x: (x > 0.1).sum(), lambda x: (x < -0.1).sum()],\n",
        "            'service_mention_ratio': 'mean',\n",
        "            'quality_mention_ratio': 'mean',\n",
        "            'price_mention_ratio': 'mean',\n",
        "            'unique_words_ratio': 'mean',\n",
        "            'avg_word_length': 'mean',\n",
        "            'has_emoji': 'mean',\n",
        "            'long_words_ratio': 'mean',\n",
        "            'sentence_count': 'mean',\n",
        "            'avg_sentence_length': 'mean'\n",
        "        }).reset_index()\n",
        "\n",
        "        review_stats.columns = [\n",
        "            'id',\n",
        "            'review_count',\n",
        "            'avg_text_length', 'std_text_length', 'max_text_length', 'min_text_length',\n",
        "            'avg_word_count', 'std_word_count', 'max_word_count', 'min_word_count',\n",
        "            'avg_char_count', 'std_char_count',\n",
        "            'avg_exclamation', 'total_exclamation', 'max_exclamation',\n",
        "            'avg_question', 'total_question', 'max_question',\n",
        "            'avg_capital_ratio',\n",
        "            'avg_comma_count',\n",
        "            'avg_dot_count',\n",
        "            'total_ellipsis',\n",
        "            'avg_line_breaks',\n",
        "            'quotes_ratio',\n",
        "            'avg_sentiment', 'std_sentiment', 'min_sentiment', 'max_sentiment', 'positive_reviews_count', 'negative_reviews_count',\n",
        "            'avg_service_mention',\n",
        "            'avg_quality_mention',\n",
        "            'avg_price_mention',\n",
        "            'avg_unique_words_ratio',\n",
        "            'avg_word_length',\n",
        "            'emoji_ratio',\n",
        "            'avg_long_words_ratio',\n",
        "            'avg_sentence_count',\n",
        "            'avg_sentence_length'\n",
        "        ]\n",
        "        review_stats['sentiment_volatility'] = review_stats['std_sentiment'].fillna(0)\n",
        "        review_stats['text_length_variation'] = review_stats['std_text_length'] / review_stats['avg_text_length'].replace(0, 1)\n",
        "        review_stats['text_length_variation'] = review_stats['text_length_variation'].replace([np.inf, -np.inf], 0)\n",
        "        review_stats['emotional_intensity'] = (review_stats['avg_exclamation'] + review_stats['avg_question']) / 2\n",
        "        review_stats['review_engagement'] = (review_stats['avg_text_length'] * review_stats['emotional_intensity'])\n",
        "        review_stats['has_positive_reviews'] = (review_stats['max_sentiment'] > 0.1).astype(int)\n",
        "        review_stats['has_negative_reviews'] = (review_stats['min_sentiment'] < -0.1).astype(int)\n",
        "        review_stats['has_mixed_reviews'] = ((review_stats['max_sentiment'] > 0.1) &\n",
        "                                           (review_stats['min_sentiment'] < -0.1)).astype(int)\n",
        "\n",
        "        review_stats['positive_negative_ratio'] = review_stats['positive_reviews_count'] / (review_stats['negative_reviews_count'] + 1)\n",
        "        review_stats['sentiment_balance'] = (review_stats['positive_reviews_count'] - review_stats['negative_reviews_count']) / review_stats['review_count'].replace(0, 1)\n",
        "        review_stats['text_complexity_index'] = (review_stats['avg_long_words_ratio'] + review_stats['avg_unique_words_ratio']) / 2\n",
        "        review_stats['punctuation_diversity'] = (review_stats['avg_exclamation'] + review_stats['avg_question'] + review_stats['avg_comma_count']) / 3\n",
        "        review_stats['has_detailed_reviews'] = (review_stats['avg_word_count'] > 20).astype(int)\n",
        "        review_stats['has_emotional_reviews'] = (review_stats['emotional_intensity'] > 0.5).astype(int)\n",
        "        review_stats['has_structured_reviews'] = (review_stats['avg_sentence_count'] > 2).astype(int)\n",
        "\n",
        "        main_data = main_data.merge(review_stats, on='id', how='left')\n",
        "\n",
        "        review_columns = [col for col in review_stats.columns if col != 'id']\n",
        "        for col in review_columns:\n",
        "            if 'count' in col or 'total' in col or 'max' in col or 'min' in col:\n",
        "                main_data[col] = main_data[col].fillna(0)\n",
        "            elif col.startswith('has_') or col.endswith('_ratio') or 'balance' in col or 'index' in col:\n",
        "                main_data[col] = main_data[col].fillna(0)\n",
        "            else:\n",
        "                main_data[col] = main_data[col].fillna(0)\n",
        "\n",
        "\n",
        "\n",
        "        return main_data\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "\n",
        "        base_features = [\n",
        "            'review_count', 'avg_text_length', 'std_text_length', 'max_text_length', 'min_text_length',\n",
        "            'avg_word_count', 'std_word_count', 'max_word_count', 'min_word_count', 'avg_char_count',\n",
        "            'std_char_count', 'avg_exclamation', 'total_exclamation', 'max_exclamation', 'avg_question',\n",
        "            'total_question', 'max_question', 'avg_capital_ratio', 'avg_comma_count', 'avg_dot_count',\n",
        "            'total_ellipsis', 'avg_line_breaks', 'quotes_ratio', 'avg_sentiment', 'std_sentiment',\n",
        "            'min_sentiment', 'max_sentiment', 'positive_reviews_count', 'negative_reviews_count',\n",
        "            'avg_service_mention', 'avg_quality_mention', 'avg_price_mention', 'avg_unique_words_ratio',\n",
        "            'avg_word_length', 'emoji_ratio', 'avg_long_words_ratio', 'avg_sentence_count',\n",
        "            'avg_sentence_length', 'sentiment_volatility', 'text_length_variation', 'emotional_intensity',\n",
        "            'review_engagement', 'has_positive_reviews', 'has_negative_reviews', 'has_mixed_reviews',\n",
        "            'positive_negative_ratio', 'sentiment_balance', 'text_complexity_index', 'punctuation_diversity',\n",
        "            'has_detailed_reviews', 'has_emotional_reviews', 'has_structured_reviews'\n",
        "        ]\n",
        "\n",
        "        for feature in base_features:\n",
        "            main_data[feature] = 0\n",
        "\n",
        "        return main_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "h2ek4xKf8iYe"
      },
      "outputs": [],
      "source": [
        "train_data = add_review_features(train_data)\n",
        "test_data = add_review_features(test_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Хочется ввести балльную систему по доходам"
      ],
      "metadata": {
        "id": "1BPpcx65l04e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "zp-hojoP_Nck"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def fqs(df):\n",
        "  income_weights = {\n",
        "      'below_average_income_1000m': 1,\n",
        "      'average_income_1000m': 2,\n",
        "      'above_average_income_1000m': 3,\n",
        "      'high_income_1000m': 4,\n",
        "      'premium_income_1000m': 5\n",
        "  }\n",
        "\n",
        "  df['wealth_numerator_1000m'] = sum(\n",
        "      df[col] * weight for col, weight in income_weights.items()\n",
        "  )\n",
        "\n",
        "  df['wealth_denominator_1000m'] = df[\n",
        "      list(income_weights.keys())\n",
        "  ].sum(axis=1) + 1e-6\n",
        "\n",
        "\n",
        "  df['wealth_index_1000m'] = df['wealth_numerator_1000m'] / df['wealth_denominator_1000m']\n",
        "\n",
        "  df.drop(['wealth_numerator_1000m', 'wealth_denominator_1000m'], axis=1, inplace=True)\n",
        "  return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "zKV7gDn7_VzA"
      },
      "outputs": [],
      "source": [
        "train_data = fqs(train_data)\n",
        "test_data = fqs(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "nsi7rEmgd7cd"
      },
      "outputs": [],
      "source": [
        "category_freq = pd.concat([train_data['category'], test_data['category']], axis=0).value_counts()\n",
        "train_data['category_freq'] = train_data['category'].map(category_freq)\n",
        "test_data['category_freq'] = test_data['category'].map(category_freq).fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "GTaaIvFnek2V"
      },
      "outputs": [],
      "source": [
        "category_target_mean = train_data.groupby('category')['target'].mean()\n",
        "train_data['category_target_enc'] = train_data['category'].map(category_target_mean)\n",
        "test_data['category_target_enc'] = test_data['category'].map(category_target_mean).fillna(train_data['target'].mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "0K3KCiYSCJsh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "s1e-ZEeqEai6"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "coords_all = pd.concat([\n",
        "    train_data[[\"longitude\", \"latitude\"]],\n",
        "    test_data[[\"longitude\", \"latitude\"]]\n",
        "], axis=0, ignore_index=True)\n",
        "\n",
        "\n",
        "N_CLUSTERS = 100\n",
        "kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=42, n_init=\"auto\")\n",
        "\n",
        "\n",
        "coords_all[\"cluster\"] = kmeans.fit_predict(coords_all)\n",
        "\n",
        "train_data[\"cluster\"] = coords_all[\"cluster\"].iloc[:len(train_data)].values\n",
        "test_data[\"cluster\"] = coords_all[\"cluster\"].iloc[len(train_data):].values\n",
        "\n",
        "\n",
        "cluster_stats = (\n",
        "    train_data.groupby(\"cluster\")[\"target\"]\n",
        "    .agg([\n",
        "        (\"mean_target_in_cluster\", \"mean\"),\n",
        "        (\"std_target_in_cluster\", \"std\"),\n",
        "        (\"count_in_cluster\", \"count\"),\n",
        "        (\"median_target_in_cluster\", \"median\"),\n",
        "    ])\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "train_data = train_data.merge(cluster_stats, on=\"cluster\", how=\"left\")\n",
        "test_data = test_data.merge(cluster_stats, on=\"cluster\", how=\"left\")\n",
        "\n",
        "train_data[\"std_target_in_cluster\"].fillna(0, inplace=True)\n",
        "test_data[\"std_target_in_cluster\"].fillna(0, inplace=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "Oo9btVyfGuj2"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import BallTree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "D1aRFq_1GdId"
      },
      "outputs": [],
      "source": [
        "\n",
        "def add_density_features(df):\n",
        "\n",
        "    coords = np.radians(df[['latitude', 'longitude']].values)\n",
        "\n",
        "    tree = BallTree(coords, metric='haversine')\n",
        "\n",
        "    r_300m = 0.3 / 6371\n",
        "    r_1000m = 1.0 / 6371\n",
        "\n",
        "    count_300m = tree.query_radius(coords, r=r_300m, count_only=True)\n",
        "    count_1000m = tree.query_radius(coords, r=r_1000m, count_only=True)\n",
        "\n",
        "    df['density_300m'] = count_300m - 1\n",
        "    df['density_1000m'] = count_1000m - 1\n",
        "\n",
        "    return df\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "w8emEk_WLTz2"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.neighbors import BallTree\n",
        "\n",
        "def add_same_category_features(df):\n",
        "\n",
        "    R = 6371\n",
        "    r_300m = 0.3 / R\n",
        "    r_1000m = 1.0 / R\n",
        "\n",
        "    df['latitude_rad'] = np.radians(df['latitude'])\n",
        "    df['longitude_rad'] = np.radians(df['longitude'])\n",
        "\n",
        "\n",
        "    df['nearest_same_category_dist'] = np.nan\n",
        "    df['same_category_count_300m'] = 0\n",
        "    df['same_category_count_1000m'] = 0\n",
        "\n",
        "    for cat in df['category'].unique():\n",
        "        mask = df['category'] == cat\n",
        "        coords = np.vstack((df.loc[mask, 'latitude_rad'], df.loc[mask, 'longitude_rad'])).T\n",
        "\n",
        "        if len(coords) < 2:\n",
        "            df.loc[mask, ['nearest_same_category_dist',\n",
        "                          'same_category_count_300m',\n",
        "                          'same_category_count_1000m']] = 0\n",
        "            continue\n",
        "\n",
        "\n",
        "        tree = BallTree(coords, metric='haversine')\n",
        "\n",
        "\n",
        "        dist, _ = tree.query(coords, k=2)\n",
        "        nearest_dist = dist[:, 1] * R * 1000\n",
        "\n",
        "\n",
        "        count_300m = tree.query_radius(coords, r=r_300m, count_only=True) - 1\n",
        "        count_1000m = tree.query_radius(coords, r=r_1000m, count_only=True) - 1\n",
        "\n",
        "        df.loc[mask, 'nearest_same_category_dist'] = nearest_dist\n",
        "        df.loc[mask, 'same_category_count_300m'] = count_300m\n",
        "        df.loc[mask, 'same_category_count_1000m'] = count_1000m\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "NSMXAhBCGRMk"
      },
      "outputs": [],
      "source": [
        "concatef = pd.concat([train_data.drop(columns=['target']), test_data], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "0d37bLh6GeKR"
      },
      "outputs": [],
      "source": [
        "concatef = add_density_features(concatef)\n",
        "concatef = add_same_category_features(concatef)\n",
        "\n",
        "\n",
        "address_counts = concatef['address'].value_counts()\n",
        "name_counts = concatef['name'].value_counts()\n",
        "category_counts = concatef['category'].value_counts()\n",
        "\n",
        "concatef['address_count'] = concatef['address'].map(address_counts)\n",
        "concatef['name_count'] = concatef['name'].map(name_counts)\n",
        "concatef['category_count'] = concatef['category'].map(category_counts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "qxZXj2_8GxzW"
      },
      "outputs": [],
      "source": [
        "train_data = pd.concat([concatef[:train_data.shape[0]], train_data['target']], axis=1)\n",
        "test_data = concatef[train_data.shape[0]:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "a_i5xmDPzHU-",
        "outputId": "cb45490c-cd7e-4eb2-b563-7258e64c3d1e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id                            0\n",
              "name                          0\n",
              "coordinates                   0\n",
              "category                      0\n",
              "address                      20\n",
              "                             ..\n",
              "same_category_count_1000m     0\n",
              "address_count                20\n",
              "name_count                    0\n",
              "category_count                0\n",
              "target                        0\n",
              "Length: 359, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>name</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>coordinates</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>category</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>address</th>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>same_category_count_1000m</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>address_count</th>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>name_count</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>category_count</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>359 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ],
      "source": [
        "train_data.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "UUGKlQxCGN-R"
      },
      "outputs": [],
      "source": [
        "url = \"https://api.hh.ru/metro/1\"\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    metro_data = response.json()\n",
        "metros = []\n",
        "for line in range(len(metro_data['lines'])):\n",
        "    here = metro_data['lines'][line]['stations']\n",
        "    for station in range(len(here)):\n",
        "        metros.append([here[station]['name'], here[station]['lng'], here[station]['lat']])\n",
        "\n",
        "metros = pd.DataFrame(metros, columns=['metro_name', 'lng', 'lat'])\n",
        "geo_metros = gpd.GeoDataFrame(\n",
        "    metros,\n",
        "    geometry=gpd.points_from_xy(metros.lng, metros.lat),\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "geo_metros = geo_metros.drop_duplicates(subset=['metro_name']).reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "16JbFAJ1LMsp"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from shapely.geometry import Point\n",
        "from sklearn.neighbors import BallTree\n",
        "\n",
        "def add_nearest_metros(df, geo_metros, n_neighbors=3):\n",
        "    df = df.copy()\n",
        "    df[['lng', 'lat']] = df['coordinates'].apply(lambda x: pd.Series(eval(x)))\n",
        "    geo_df = gpd.GeoDataFrame(\n",
        "        df,\n",
        "        geometry=gpd.points_from_xy(df.lng, df.lat),\n",
        "        crs=\"EPSG:4326\"\n",
        "    )\n",
        "    metros_radians = np.deg2rad(geo_metros[['lat', 'lng']])\n",
        "    places_radians = np.deg2rad(geo_df[['lat', 'lng']])\n",
        "\n",
        "    tree = BallTree(metros_radians, metric='haversine')\n",
        "    dist, ind = tree.query(places_radians, k=n_neighbors)\n",
        "    dist_km = dist * 6371\n",
        "    for i in range(n_neighbors):\n",
        "        geo_df[f'metro_{i+1}_name'] = geo_metros.iloc[ind[:, i]]['metro_name'].values\n",
        "        geo_df[f'metro_{i+1}_dist_km'] = dist_km[:, i]\n",
        "    geo_df = geo_df.drop(columns='geometry')\n",
        "    return geo_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "efFkug4DGnhK"
      },
      "outputs": [],
      "source": [
        "geo_cols = [\"traffic_300m\", \"homes_300m\", \"works_300m\", \"mean_income_300m\"]\n",
        "dem_cols = [\"female_300m\", \"male_300m\", \"age_25-34_300m\", \"employed_300m\", \"higher_education_300m\"]\n",
        "\n",
        "cluster_geo_dem = train_data.groupby(\"cluster\")[geo_cols + dem_cols].mean().add_prefix(\"cluster_mean_\")\n",
        "train_data = train_data.merge(cluster_geo_dem, on=\"cluster\", how=\"left\")\n",
        "test_data = test_data.merge(cluster_geo_dem, on=\"cluster\", how=\"left\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "HlBVdA3rx7s6"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "tfidf_columns = [col for col in result.columns if col != 'id']\n",
        "\n",
        "train_with_tfidf = train_data.merge(result, on='id', how='left')\n",
        "test_with_tfidf = test_data.merge(result, on='id', how='left')\n",
        "\n",
        "train_with_tfidf[tfidf_columns] = train_with_tfidf[tfidf_columns].fillna(0)\n",
        "test_with_tfidf[tfidf_columns] = test_with_tfidf[tfidf_columns].fillna(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "QTIkXs-ANPmx"
      },
      "outputs": [],
      "source": [
        "train_with_tfidf = add_nearest_metros(train_with_tfidf, geo_metros)\n",
        "test_with_tfidf = add_nearest_metros(test_with_tfidf, geo_metros)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "wqwpX3evzQ7z",
        "outputId": "daea1210-daca-4705-89df-48d22c00de3d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       0.0\n",
              "1       0.0\n",
              "2       0.0\n",
              "3       0.0\n",
              "4       0.0\n",
              "       ... \n",
              "9271    0.0\n",
              "9272    1.0\n",
              "9273    0.0\n",
              "9274    0.0\n",
              "9275    0.0\n",
              "Name: total_ellipsis, Length: 9276, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>total_ellipsis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9271</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9272</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9273</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9274</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9275</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9276 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ],
      "source": [
        "test_with_tfidf.iloc[:, 308]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DpclfCKzrRw",
        "outputId": "a62469eb-9dc4-4bb1-bbde-716a46635670"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ],
      "source": [
        "sum(test_with_tfidf.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "uUUXYu5O3baI",
        "outputId": "6a6cc843-7a91-48b9-f695-17175a70e8e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         734.0\n",
              "1        4701.0\n",
              "2        1204.0\n",
              "3         540.0\n",
              "4        2411.0\n",
              "          ...  \n",
              "37114     638.0\n",
              "37115    1478.0\n",
              "37116    1209.0\n",
              "37117     981.0\n",
              "37118     896.0\n",
              "Name: train_ticket_order_300m, Length: 37119, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_ticket_order_300m</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>734.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4701.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1204.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>540.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2411.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37114</th>\n",
              "      <td>638.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37115</th>\n",
              "      <td>1478.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37116</th>\n",
              "      <td>1209.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37117</th>\n",
              "      <td>981.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37118</th>\n",
              "      <td>896.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>37119 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ],
      "source": [
        "train_with_tfidf['train_ticket_order_300m']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UssgYYfH2pK3",
        "outputId": "d660d3ec-f755-4813-ad17-f3c4f25627c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['id', 'name', 'coordinates', 'category', 'address', 'traffic_300m',\n",
              "       'homes_300m', 'works_300m', 'female_300m',\n",
              "       'train_ticket_order_300m', 'mortgage_300m', 'recipes_300m',\n",
              "       'online_shops_300m', 'manga_300m', 'children_goods_300m',\n",
              "       'language_courses_300m', 'commercial_real_estate_purchase_300m',\n",
              "       'grocery_stores_300m', 'preschool_300m',\n",
              "       'cultural_leisure_events_300m', 'banking_services_300m',\n",
              "       'no_children_300m', 'mid_class_cars_300m', 'home_appliances_300m',\n",
              "       'beauty_and_health_devices_300m', 'computer_games_300m',\n",
              "       'cosplay_300m', 'unemployed_300m', 'pizza_delivery_300m',\n",
              "       'science_and_technology_300m', 'married_300m',\n",
              "       'mobile_phones_300m', 'events_300m', 'deposits_300m',\n",
              "       'clinics_300m', 'economics_300m', 'home_goods_300m',\n",
              "       'care_products_300m', 'books_300m', 'employed_300m',\n",
              "       'pharmacy_300m', 'anime_300m', 'toys_and_games_300m',\n",
              "       'restaurants_cafes_bars_300m', 'interest_in_buying_new_car_300m',\n",
              "       'pet_food_and_supplies_300m', 'womens_clothing_300m',\n",
              "       'laptops_tablets_computers_300m', 'age_<17_300m',\n",
              "       'sushi_delivery_300m', 'football_fans_300m',\n",
              "       'sports_clothing_300m', 'politics_300m',\n",
              "       'cosmetics_and_perfumes_300m', 'childrens_medicine_300m',\n",
              "       'movies_and_series_300m', 'above_average_income_300m',\n",
              "       'coffee_shops_300m', 'society_300m', 'car_news_300m',\n",
              "       'beauty_and_personal_care_300m', 'lending_300m',\n",
              "       'building_materials_300m', 'age_45-54_300m',\n",
              "       'jewelry_pawnshops_300m', 'tours_and_travel_agencies_300m',\n",
              "       'education_300m', 'below_average_income_300m', 'manicure_300m',\n",
              "       'secondary_education_300m', 'pets_300m', 'premium_income_300m',\n",
              "       'childrens_books_300m', 'food_delivery_300m', 'mens_clothing_300m',\n",
              "       'cooking_300m', 'car_parts_300m',\n",
              "       'goods_for_creativity_and_hobbies_300m', 'courses_300m',\n",
              "       'mobile_communications_and_internet_300m', 'age_18-24_300m',\n",
              "       'age_35-44_300m', 'new_buildings_purchase_300m',\n",
              "       'delivery_services_300m', 'restaurants_cafes_300m',\n",
              "       'garden_supplies_300m', 'school_supplies_300m',\n",
              "       'game_consoles_300m', 'baby_food_300m', 'higher_education_300m',\n",
              "       'not_married_300m', 'childrens_transport_300m',\n",
              "       'online_video_300m', 'portable_devices_300m',\n",
              "       'childrens_sports_300m', 'microloans_300m', 'motorcycles_300m',\n",
              "       'optics_and_lenses_300m', 'bars_300m', 'has_children_300m',\n",
              "       'italian_cuisine_300m', 'tourism_our_sea_300m',\n",
              "       'car_services_300m', 'insurance_300m', 'interior_300m',\n",
              "       'medications_and_supplements_300m',\n",
              "       'children_goods_for_walks_and_travel_300m',\n",
              "       'premium_class_cars_300m', 'jewelry_watches_accessories_300m',\n",
              "       'asian_cuisine_300m', 'flowers_300m', 'beauty_salons_300m',\n",
              "       'clothing_shoes_accessories_300m',\n",
              "       'construction_and_renovation_300m', 'childrens_websites_300m',\n",
              "       'culture_300m', 'diets_and_healthy_eating_300m',\n",
              "       'high_income_300m', 'securities_market_300m', 'age_>55_300m',\n",
              "       'country_real_estate_purchase_300m', 'economy_class_cars_300m',\n",
              "       'pregnancy_websites_300m', 'groceries_and_drinks_300m',\n",
              "       'kitchen_appliances_300m', 'rental_real_estate_300m',\n",
              "       'photo_and_video_cameras_300m',\n",
              "       'secondary_real_estate_purchase_300m', 'sports_300m',\n",
              "       'leisure_and_entertainment_300m', 'average_income_300m',\n",
              "       'dating_300m', 'laser_hair_removal_300m', 'car_owners_300m',\n",
              "       'k-pop_300m', 'doramas_300m', 'computer_components_300m',\n",
              "       'humor_300m', 'car_market_300m', 'no_higher_education_300m',\n",
              "       'goods_for_moms_and_babies_300m', 'age_25-34_300m', 'male_300m',\n",
              "       'phone_repair_300m', 'mean_income_300m', 'traffic_1000m',\n",
              "       'homes_1000m', 'works_1000m', 'female_1000m',\n",
              "       'train_ticket_order_1000m', 'mortgage_1000m', 'recipes_1000m',\n",
              "       'online_shops_1000m', 'manga_1000m', 'children_goods_1000m',\n",
              "       'language_courses_1000m', 'commercial_real_estate_purchase_1000m',\n",
              "       'grocery_stores_1000m', 'preschool_1000m',\n",
              "       'cultural_leisure_events_1000m', 'banking_services_1000m',\n",
              "       'no_children_1000m', 'mid_class_cars_1000m',\n",
              "       'home_appliances_1000m', 'beauty_and_health_devices_1000m',\n",
              "       'computer_games_1000m', 'cosplay_1000m', 'unemployed_1000m',\n",
              "       'pizza_delivery_1000m', 'science_and_technology_1000m',\n",
              "       'married_1000m', 'mobile_phones_1000m', 'events_1000m',\n",
              "       'deposits_1000m', 'clinics_1000m', 'economics_1000m',\n",
              "       'home_goods_1000m', 'care_products_1000m', 'books_1000m',\n",
              "       'employed_1000m', 'pharmacy_1000m', 'anime_1000m',\n",
              "       'toys_and_games_1000m', 'restaurants_cafes_bars_1000m',\n",
              "       'interest_in_buying_new_car_1000m', 'pet_food_and_supplies_1000m',\n",
              "       'womens_clothing_1000m', 'laptops_tablets_computers_1000m',\n",
              "       'age_<17_1000m', 'sushi_delivery_1000m', 'football_fans_1000m',\n",
              "       'sports_clothing_1000m', 'politics_1000m',\n",
              "       'cosmetics_and_perfumes_1000m', 'childrens_medicine_1000m',\n",
              "       'movies_and_series_1000m', 'above_average_income_1000m',\n",
              "       'coffee_shops_1000m', 'society_1000m', 'car_news_1000m',\n",
              "       'beauty_and_personal_care_1000m', 'lending_1000m',\n",
              "       'building_materials_1000m', 'age_45-54_1000m',\n",
              "       'jewelry_pawnshops_1000m', 'tours_and_travel_agencies_1000m',\n",
              "       'education_1000m', 'below_average_income_1000m', 'manicure_1000m',\n",
              "       'secondary_education_1000m', 'pets_1000m', 'premium_income_1000m',\n",
              "       'childrens_books_1000m', 'food_delivery_1000m',\n",
              "       'mens_clothing_1000m', 'cooking_1000m', 'car_parts_1000m',\n",
              "       'goods_for_creativity_and_hobbies_1000m', 'courses_1000m',\n",
              "       'mobile_communications_and_internet_1000m', 'age_18-24_1000m',\n",
              "       'age_35-44_1000m', 'new_buildings_purchase_1000m',\n",
              "       'delivery_services_1000m', 'restaurants_cafes_1000m',\n",
              "       'garden_supplies_1000m', 'school_supplies_1000m',\n",
              "       'game_consoles_1000m', 'baby_food_1000m', 'higher_education_1000m',\n",
              "       'not_married_1000m', 'childrens_transport_1000m',\n",
              "       'online_video_1000m', 'portable_devices_1000m',\n",
              "       'childrens_sports_1000m', 'microloans_1000m', 'motorcycles_1000m',\n",
              "       'optics_and_lenses_1000m', 'bars_1000m', 'has_children_1000m',\n",
              "       'italian_cuisine_1000m', 'tourism_our_sea_1000m',\n",
              "       'car_services_1000m', 'insurance_1000m', 'interior_1000m',\n",
              "       'medications_and_supplements_1000m',\n",
              "       'children_goods_for_walks_and_travel_1000m',\n",
              "       'premium_class_cars_1000m', 'jewelry_watches_accessories_1000m',\n",
              "       'asian_cuisine_1000m', 'flowers_1000m', 'beauty_salons_1000m',\n",
              "       'clothing_shoes_accessories_1000m',\n",
              "       'construction_and_renovation_1000m', 'childrens_websites_1000m',\n",
              "       'culture_1000m', 'diets_and_healthy_eating_1000m',\n",
              "       'high_income_1000m', 'securities_market_1000m', 'age_>55_1000m',\n",
              "       'country_real_estate_purchase_1000m', 'economy_class_cars_1000m',\n",
              "       'pregnancy_websites_1000m', 'groceries_and_drinks_1000m',\n",
              "       'kitchen_appliances_1000m', 'rental_real_estate_1000m',\n",
              "       'photo_and_video_cameras_1000m',\n",
              "       'secondary_real_estate_purchase_1000m', 'sports_1000m',\n",
              "       'leisure_and_entertainment_1000m', 'average_income_1000m',\n",
              "       'dating_1000m', 'laser_hair_removal_1000m', 'car_owners_1000m',\n",
              "       'k-pop_1000m', 'doramas_1000m', 'computer_components_1000m',\n",
              "       'humor_1000m', 'car_market_1000m', 'no_higher_education_1000m',\n",
              "       'goods_for_moms_and_babies_1000m', 'age_25-34_1000m', 'male_1000m',\n",
              "       'phone_repair_1000m', 'mean_income_1000m', 'longitude', 'latitude',\n",
              "       'distance_to_moscow_center_km', 'review_count', 'avg_text_length',\n",
              "       'std_text_length', 'max_text_length', 'min_text_length',\n",
              "       'avg_word_count', 'std_word_count', 'max_word_count',\n",
              "       'min_word_count', 'avg_char_count', 'std_char_count',\n",
              "       'avg_exclamation', 'total_exclamation', 'max_exclamation',\n",
              "       'avg_question', 'total_question', 'max_question',\n",
              "       'avg_capital_ratio', 'avg_comma_count', 'avg_dot_count',\n",
              "       'total_ellipsis', 'avg_line_breaks', 'quotes_ratio',\n",
              "       'avg_sentiment', 'std_sentiment', 'min_sentiment', 'max_sentiment',\n",
              "       'positive_reviews_count', 'negative_reviews_count',\n",
              "       'avg_service_mention', 'avg_quality_mention', 'avg_price_mention',\n",
              "       'avg_unique_words_ratio', 'avg_word_length', 'emoji_ratio',\n",
              "       'avg_long_words_ratio', 'avg_sentence_count',\n",
              "       'avg_sentence_length', 'sentiment_volatility',\n",
              "       'text_length_variation', 'emotional_intensity',\n",
              "       'review_engagement', 'has_positive_reviews',\n",
              "       'has_negative_reviews', 'has_mixed_reviews',\n",
              "       'positive_negative_ratio', 'sentiment_balance',\n",
              "       'text_complexity_index', 'punctuation_diversity',\n",
              "       'has_detailed_reviews', 'has_emotional_reviews',\n",
              "       'has_structured_reviews', 'wealth_index_1000m', 'category_freq',\n",
              "       'category_target_enc', 'cluster', 'mean_target_in_cluster',\n",
              "       'std_target_in_cluster', 'count_in_cluster',\n",
              "       'median_target_in_cluster', 'density_300m', 'density_1000m',\n",
              "       'latitude_rad', 'longitude_rad', 'nearest_same_category_dist',\n",
              "       'same_category_count_300m', 'same_category_count_1000m',\n",
              "       'address_count', 'name_count', 'category_count', 'target',\n",
              "       'cluster_mean_traffic_300m', 'cluster_mean_homes_300m',\n",
              "       'cluster_mean_works_300m', 'cluster_mean_mean_income_300m',\n",
              "       'cluster_mean_female_300m', 'cluster_mean_male_300m',\n",
              "       'cluster_mean_age_25-34_300m', 'cluster_mean_employed_300m',\n",
              "       'cluster_mean_higher_education_300m', 'text', 'tfidf_0', 'tfidf_1',\n",
              "       'tfidf_2', 'tfidf_3', 'tfidf_4', 'tfidf_5', 'tfidf_6', 'tfidf_7',\n",
              "       'tfidf_8', 'tfidf_9', 'tfidf_10', 'tfidf_11', 'tfidf_12',\n",
              "       'tfidf_13', 'tfidf_14', 'tfidf_15', 'tfidf_16', 'tfidf_17',\n",
              "       'tfidf_18', 'tfidf_19', 'tfidf_20', 'tfidf_21', 'tfidf_22',\n",
              "       'tfidf_23', 'tfidf_24', 'tfidf_25', 'tfidf_26', 'tfidf_27',\n",
              "       'tfidf_28', 'tfidf_29', 'tfidf_30', 'tfidf_31', 'tfidf_32',\n",
              "       'tfidf_33', 'tfidf_34', 'tfidf_35', 'tfidf_36', 'tfidf_37',\n",
              "       'tfidf_38', 'tfidf_39', 'tfidf_40', 'tfidf_41', 'tfidf_42',\n",
              "       'tfidf_43', 'tfidf_44', 'tfidf_45', 'tfidf_46', 'tfidf_47',\n",
              "       'tfidf_48', 'tfidf_49', 'tfidf_50', 'tfidf_51', 'tfidf_52',\n",
              "       'tfidf_53', 'tfidf_54', 'tfidf_55', 'tfidf_56', 'tfidf_57',\n",
              "       'tfidf_58', 'tfidf_59', 'tfidf_60', 'tfidf_61', 'tfidf_62',\n",
              "       'tfidf_63', 'tfidf_64', 'tfidf_65', 'tfidf_66', 'tfidf_67',\n",
              "       'tfidf_68', 'tfidf_69', 'tfidf_70', 'tfidf_71', 'tfidf_72',\n",
              "       'tfidf_73', 'tfidf_74', 'tfidf_75', 'tfidf_76', 'tfidf_77',\n",
              "       'tfidf_78', 'tfidf_79', 'tfidf_80', 'tfidf_81', 'tfidf_82',\n",
              "       'tfidf_83', 'tfidf_84', 'tfidf_85', 'tfidf_86', 'tfidf_87',\n",
              "       'tfidf_88', 'tfidf_89', 'tfidf_90', 'tfidf_91', 'tfidf_92',\n",
              "       'tfidf_93', 'tfidf_94', 'tfidf_95', 'tfidf_96', 'tfidf_97',\n",
              "       'tfidf_98', 'tfidf_99', 'lng', 'lat', 'metro_1_name',\n",
              "       'metro_1_dist_km', 'metro_2_name', 'metro_2_dist_km',\n",
              "       'metro_3_name', 'metro_3_dist_km'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ],
      "source": [
        "np.array(train_with_tfidf.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "bQMtqsjbcioV"
      },
      "outputs": [],
      "source": [
        "# !pip install -U autogluon.tabular catboost lightgbm\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rB6KRI_VbhYF",
        "outputId": "92a36bc2-19bd-4ae9-93f3-0f8fcf6c77f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "address_count    20\n",
            "address          20\n",
            "dtype: int64\n",
            "Series([], dtype: int64)\n"
          ]
        }
      ],
      "source": [
        "train_with_tfidf = train_with_tfidf.loc[:, ~train_with_tfidf.columns.duplicated()]\n",
        "test_with_tfidf = test_with_tfidf.loc[:, ~test_with_tfidf.columns.duplicated()]\n",
        "\n",
        "\n",
        "numeric_features = []\n",
        "for col in train_with_tfidf.columns:\n",
        "    if col not in ['id', 'category', 'coordinates', 'target']:\n",
        "        if train_with_tfidf[col].dtype in ['int64', 'float64']:\n",
        "            numeric_features.append(col)\n",
        "\n",
        "categorical_features = [\n",
        "    'category', 'metro_1_name', 'metro_2_name', 'metro_3_name',\n",
        "    'name', 'address'\n",
        "]\n",
        "\n",
        "X = train_with_tfidf[numeric_features + categorical_features]\n",
        "y = train_with_tfidf['target']\n",
        "X_test = test_with_tfidf[numeric_features + categorical_features]\n",
        "\n",
        "print(X.isna().sum()[X.isna().sum() > 0])\n",
        "\n",
        "X[numeric_features] = X[numeric_features].fillna(X[numeric_features].median())\n",
        "X_test[numeric_features] = X_test[numeric_features].fillna(X[numeric_features].median())\n",
        "\n",
        "\n",
        "X[categorical_features] = X[categorical_features].fillna('unknown')\n",
        "X_test[categorical_features] = X_test[categorical_features].fillna('unknown')\n",
        "\n",
        "print(X.isna().sum()[X.isna().sum() > 0])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Переходим к моделям, обучаем CatBoost, LightGBM и блендим их"
      ],
      "metadata": {
        "id": "zkrlx1FWmBh_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "ZkoZv7ntxNBq"
      },
      "outputs": [],
      "source": [
        "from catboost import CatBoostRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIajxnkh8REL",
        "outputId": "c7d00fb9-3202-4b4e-b302-c36735fb3cba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because MAE is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 0.3283859\ttest: 0.3248655\tbest: 0.3248655 (0)\ttotal: 126ms\tremaining: 31m 25s\n",
            "100:\tlearn: 0.2739266\ttest: 0.2680782\tbest: 0.2680782 (100)\ttotal: 7.46s\tremaining: 18m 21s\n",
            "200:\tlearn: 0.2556974\ttest: 0.2487659\tbest: 0.2487659 (200)\ttotal: 13.9s\tremaining: 17m 4s\n",
            "300:\tlearn: 0.2490093\ttest: 0.2419847\tbest: 0.2419847 (300)\ttotal: 20.7s\tremaining: 16m 53s\n",
            "400:\tlearn: 0.2454127\ttest: 0.2389641\tbest: 0.2389641 (400)\ttotal: 27.5s\tremaining: 16m 40s\n",
            "500:\tlearn: 0.2426889\ttest: 0.2369799\tbest: 0.2369799 (500)\ttotal: 33.7s\tremaining: 16m 14s\n",
            "600:\tlearn: 0.2405167\ttest: 0.2354198\tbest: 0.2354198 (600)\ttotal: 40.5s\tremaining: 16m 10s\n",
            "700:\tlearn: 0.2387403\ttest: 0.2342671\tbest: 0.2342671 (700)\ttotal: 46.4s\tremaining: 15m 47s\n",
            "800:\tlearn: 0.2371042\ttest: 0.2331782\tbest: 0.2331782 (800)\ttotal: 56.5s\tremaining: 16m 41s\n",
            "900:\tlearn: 0.2356572\ttest: 0.2323456\tbest: 0.2323456 (900)\ttotal: 1m 4s\tremaining: 16m 51s\n",
            "1000:\tlearn: 0.2342762\ttest: 0.2316481\tbest: 0.2316481 (1000)\ttotal: 1m 13s\tremaining: 17m 7s\n",
            "1100:\tlearn: 0.2328487\ttest: 0.2309899\tbest: 0.2309899 (1100)\ttotal: 1m 22s\tremaining: 17m 26s\n"
          ]
        }
      ],
      "source": [
        "model = CatBoostRegressor(\n",
        "    iterations=15000,\n",
        "    learning_rate=0.01,\n",
        "    loss_function='MAE',\n",
        "    random_state=42,\n",
        "    verbose=100,\n",
        "    task_type='GPU',\n",
        "    cat_features=categorical_features, # ВСЕ ЕЩЕ НЕ ОПТИМАЛЬНЫЕ ГИПЕРПАРАМЕТРЫ!!, МОЖНО ПОЛУЧШЕ\n",
        ")\n",
        "model.fit(\n",
        "     X_train, y_train,\n",
        "    eval_set=(X_val, y_val),\n",
        "    early_stopping_rounds=5000,\n",
        "    verbose=100, cat_features=categorical_features,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnUib47s62_z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y, cat_features=categorical_features, verbose=1000)"
      ],
      "metadata": {
        "id": "E_a_hoNPZgeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMRegressor"
      ],
      "metadata": {
        "id": "qCojuhZQZ0Gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhgtCslPd1-r"
      },
      "outputs": [],
      "source": [
        "for col in categorical_features:\n",
        "    X[col] = X[col].astype('category')\n",
        "for col in categorical_features:\n",
        "    X_test[col] = X_test[col].astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "model_lgb = LGBMRegressor(\n",
        "    n_estimators=4000,\n",
        "    learning_rate=0.3, #АНАЛОГИЧНО И ЗДЕСЬ\n",
        "    objective='mae',\n",
        "    random_state=42,\n",
        "    verbose=-1,\n",
        ")\n",
        "\n",
        "model_lgb.fit(\n",
        "    X, y,\n",
        "    categorical_feature=categorical_features\n",
        ")"
      ],
      "metadata": {
        "id": "-Zdb6O3RZple"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEhTZL_2RbI1"
      },
      "outputs": [],
      "source": [
        "feature_names = X_train.columns\n",
        "importances = model.get_feature_importance()\n",
        "\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': importances\n",
        "}).sort_values(by='importance', ascending=False)\n",
        "\n",
        "print(feature_importance_df.head(30))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ip_USHGpy0xf"
      },
      "outputs": [],
      "source": [
        "X_test = X_test.fillna('unknown')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcdWLOtXAYm8"
      },
      "outputs": [],
      "source": [
        "pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_q = model_lgb.predict(X_test)"
      ],
      "metadata": {
        "id": "knpEN7i-jql3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kd7zDPuX1qGr"
      },
      "outputs": [],
      "source": [
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overall_pred = pred * 0.9 + pred_q * 0.1"
      ],
      "metadata": {
        "id": "II2mzyNskd-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6yEpnE3XW3r"
      },
      "outputs": [],
      "source": [
        "submission = pd.DataFrame({\n",
        "    'id': test_data['id'],\n",
        "    'target': overall_pred\n",
        "}).to_csv('Our_final_answer.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Jb278QfXgAj"
      },
      "outputs": [],
      "source": [
        "submission"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
