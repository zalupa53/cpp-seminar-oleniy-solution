{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133debb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def objective_catboost(trial, X, y, task_type='classification'):\n",
    "    \"\"\"\n",
    "    Функция для оптимизации гиперпараметров CatBoost\n",
    "    \"\"\"\n",
    "    # Параметры для оптимизации\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000), # МЕНЯЙ НА\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "        'random_strength': trial.suggest_float('random_strength', 0.1, 10),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "        'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations', 1, 10),\n",
    "        'verbose': False,\n",
    "        'random_seed': 42,\n",
    "        'task_type': 'GPU'\n",
    "    }\n",
    "    \n",
    "    # Разделение данных\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Выбор модели в зависимости от задачи\n",
    "    if task_type == 'classification':\n",
    "        model = CatBoostClassifier(**params)\n",
    "        metric = accuracy_score\n",
    "    else:  # regression\n",
    "        model = CatBoostRegressor(**params)\n",
    "        metric = mean_squared_error\n",
    "    \n",
    "    # Обучение модели\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=(X_val, y_val),\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Предсказание и оценка\n",
    "    y_pred = model.predict(X_val)\n",
    "    score = metric(y_val, y_pred)\n",
    "    \n",
    "    # Для регрессии используем отрицательный MSE (Optuna максимизирует)\n",
    "    if task_type == 'regression':\n",
    "        score = -score\n",
    "    \n",
    "    return score\n",
    "\n",
    "# Пример использования\n",
    "def optimize_catboost(X, y, task_type='classification', n_trials=100):\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(\n",
    "        lambda trial: objective_catboost(trial, X, y, task_type), \n",
    "        n_trials=n_trials\n",
    "    )\n",
    "    \n",
    "    print(\"Лучшие параметры CatBoost:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(f\"Лучшее значение метрики: {study.best_value}\")\n",
    "    \n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a967ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def objective_lightgbm(trial, X, y, task_type='classification'):\n",
    "    \"\"\"\n",
    "    Функция для оптимизации гиперпараметров LightGBM\n",
    "    \"\"\"\n",
    "    # Параметры для оптимизации\n",
    "    params = {\n",
    "        'objective': 'binary' if task_type == 'classification' else 'regression',\n",
    "        'metric': 'binary_logloss' if task_type == 'classification' else 'rmse',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # Разделение данных\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Создание датасетов LightGBM\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "    \n",
    "    # Обучение модели\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(0)]\n",
    "    )\n",
    "    \n",
    "    # Предсказание и оценка\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    if task_type == 'classification':\n",
    "        y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "        score = accuracy_score(y_val, y_pred_binary)\n",
    "    else:  # regression\n",
    "        score = -mean_squared_error(y_val, y_pred)  # отрицательный MSE\n",
    "    \n",
    "    return score\n",
    "\n",
    "def optimize_lightgbm(X, y, task_type='classification', n_trials=100):\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(\n",
    "        lambda trial: objective_lightgbm(trial, X, y, task_type), \n",
    "        n_trials=n_trials\n",
    "    )\n",
    "    \n",
    "    print(\"Лучшие параметры LightGBM:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(f\"Лучшее значение метрики: {study.best_value}\")\n",
    "    \n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5165f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def objective_xgboost(trial, X, y, task_type='classification'):\n",
    "    \"\"\"\n",
    "    Функция для оптимизации гиперпараметров XGBoost\n",
    "    \"\"\"\n",
    "    # Параметры для оптимизации\n",
    "    params = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'binary:logistic' if task_type == 'classification' else 'reg:squarederror',\n",
    "        'booster': trial.suggest_categorical('booster', ['gbtree', 'gblinear', 'dart']),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 1.0),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 1.0),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.3),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0),\n",
    "        'grow_policy': trial.suggest_categorical('grow_policy', ['depthwise', 'lossguide']),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    if params['booster'] == 'gbtree' or params['booster'] == 'dart':\n",
    "        params['max_depth'] = trial.suggest_int('max_depth', 3, 10)\n",
    "        params['min_child_weight'] = trial.suggest_int('min_child_weight', 1, 10)\n",
    "    \n",
    "    # Разделение данных\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Обучение модели\n",
    "    model = xgb.XGBClassifier(**params) if task_type == 'classification' else xgb.XGBRegressor(**params)\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Предсказание и оценка\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    if task_type == 'classification':\n",
    "        score = accuracy_score(y_val, y_pred)\n",
    "    else:  # regression\n",
    "        score = -mean_squared_error(y_val, y_pred)  # отрицательный MSE\n",
    "    \n",
    "    return score\n",
    "\n",
    "def optimize_xgboost(X, y, task_type='classification', n_trials=100):\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(\n",
    "        lambda trial: objective_xgboost(trial, X, y, task_type), \n",
    "        n_trials=n_trials\n",
    "    )\n",
    "    \n",
    "    print(\"Лучшие параметры XGBoost:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(f\"Лучшее значение метрики: {study.best_value}\")\n",
    "    \n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01384a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49d7a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
