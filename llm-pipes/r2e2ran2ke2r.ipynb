{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6adaf5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Tuple\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-Reranker-0.6B\"\n",
    "\n",
    "# ===== Инициализация модели один раз =====\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    padding_side=\"left\",\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device).eval()\n",
    "\n",
    "# Токены для \"классов\" yes/no\n",
    "TOKEN_TRUE_ID = tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "TOKEN_FALSE_ID = tokenizer.convert_tokens_to_ids(\"no\")\n",
    "\n",
    "# Префикс/суффикс — как в оф. примере\n",
    "PREFIX = (\n",
    "    \"<|im_start|>system\\n\"\n",
    "    \"Judge whether the Document meets the requirements based on the Query and the Instruct provided. \"\n",
    "    \"Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n\"\n",
    "    \"<|im_start|>user\\n\"\n",
    ")\n",
    "SUFFIX = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "\n",
    "MAX_LEN = 8192\n",
    "PREFIX_TOKENS = tokenizer.encode(PREFIX, add_special_tokens=False)\n",
    "SUFFIX_TOKENS = tokenizer.encode(SUFFIX, add_special_tokens=False)\n",
    "\n",
    "\n",
    "def _build_inputs(\n",
    "    query: str,\n",
    "    docs: List[str],\n",
    "    instruction: str,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Готовим батч токенов под Qwen3-Reranker.\n",
    "    \"\"\"\n",
    "    # Строим текст для каждого (query, doc)\n",
    "    pairs = [\n",
    "        f\"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\"\n",
    "        for doc in docs\n",
    "    ]\n",
    "\n",
    "    # Сначала токенизируем без префикса/суффикса\n",
    "    tokenized = tokenizer(\n",
    "        pairs,\n",
    "        padding=False,\n",
    "        truncation=\"longest_first\",\n",
    "        return_attention_mask=False,\n",
    "        max_length=MAX_LEN - len(PREFIX_TOKENS) - len(SUFFIX_TOKENS),\n",
    "    )\n",
    "\n",
    "    # Добавляем префикс/суффикс токенами\n",
    "    for i, ids in enumerate(tokenized[\"input_ids\"]):\n",
    "        tokenized[\"input_ids\"][i] = PREFIX_TOKENS + ids + SUFFIX_TOKENS\n",
    "\n",
    "    # Паддинг до общего размера\n",
    "    tokenized = tokenizer.pad(\n",
    "        tokenized,\n",
    "        padding=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # На нужное устройство\n",
    "    for key in tokenized:\n",
    "        tokenized[key] = tokenized[key].to(device)\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def qwen3_rerank(\n",
    "    query: str,\n",
    "    docs: List[str],\n",
    "    instruction: str = (\n",
    "        \"Given a web search query, retrieve relevant passages that answer the query\"\n",
    "    ),\n",
    "    top_k: int | None = None,\n",
    ") -> List[Tuple[int, str, float]]:\n",
    "    \"\"\"\n",
    "    Реранкинг документов под один запрос.\n",
    "\n",
    "    Возвращает список (original_index, doc, score),\n",
    "    отсортированный по score по убыванию.\n",
    "    \"\"\"\n",
    "    if not docs:\n",
    "        return []\n",
    "\n",
    "    inputs = _build_inputs(query, docs, instruction)\n",
    "\n",
    "    # Берём logits последнего токена\n",
    "    logits = model(**inputs).logits[:, -1, :]\n",
    "\n",
    "    # Logits для \"yes\" и \"no\"\n",
    "    true_logits = logits[:, TOKEN_TRUE_ID]\n",
    "    false_logits = logits[:, TOKEN_FALSE_ID]\n",
    "\n",
    "    # Превращаем в вероятность \"yes\" через softmax\n",
    "    stacked = torch.stack([false_logits, true_logits], dim=1)\n",
    "    probs = torch.softmax(stacked, dim=1)[:, 1]  # P(\"yes\")\n",
    "    scores = probs.tolist()\n",
    "\n",
    "    # Сортируем документы по убыванию score\n",
    "    ranked = sorted(\n",
    "        enumerate(zip(docs, scores)),\n",
    "        key=lambda x: x[1][1],\n",
    "        reverse=True,\n",
    "    )\n",
    "\n",
    "    if top_k is not None:\n",
    "        ranked = ranked[:top_k]\n",
    "\n",
    "    # Приводим к формату (orig_idx, doc, score)\n",
    "    return [(idx, doc, score) for idx, (doc, score) in ranked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f344b5e1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "query = \"Как работает градиентный бустинг?\"\n",
    "docs = [\n",
    "    \"Градиентный бустинг — ансамблевый метод, который добавляет деревья итеративно.\",\n",
    "    \"Линейная регрессия — простая модель с одной матрицей весов.\",\n",
    "]\n",
    "\n",
    "results = qwen3_rerank(query, docs, top_k=None)\n",
    "for orig_idx, doc, score in results:\n",
    "    print(f\"{score:.4f} | #{orig_idx}: {doc}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
