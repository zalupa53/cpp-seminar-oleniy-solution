{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d919fef",
   "metadata": {
    "papermill": {
     "duration": 8.826578,
     "end_time": "2025-08-15T15:28:57.474471",
     "exception": true,
     "start_time": "2025-08-15T15:28:48.647893",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 13:41:07.174860: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-21 13:41:07.217381: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-09-21 13:41:07.217419: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-09-21 13:41:07.218816: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-09-21 13:41:07.225603: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-21 13:41:07.981018: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-21 13:41:08 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "import vllm\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40de4691",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30333d17",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-21 13:41:49 [utils.py:328] non-default args: {'trust_remote_code': True, 'seed': 56, 'max_model_len': 32768, 'max_num_seqs': 64, 'disable_log_stats': True, 'model': 'nvidia/OpenReasoning-Nemotron-7B'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7830fb2ce17a4ad4a2317b0d6e2e1e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/763 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-21 13:41:57 [__init__.py:742] Resolved architecture: Qwen2ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-21 13:41:57 [__init__.py:1815] Using max model len 32768\n",
      "INFO 09-21 13:41:58 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada954fb39994e6aaa340870ae9c8fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f95a0d4c16c487595c82f632a74c1f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc89dd9cbfb4a0a994ae3e40f41de08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bbacda1a0ab40e29e45dd38a2e99bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a090be13be4f441abdb39c64cc9ea7b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb59d86f37ae4ef6baa1c63ac5a51bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8143034682431294951b72ca2c4769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/121 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1647)\u001b[0;0m INFO 09-21 13:41:59 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1647)\u001b[0;0m INFO 09-21 13:41:59 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='nvidia/OpenReasoning-Nemotron-7B', speculative_config=None, tokenizer='nvidia/OpenReasoning-Nemotron-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=56, served_model_name=nvidia/OpenReasoning-Nemotron-7B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":128,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1647)\u001b[0;0m INFO 09-21 13:42:02 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1647)\u001b[0;0m WARNING 09-21 13:42:02 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1647)\u001b[0;0m INFO 09-21 13:42:02 [gpu_model_runner.py:2338] Starting to load model nvidia/OpenReasoning-Nemotron-7B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W921 13:42:02.156728574 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1647)\u001b[0;0m INFO 09-21 13:42:02 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1647)\u001b[0;0m INFO 09-21 13:42:02 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1647)\u001b[0;0m INFO 09-21 13:42:02 [weight_utils.py:348] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ff33864ccf4f1881c1e30ed090e4d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/5.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08f7219d58d4f2a814a64b0624ccd56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1647)\u001b[0;0m INFO 09-21 13:42:42 [weight_utils.py:369] Time spent downloading weights for nvidia/OpenReasoning-Nemotron-7B: 39.504054 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed51f3d0f1e4bd2ae79d2c1d19a9d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9e752ee06443ac9b677fda6ab067da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1647)\u001b[0;0m INFO 09-21 13:42:45 [default_loader.py:268] Loading weights took 2.66 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1647)\u001b[0;0m INFO 09-21 13:42:45 [gpu_model_runner.py:2392] Model loading took 14.2717 GiB and 42.552028 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1647)\u001b[0;0m INFO 09-21 13:42:49 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/377aff7576/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1647)\u001b[0;0m INFO 09-21 13:42:49 [backends.py:550] Dynamo bytecode transform time: 3.81 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1647)\u001b[0;0m INFO 09-21 13:42:52 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1647)\u001b[0;0m INFO 09-21 13:43:08 [backends.py:215] Compiling a graph for dynamic shape takes 17.91 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1647)\u001b[0;0m INFO 09-21 13:43:11 [monitor.py:34] torch.compile takes 21.72 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1647)\u001b[0;0m INFO 09-21 13:43:13 [gpu_worker.py:298] Available KV cache memory: 55.96 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1647)\u001b[0;0m INFO 09-21 13:43:13 [kv_cache_utils.py:864] GPU KV cache size: 1,047,904 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1647)\u001b[0;0m INFO 09-21 13:43:13 [kv_cache_utils.py:868] Maximum concurrency for 32,768 tokens per request: 31.98x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 32.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1647)\u001b[0;0m INFO 09-21 13:43:15 [gpu_model_runner.py:3118] Graph capturing finished in 2 secs, took 0.19 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1647)\u001b[0;0m INFO 09-21 13:43:15 [gpu_worker.py:391] Free memory on device (78.83/79.32 GiB) on startup. Desired GPU memory utilization is (0.9, 71.39 GiB). Actual usage is 14.27 GiB for weight, 1.13 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.19 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=59728225689` to fit into requested memory, or `--kv-cache-memory=67724004352` to fully utilize gpu memory. Current kv cache memory in use is 60091032985 bytes.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1647)\u001b[0;0m INFO 09-21 13:43:15 [core.py:218] init engine (profile, create kv cache, warmup model) took 29.31 seconds\n",
      "INFO 09-21 13:43:16 [llm.py:295] Supported_tasks: ['generate']\n",
      "INFO 09-21 13:43:16 [__init__.py:36] No IOProcessor plugins requested by the model\n"
     ]
    }
   ],
   "source": [
    "llm_model_pth = 'nvidia/OpenReasoning-Nemotron-7B'\n",
    "\n",
    "llm = LLM(\n",
    "    llm_model_pth,\n",
    "    max_num_seqs=64,             \n",
    "    max_model_len=32768,         \n",
    "    trust_remote_code=True,       \n",
    "    tensor_parallel_size=1,   \n",
    "    seed=56,\n",
    ")\n",
    "\n",
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "937c2866",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=0.6,              \n",
    "    n=16,            \n",
    "    top_p=0.95,                    \n",
    "    min_p=0.05,                    \n",
    "    skip_special_tokens=True,   \n",
    "    max_tokens=32768,             \n",
    "    seed=56,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bd133aa-cc34-4d77-bd53-5ed610d1b1c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_messages(task_text):\n",
    "    sys_prompt = \"\"\"\n",
    "    You should think step-by-step. Return final answer within \\\\boxed{}. The final answer should not be in the format LaTeX.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": sys_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Task: {task_text}\",\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    list_of_texts = tokenizer.apply_chat_template(\n",
    "        conversation=messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return list_of_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7386df3b-9655-4d77-85ae-c383f4589702",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final_submission.csv\")\n",
    "messages = df['task'].apply(create_messages).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75313f82-4723-4eeb-a329-d96dd1392ab0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba4f6a3821f4c2e88a5da3c47815945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e5151cd43b4aa0905c1fb0142e5068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3c5312787647668dbb96b24958bcf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec26eafc7634634b1084b871ba08299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bs = 16\n",
    "for i in range(0,len(df),bs):\n",
    "    messages_batch = messages[i:i+bs]\n",
    "    results = llm.generate(prompts=messages_batch, sampling_params=sampling_params)\n",
    "    answers = [[o.text for o in r.outputs] for r in results]\n",
    "    sub = pd.DataFrame()\n",
    "    sub['answers'] = answers\n",
    "    sub['tasks'] = df['task'][i:i+bs].tolist()\n",
    "    sub.to_parquet(f'sub_nemo_{i}.pqt',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf84ce4-cc4a-4230-9610-65df2f45321b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = llm.generate(prompts=messages, sampling_params=sampling_params)\n",
    "answers = [[o.text for o in r.outputs] for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a1c057-6cf1-46af-9b48-18f4f98fe567",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answers = [[o.text for o in r.outputs] for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b968f32-3e68-49bb-8193-e38a48a2552f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['answer'] = answers\n",
    "df.to_parquet('predicts_qwen.pqt',index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11802066,
     "isSourceIdPinned": false,
     "sourceId": 91496,
     "sourceType": "competition"
    },
    {
     "datasetId": 8056443,
     "sourceId": 12744645,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8105821,
     "sourceId": 12818420,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8132558,
     "sourceId": 12857641,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 215.813362,
   "end_time": "2025-08-15T15:28:58.694867",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-15T15:25:22.881505",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
