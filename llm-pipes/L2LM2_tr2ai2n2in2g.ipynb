{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc082636-39f8-4509-bce1-24c2df916c70",
   "metadata": {},
   "source": [
    "# Для начала обучения должны быть установлены зависимости из файла requirements.txt\n",
    "### Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33b67eea-a175-4a51-890a-0ad5a531b75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: True bf16: True use_qlora: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    "    set_seed,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "base_model_name = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "use_qlora = True\n",
    "bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "fp16 = not bf16\n",
    "\n",
    "output_dir = \"outputs/mistake_tagger_lora\"\n",
    "max_seq_len = 2048\n",
    "per_device_batch_size = 1\n",
    "grad_accum = 16\n",
    "num_epochs = 10\n",
    "learning_rate = 3e-5\n",
    "warmup_ratio = 0.05\n",
    "logging_steps = 1\n",
    "save_steps = 75\n",
    "eval_steps = 15\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b91259-f406-4ad5-81e1-9c3e17aeba55",
   "metadata": {},
   "source": [
    "### Подгружаем данные для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa04c48-a97c-428c-8ef3-d05a50614d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('private_test.csv')\n",
    "df_old_train = pd.read_csv('train.csv')\n",
    "df = pd.concat([df, df_old_train], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "798bbc90-b470-4696-91df-fc4a19a5c353",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3013cbaf-9c36-45f3-91a1-f7c35b3f8985",
   "metadata": {},
   "source": [
    "### Расставляем тэги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bebabd7-75e6-4c2c-8887-4910ceb61d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_razmetka(x, sol):\n",
    "    text = ''\n",
    "    prev = 0\n",
    "    for item in eval(x):\n",
    "        text += sol[prev:item[0]] + '<mistake>' + sol[item[0]:item[1]] + '</mistake>'\n",
    "        prev = item[1]\n",
    "    text += sol[prev:]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e800119c-a69e-4c5e-ae1b-9ba3ae75bafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tagged_solution'] = df.apply(lambda x: make_razmetka(x['answer'], x['solution']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfd6d8d2-042f-4793-81a1-09f6dd7b727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_INSTRUCTIONS = (\n",
    "    \"You tag mistakes in student math solutions.\\n\"\n",
    "    \"- Output must be EXACTLY the student's solution text, with <mistake>...</mistake> tags around mistakes.\\n\"\n",
    "    \"- Do NOT add or remove any other text, lines, or spaces.\\n\"\n",
    "    \"- Do NOT add commentary or explanations.\\n\"\n",
    ")\n",
    "\n",
    "def format_prompt(task: str, solution: str) -> str:\n",
    "    return (\n",
    "        f\"{SYSTEM_INSTRUCTIONS}\\n\\n\"\n",
    "        f\"Problem:\\n{task}\\n\\n\"\n",
    "        f\"Student solution:\\n{solution}\\n\\n\"\n",
    "        f\"Tagged solution:\\n\"\n",
    "    )\n",
    "\n",
    "def get_lora_target_modules(model):\n",
    "    present = {\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"}\n",
    "    return sorted(list(present))\n",
    "\n",
    "class DataCollatorPadToMax:\n",
    "    def __init__(self, tokenizer, pad_to_multiple_of=8):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_to_multiple_of = pad_to_multiple_of\n",
    "\n",
    "    def __call__(self, features):\n",
    "        max_len = max(len(f[\"input_ids\"]) for f in features)\n",
    "        if self.pad_to_multiple_of is not None and max_len % self.pad_to_multiple_of != 0:\n",
    "            max_len = ((max_len // self.pad_to_multiple_of) + 1) * self.pad_to_multiple_of\n",
    "\n",
    "        pad_id = self.tokenizer.pad_token_id\n",
    "        batch_input_ids, batch_attention_mask, batch_labels = [], [], []\n",
    "\n",
    "        for f in features:\n",
    "            ids = f[\"input_ids\"]\n",
    "            labs = f[\"labels\"]\n",
    "            attn = [1] * len(ids)\n",
    "\n",
    "            pad_len = max_len - len(ids)\n",
    "            if pad_len > 0:\n",
    "                ids = ids + [pad_id] * pad_len\n",
    "                attn = attn + [0] * pad_len\n",
    "                labs = labs + [-100] * pad_len\n",
    "\n",
    "            batch_input_ids.append(ids)\n",
    "            batch_attention_mask.append(attn)\n",
    "            batch_labels.append(labs)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(batch_input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(batch_attention_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(batch_labels, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "def make_tokenize_fn(tokenizer, max_seq_len):\n",
    "    def tokenize_batch(batch):\n",
    "        input_ids_list, labels_list = [], []\n",
    "        for task, solution, tagged in zip(batch[\"task\"], batch[\"solution\"], batch[\"tagged_solution\"]):\n",
    "            prompt = format_prompt(task, solution)\n",
    "            prompt_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "            target_ids = tokenizer.encode(tagged, add_special_tokens=False)\n",
    "\n",
    "            eos_id = tokenizer.eos_token_id\n",
    "            if eos_id is not None:\n",
    "                target_ids = target_ids + [eos_id]\n",
    "\n",
    "            input_ids = prompt_ids + target_ids\n",
    "            labels = [-100] * len(prompt_ids) + target_ids\n",
    "\n",
    "            if len(input_ids) > max_seq_len:\n",
    "                input_ids = input_ids[:max_seq_len]\n",
    "                labels = labels[:max_seq_len]\n",
    "\n",
    "            input_ids_list.append(input_ids)\n",
    "            labels_list.append(labels)\n",
    "\n",
    "        return {\"input_ids\": input_ids_list, \"labels\": labels_list}\n",
    "    return tokenize_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e394fd47-707f-41d2-9e20-9744a3b4bc22",
   "metadata": {},
   "source": [
    "### Обучение 1 фолда"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2cc45c5-9089-4d0a-a6a0-dbc50fe2c235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_solo_model(train_df,val_df,output_dir):\n",
    "    set_seed(56)\n",
    "    #data\n",
    "    train_ds = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "    val_ds = Dataset.from_pandas(val_df, preserve_index=False)\n",
    "    raw_ds = DatasetDict({\"train\": train_ds, \"val\": val_ds})\n",
    "\n",
    "    # setitng up\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    quant_cfg = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16 if bf16 else torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    ) if use_qlora else None\n",
    "    \n",
    "    torch_dtype = None if use_qlora else (torch.bfloat16 if bf16 else torch.float16)\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch_dtype,\n",
    "        quantization_config=quant_cfg,\n",
    "    )\n",
    "    \n",
    "    if use_qlora:\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    target_modules = get_lora_target_modules(model)\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=target_modules,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_cfg)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    model.config.use_cache = False\n",
    "\n",
    "    tokenize_fn = make_tokenize_fn(tokenizer, max_seq_len)\n",
    "\n",
    "    tokenized_ds = raw_ds.map(\n",
    "        tokenize_fn,\n",
    "        batched=True,\n",
    "        remove_columns=raw_ds[\"train\"].column_names,\n",
    "        desc=\"Tokenizing dataset\",\n",
    "    )\n",
    "\n",
    "    # parameters\n",
    "    data_collator = DataCollatorPadToMax(tokenizer)\n",
    "\n",
    "    optim_name = \"paged_adamw_32bit\" if use_qlora else \"adamw_torch\"\n",
    "\n",
    "    train_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=per_device_batch_size,\n",
    "        per_device_eval_batch_size=per_device_batch_size,\n",
    "        gradient_accumulation_steps=grad_accum,\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_epochs,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        logging_steps=logging_steps,\n",
    "        save_steps=save_steps,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=eval_steps,\n",
    "        save_total_limit=2,\n",
    "        bf16=True,\n",
    "        report_to=\"none\",\n",
    "        gradient_checkpointing=True,\n",
    "        optim=optim_name,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "    )\n",
    "\n",
    "    #trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=train_args,\n",
    "        train_dataset=tokenized_ds[\"train\"],\n",
    "        eval_dataset=tokenized_ds[\"val\"],\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    #train\n",
    "    out = trainer.train()\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Saved LoRA adapter + tokenizer to {output_dir}\")\n",
    "\n",
    "    model.cpu()\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a3732f-812b-4249-bd8b-aa64f224e59a",
   "metadata": {},
   "source": [
    "### Запускаем обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d70937e-5d70-4cba-9762-09ad42aa60f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "kf = GroupKFold(n_splits=5, shuffle=True, random_state=56)\n",
    "df = df[[\"task\", \"solution\", \"tagged_solution\"]].dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46be4c1-8ae2-4ef6-b1f5-e6a7008ee00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f94d7b80ad741fcab5402ba3aa1466e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 33,030,144 || all params: 4,055,498,240 || trainable%: 0.8145\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb58e52638594939bc476b85e719f285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/336 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a4f42591f33432d943ed2d10e18f4d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/74 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 34:45, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.082800</td>\n",
       "      <td>0.080013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.053400</td>\n",
       "      <td>0.053313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.041400</td>\n",
       "      <td>0.047168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.043700</td>\n",
       "      <td>0.043690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>0.041395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>0.040188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.038100</td>\n",
       "      <td>0.039124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.038200</td>\n",
       "      <td>0.038552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.028900</td>\n",
       "      <td>0.038247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.029000</td>\n",
       "      <td>0.038158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.030300</td>\n",
       "      <td>0.038046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>0.037939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.037871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.037924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LoRA adapter + tokenizer to outputs/qwen-sft-fold-0-1-1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d9bed04ada4211a11e137760eea4a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 33,030,144 || all params: 4,055,498,240 || trainable%: 0.8145\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9752fc0f13234d2b8274d41643ea7915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/325 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb2979d89674f22948f05042500ded2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/85 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='121' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [121/210 19:23 < 14:29, 0.10 it/s, Epoch 5.74/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.091000</td>\n",
       "      <td>0.077753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.047800</td>\n",
       "      <td>0.050302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.043300</td>\n",
       "      <td>0.044533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.041800</td>\n",
       "      <td>0.041723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.039696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.038100</td>\n",
       "      <td>0.038754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.044600</td>\n",
       "      <td>0.037528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='85' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/85 00:00 < 00:14, 5.61 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "test_results_on_folds = []\n",
    "for fold, (train_idxs, val_idxs) in enumerate(kf.split(df, groups=df['task'])):\n",
    "    fold_train = df.iloc[train_idxs, :]\n",
    "    fold_val = df.iloc[val_idxs, :]\n",
    "    fold_name = f'outputs/qwen-sft-fold-{fold}'\n",
    "    tok = train_solo_model(\n",
    "        fold_train,\n",
    "        fold_val,\n",
    "        fold_name\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (System)",
   "language": "python",
   "name": "system-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
