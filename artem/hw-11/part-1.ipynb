{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import nlpaug.augmenter.word as naw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def seed_everything(seed):\n",
    "    global SEED\n",
    "    SEED = seed\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, train_dataset, val_dataset=None, \n",
    "#           epochs=16, batch_size=(64, None), criterion=nn.MSELoss(), \n",
    "#           lr=(1e-3, 1e-6), warmup_epochs=0.0, weight_decay=0.01, grad_accum_steps=1, clip_grad_norm=None, \n",
    "#           metrics=None, device=None):\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "\n",
    "#     if not device:\n",
    "#         device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model = model.to(device)\n",
    "\n",
    "#     train_dataloader = DataLoader(train_dataset, batch_size=batch_size[0], shuffle=True)\n",
    "#     if val_dataset:\n",
    "#         val_dataloader = DataLoader(val_dataset, batch_size=batch_size[1], shuffle=False)\n",
    "#     if warmup_epochs:\n",
    "#         if len(lr) != 3: raise ValueError('If warmup is set, lr should contain 3 values.')\n",
    "#         optimizer = optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=lr[1], weight_decay=weight_decay)\n",
    "#         warmup_iters = int(len(train_dataloader) * warmup_epochs)\n",
    "#         warmup = optim.lr_scheduler.LinearLR(optimizer, start_factor=lr[0] / lr[1], total_iters=warmup_iters)\n",
    "#         cosine = optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_dataloader) * epochs - warmup_iters, lr[2])\n",
    "#         scheduler = optim.lr_scheduler.SequentialLR(optimizer, schedulers=[warmup, cosine], milestones=[warmup_iters])\n",
    "#         print(f'Warmup set to {warmup_iters} batches ({(warmup_iters / grad_accum_steps):.2f} steps).')\n",
    "#     else:\n",
    "#         optimizer = optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=lr[0], weight_decay=weight_decay)\n",
    "#         scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_dataloader) * epochs, lr[1])\n",
    "#     if val_dataset:\n",
    "#         best_loss = np.inf\n",
    "#         best_model = deepcopy(model.cpu()).to(device)\n",
    "#         model = model.to(device)\n",
    "#     for epoch in range(epochs):\n",
    "#         print(f'Epoch {epoch + 1}/{epochs}')\n",
    "#         train_losses = []\n",
    "#         pb = tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n",
    "#         if metrics:\n",
    "#             trues = []\n",
    "#             preds = []\n",
    "#         model.train()\n",
    "#         optimizer.zero_grad()\n",
    "#         for step_idx, (x, y) in pb:\n",
    "#             for i in range(len(x)):\n",
    "#                 x[i] = x[i].to(device)\n",
    "#             y = y.to(device)\n",
    "#             pred = model._forward(*x)\n",
    "#             train_loss = criterion(pred, y)\n",
    "#             (train_loss / grad_accum_steps).backward()\n",
    "#             grads = [p.grad.data.abs().mean().item() for p in model.parameters() if p.grad is not None]\n",
    "#             mean_grad_norm = np.mean(grads) / ((step_idx % grad_accum_steps) + 1) if len(grads) > 0 else np.nan\n",
    "#             current_lr = f'{optimizer.param_groups[0][\"lr\"]:.3e}'.replace(\"e+0\", \"e+\").replace(\"e-0\", \"e-\")\n",
    "#             if ((step_idx + 1) % grad_accum_steps == 0) or ((step_idx + 1) == len(train_dataloader)):\n",
    "#                 if clip_grad_norm:\n",
    "#                     torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad_norm)\n",
    "#                 optimizer.step()\n",
    "#                 optimizer.zero_grad()\n",
    "#             scheduler.step()\n",
    "#             train_losses.append(train_loss.item())\n",
    "#             if metrics:\n",
    "#                 trues += y.tolist()\n",
    "#                 with torch.no_grad():\n",
    "#                     preds += model(*x).tolist()\n",
    "#             pb.set_description(f'Train      | loss={np.mean(train_losses):.4f} | current_lr={current_lr} | step_mean_grad_norm={mean_grad_norm:.6f}')\n",
    "#         if metrics:\n",
    "#             metrics_info = ''\n",
    "#             for metric_name, metric in metrics.items():\n",
    "#                 metrics_info += f'{metric_name}={metric(trues, preds):.4f}; '\n",
    "#             print('Metrics:', metrics_info[:-2])\n",
    "#         if val_dataset:\n",
    "#             model.eval()\n",
    "#             with torch.no_grad():\n",
    "#                 val_losses = []\n",
    "#                 if metrics:\n",
    "#                     trues = []\n",
    "#                     preds = []\n",
    "#                 pb = tqdm(val_dataloader)\n",
    "#                 for (x, y) in pb:\n",
    "#                     for i in range(len(x)):\n",
    "#                         x[i] = x[i].to(device)\n",
    "#                     y = y.to(device)\n",
    "#                     pred = model._forward(*x)\n",
    "#                     val_loss = criterion(pred, y)\n",
    "#                     val_losses.append(val_loss.item())\n",
    "#                     if metrics:\n",
    "#                         trues += y.tolist()\n",
    "#                         pred = model(*x)\n",
    "#                         preds += pred.tolist()\n",
    "#                     pb.set_description(f'Validation | loss={np.mean(val_losses):.4f}')\n",
    "#                 if metrics:\n",
    "#                     metrics_info = ''\n",
    "#                     for metric_name, metric in metrics.items():\n",
    "#                         metrics_info += f'{metric_name}={metric(trues, preds):.4f}; '\n",
    "#                     print('Metrics:', metrics_info[:-2])\n",
    "#                 if np.mean(val_losses) < best_loss:\n",
    "#                     best_loss = np.mean(val_losses)\n",
    "#                     best_model = deepcopy(model.cpu()).to(device)\n",
    "#                     model = model.to(device)\n",
    "#         print()\n",
    "    \n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "\n",
    "#     if val_dataset:\n",
    "#         return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, val_dataset=None, \n",
    "          epochs=16, batch_size=(64, None), train_sampler=None, criterion=nn.MSELoss(), \n",
    "          lr=(1e-3, 1e-6), warmup_epochs=0.0, weight_decay=0.01, grad_accum_steps=1, clip_grad_norm=None, \n",
    "          metrics=None, device=None):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    if not device:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    if train_sampler is not None:\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size[0], sampler=train_sampler)\n",
    "    else:\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size[0], shuffle=True)\n",
    "    if val_dataset:\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=batch_size[1], shuffle=False)\n",
    "    if warmup_epochs:\n",
    "        if len(lr) != 3: raise ValueError('If warmup is set, lr should contain 3 values.')\n",
    "        optimizer = optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=lr[1], weight_decay=weight_decay)\n",
    "        warmup_iters = int(len(train_dataloader) * warmup_epochs)\n",
    "        warmup = optim.lr_scheduler.LinearLR(optimizer, start_factor=lr[0] / lr[1], total_iters=warmup_iters)\n",
    "        cosine = optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_dataloader) * epochs - warmup_iters, lr[2])\n",
    "        scheduler = optim.lr_scheduler.SequentialLR(optimizer, schedulers=[warmup, cosine], milestones=[warmup_iters])\n",
    "        print(f'Warmup set to {warmup_iters} batches ({(warmup_iters / grad_accum_steps):.2f} steps).')\n",
    "    else:\n",
    "        optimizer = optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=lr[0], weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_dataloader) * epochs, lr[1])\n",
    "    if val_dataset:\n",
    "        best_loss = np.inf\n",
    "        best_model = deepcopy(model.cpu()).to(device)\n",
    "        model = model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch + 1}/{epochs}')\n",
    "        train_losses = []\n",
    "        pb = tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n",
    "        if metrics:\n",
    "            trues = []\n",
    "            preds = []\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        for step_idx, (x, y) in pb:\n",
    "            for i in range(len(x)):\n",
    "                x[i] = x[i].to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model._forward(*x)\n",
    "            train_loss = criterion(pred, y)\n",
    "            (train_loss / grad_accum_steps).backward()\n",
    "            grads = [p.grad.data.abs().mean().item() for p in model.parameters() if p.grad is not None]\n",
    "            mean_grad_norm = np.mean(grads) / ((step_idx % grad_accum_steps) + 1) if len(grads) > 0 else np.nan\n",
    "            current_lr = f'{optimizer.param_groups[0][\"lr\"]:.3e}'.replace(\"e+0\", \"e+\").replace(\"e-0\", \"e-\")\n",
    "            if ((step_idx + 1) % grad_accum_steps == 0) or ((step_idx + 1) == len(train_dataloader)):\n",
    "                if clip_grad_norm:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad_norm)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            train_losses.append(train_loss.item())\n",
    "            if metrics:\n",
    "                trues += y.tolist()\n",
    "                with torch.no_grad():\n",
    "                    preds += model(*x).tolist()\n",
    "            pb.set_description(f'Train      | loss={np.mean(train_losses):.4f} | current_lr={current_lr} | step_mean_grad_norm={mean_grad_norm:.6f}')\n",
    "        if metrics:\n",
    "            metrics_info = ''\n",
    "            for metric_name, metric in metrics.items():\n",
    "                metrics_info += f'{metric_name}={metric(trues, preds):.4f}; '\n",
    "            print('Metrics:', metrics_info[:-2])\n",
    "        if val_dataset:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_losses = []\n",
    "                if metrics:\n",
    "                    trues = []\n",
    "                    preds = []\n",
    "                pb = tqdm(val_dataloader)\n",
    "                for (x, y) in pb:\n",
    "                    for i in range(len(x)):\n",
    "                        x[i] = x[i].to(device)\n",
    "                    y = y.to(device)\n",
    "                    pred = model._forward(*x)\n",
    "                    val_loss = criterion(pred, y)\n",
    "                    val_losses.append(val_loss.item())\n",
    "                    if metrics:\n",
    "                        trues += y.tolist()\n",
    "                        pred = model(*x)\n",
    "                        preds += pred.tolist()\n",
    "                    pb.set_description(f'Validation | loss={np.mean(val_losses):.4f}')\n",
    "                if metrics:\n",
    "                    metrics_info = ''\n",
    "                    for metric_name, metric in metrics.items():\n",
    "                        metrics_info += f'{metric_name}={metric(trues, preds):.4f}; '\n",
    "                    print('Metrics:', metrics_info[:-2])\n",
    "                if np.mean(val_losses) < best_loss:\n",
    "                    best_loss = np.mean(val_losses)\n",
    "                    best_model = deepcopy(model.cpu()).to(device)\n",
    "                    model = model.to(device)\n",
    "        print()\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    if val_dataset:\n",
    "        return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def list_replace(search, replacement, text):\n",
    "    \"\"\"\n",
    "    Replaces all symbols of text which are present\n",
    "    in the search string with the replacement string.\n",
    "    \"\"\"\n",
    "    search = [el for el in search if el in text]\n",
    "    for c in search:\n",
    "        text = text.replace(c, replacement)\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    text = list_replace \\\n",
    "        ('\\u00AB\\u00BB\\u2039\\u203A\\u201E\\u201A\\u201C\\u201F\\u2018\\u201B\\u201D\\u2019', '\\u0022', text)\n",
    "\n",
    "    text = list_replace \\\n",
    "        ('\\u2012\\u2013\\u2014\\u2015\\u203E\\u0305\\u00AF', '\\u2003\\u002D\\u002D\\u2003', text)\n",
    "\n",
    "    text = list_replace('\\u2010\\u2011', '\\u002D', text)\n",
    "\n",
    "    text = list_replace \\\n",
    "            (\n",
    "            '\\u2000\\u2001\\u2002\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200A\\u200B\\u202F\\u205F\\u2060\\u3000',\n",
    "            '\\u2002', text)\n",
    "\n",
    "    text = re.sub('\\u2003\\u2003', '\\u2003', text)\n",
    "    text = re.sub('\\t\\t', '\\t', text)\n",
    "\n",
    "    text = list_replace \\\n",
    "            (\n",
    "            '\\u02CC\\u0307\\u0323\\u2022\\u2023\\u2043\\u204C\\u204D\\u2219\\u25E6\\u00B7\\u00D7\\u22C5\\u2219\\u2062',\n",
    "            '.', text)\n",
    "\n",
    "    text = list_replace('\\u2217', '\\u002A', text)\n",
    "\n",
    "    text = list_replace('…', '...', text)\n",
    "\n",
    "    text = list_replace('\\u00C4', 'A', text)\n",
    "    text = list_replace('\\u00E4', 'a', text)\n",
    "    text = list_replace('\\u00CB', 'E', text)\n",
    "    text = list_replace('\\u00EB', 'e', text)\n",
    "    text = list_replace('\\u1E26', 'H', text)\n",
    "    text = list_replace('\\u1E27', 'h', text)\n",
    "    text = list_replace('\\u00CF', 'I', text)\n",
    "    text = list_replace('\\u00EF', 'i', text)\n",
    "    text = list_replace('\\u00D6', 'O', text)\n",
    "    text = list_replace('\\u00F6', 'o', text)\n",
    "    text = list_replace('\\u00DC', 'U', text)\n",
    "    text = list_replace('\\u00FC', 'u', text)\n",
    "    text = list_replace('\\u0178', 'Y', text)\n",
    "    text = list_replace('\\u00FF', 'y', text)\n",
    "    text = list_replace('\\u00DF', 's', text)\n",
    "    text = list_replace('\\u1E9E', 'S', text)\n",
    "    # Removing punctuation\n",
    "    text = list_replace(',.[]{}()=+-−*&^%$#@!~;:§/\\|\\?\"\\n', ' ', text)\n",
    "    # Replacing all numbers with masks\n",
    "    text = list_replace('0123456789', 'x', text)\n",
    "\n",
    "    currencies = list \\\n",
    "            (\n",
    "            '\\u20BD\\u0024\\u00A3\\u20A4\\u20AC\\u20AA\\u2133\\u20BE\\u00A2\\u058F\\u0BF9\\u20BC\\u20A1\\u20A0\\u20B4\\u20A7\\u20B0\\u20BF\\u20A3\\u060B\\u0E3F\\u20A9\\u20B4\\u20B2\\u0192\\u20AB\\u00A5\\u20AD\\u20A1\\u20BA\\u20A6\\u20B1\\uFDFC\\u17DB\\u20B9\\u20A8\\u20B5\\u09F3\\u20B8\\u20AE\\u0192'\n",
    "        )\n",
    "\n",
    "    alphabet = list \\\n",
    "            (\n",
    "            '\\t\\r абвгдеёзжийклмнопрстуфхцчшщьыъэюяАБВГДЕЁЗЖИЙКЛМНОПРСТУФХЦЧШЩЬЫЪЭЮЯabcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ ')\n",
    "\n",
    "    allowed = set(currencies + alphabet)\n",
    "\n",
    "    cleaned_text = [sym for sym in text if sym in allowed]\n",
    "    cleaned_text = ''.join(cleaned_text)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "tweet",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "class",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "8e5196a4-51b8-4335-ba4b-8d21b3d433b9",
       "rows": [
        [
         "0",
         "760402871867367424",
         "настало время для ингаляторов дружок сальбутамол где ты",
         "0"
        ],
        [
         "1",
         "1035908416869462016",
         "xx на прошлой зимней олимпиаде большинство лыжников приехало со справкой о том что у них якобы астма сделано это было для того чтобы легально принимать сальбутамол то же что и я принимаю в ингаляторах который расширяет объём легких по сути допинг для здорового человека",
         "1"
        ],
        [
         "2",
         "1089839736427032577",
         "не соглашусь с заменой зок на метопролол в таком виде первый в форме сукцината держит концентрацию в плазме сутки второй за два приёма x пика внимание вопрос а оно надо",
         "0"
        ],
        [
         "3",
         "779671488748224513",
         "dixmx мезим смекта если отравление то лоперамид",
         "0"
        ],
        [
         "4",
         "738309299756240897",
         "уберите микроволновки и имодиум действуют соулмэйты",
         "0"
        ],
        [
         "5",
         "1202310351943032834",
         "luvkismau это видео было снято во время приема оланзапина",
         "0"
        ],
        [
         "6",
         "933680693380571136",
         "кроме того есть выстраданные стандарты определения эффективности и безопасности лекарств да да после того самого талидомида которые еще не один гомеопатические препарат не прошел",
         "0"
        ],
        [
         "7",
         "738771297228951552",
         "теперь я отчаянно пытаюсь найти тату с молекулой флуоксетина",
         "0"
        ],
        [
         "8",
         "688135039578550272",
         "все этислюнтяи утверждающие будтосуицид удел слабых людей найдут тысячу причин неглотать горсть прозака импросто нехватает смелости",
         "0"
        ],
        [
         "9",
         "966241801706319878",
         "это вам не лыжи сальбутамол не поможет россия норвегия x x",
         "0"
        ],
        [
         "10",
         "1007633833922056192",
         "ципрофлоксацин x xг x р д как смысл жизни",
         "0"
        ],
        [
         "11",
         "764157235127803908",
         "очень скоро у меня закончатся все остатки сероквеля и то что я выменивала на другие лекарства опять просить у папы снять xxк на аптеку",
         "0"
        ],
        [
         "12",
         "882656373309362181",
         "пашка техник любовь с первого ксанакса",
         "0"
        ],
        [
         "13",
         "954750792900272128",
         "это жи прозак жи",
         "0"
        ],
        [
         "14",
         "1010145105192222720",
         "kissmyblacklist а не лечатся ли их футболисты от астмы синдрома дефицита внимания и т п столь любимыми у нероссийских спортсменов законными амфетамином и декстроамфетамином а",
         "0"
        ],
        [
         "15",
         "1073171045056679936",
         "есть вентолин он немного слабее но если выписали то лучше купить беродуал",
         "0"
        ],
        [
         "16",
         "851993151061598208",
         "убойный коктейль из мелатонина ламотриджина и аминофенилмасляной кислоты на ночь превратил моё пробуждение в неподъёмную цель",
         "0"
        ],
        [
         "17",
         "735850626614169600",
         "форадил комби кому нужен целые x упаковки",
         "0"
        ],
        [
         "18",
         "965614047847698432",
         "vremyapokazhet я бы порекомендовал всем сотрудникам wada выпить мельдония вдохнуть сальбутамола и попробовать приблизиться к результату любого медалиста wada вада крушельницкий пхенчханxxxx",
         "0"
        ],
        [
         "19",
         "975275534266466304",
         "если оланзапин мне не поможет тогда мне назначат галоперидол не хочу докатываться до галоперидола надо что то делать с этой проклятой тревожностью",
         "0"
        ],
        [
         "20",
         "905675633203740672",
         "ветеринар прописал ингаляции фликсотидом xxx мгк x раза в сутки и еще прописал вентолин xxx мгк на случай приступов",
         "0"
        ],
        [
         "21",
         "994882887316058112",
         "потому что они сами этими лекарствами пользуются и если та же американская хумира в среднем стоит xx тыщ то наш аналог пусть они себе в анал запихнут по возможности передайте им",
         "0"
        ],
        [
         "22",
         "1018796487378198529",
         "путать фуросемид и флуоксетин так забавно флуоксетин пожалуйста у вас есть рецепт стоит с лицом лица вспоминая когда это на мочегонные появился рецепт блять фуросемид пожалуйста",
         "0"
        ],
        [
         "23",
         "1033728517937221632",
         "венлафаксин теперь вместо золофта пойду погуглю что там за побочки такие интересные",
         "0"
        ],
        [
         "24",
         "1037315634860883970",
         "у кого есть алпразолам в лс пожалуйста",
         "0"
        ],
        [
         "25",
         "1199642365356650496",
         "northgender питер мем ещё в том что тот же сероквель пару недель ждали а потом выяснилось что его нет нигде и по заказу просто не позвонили вот ща опять ждём квентиапин уже раз четвёртый дохера аптек обшарили",
         "0"
        ],
        [
         "26",
         "800590961608916992",
         "крч sopranos это такой сериал где все крутится вокруг прозака и тех кому его назначают",
         "0"
        ],
        [
         "27",
         "1049697733823606786",
         "мне одновременно с оланзапином назначали анаприлин и ещё кучу какой то дополнительной фармы тк нейролептики всё же нагружают сердце",
         "0"
        ],
        [
         "28",
         "847691517120491520",
         "я переходу дорогу в шарагу и тут в голове у меня появляется мысль что пора бы объебаться флуоксетином",
         "0"
        ],
        [
         "29",
         "1066063916034387968",
         "поняла что никакие отхода от веществ не сравнятся с синдромом отмены пароксетина",
         "1"
        ],
        [
         "30",
         "716325452453908482",
         "флуоксетина еще две пачки гдето валяется",
         "0"
        ],
        [
         "31",
         "1020748209080217601",
         "ааааааа бэйби к твоим ногам я сыпала таблетки ксанакса прости",
         "0"
        ],
        [
         "32",
         "1026144372801318912",
         "stevepxxx без них чаще с давлением маялся тохикардией мигренями сейчас почти не бывает престариум пью и беталок как дед",
         "0"
        ],
        [
         "33",
         "1144613270982184961",
         "ну что могу сказать девочки самое время заменять мальборо айкосом а спиды ксанаксом",
         "0"
        ],
        [
         "34",
         "1125837612432928768",
         "о у меня тоже ламотриджин но тебе придётся недель шесть ждать пока он расчехлится",
         "0"
        ],
        [
         "35",
         "1127418070463598592",
         "может и совпадение но после нескольких рах использования этого предмета получил хр бронхит курил сальбутамол",
         "1"
        ],
        [
         "36",
         "836543103687344130",
         "ну типа нация прозака конечно привозят",
         "0"
        ],
        [
         "37",
         "962757500213846018",
         "кулстори про самолечение чисто по фану перестала пить паксил а теперь опять тону и удивляюсь почему это я опять тону ну такое",
         "0"
        ],
        [
         "38",
         "1063754507174191106",
         "я набираю аптека ру и там смотрю цены на лекарства и часто заказываю в аптеках всегда пытаются втюхать самое дорогое лекарство метформин например разброс от xx до xxxр за xxтб xxxед шприц xмл от x до xxруб а посмотрите аспирин",
         "0"
        ],
        [
         "39",
         "1046035932154130433",
         "вердикт суда на закупке тамифлю тимошенко наварила xx миллионов долларов",
         "0"
        ],
        [
         "40",
         "975781949464756226",
         "инфантильная амнезия забирает у нас возможно самые яркие воспоминания схоже на память действует прозак",
         "0"
        ],
        [
         "41",
         "1124655033306501122",
         "я посоветуйте витаминки друзья ксанакс экстази мдма батя ревит сорок рублей вот такая тема",
         "0"
        ],
        [
         "42",
         "1077422073008525317",
         "что плохого что poroshenko производит конфеты и торты неплохие кстати и недорогие юля умеет производить лапшу на уши недалёкого лехтората и воровать бюджетные не ей заработанные деньги тамифлю и тд зарабатывать vs воровать неужели не видно разницы",
         "0"
        ],
        [
         "43",
         "654542359246585856",
         "anatoliisharii только параше в отличии от путина пришлось опрокинуть соточку для храбрости и лоперамида от адреналина",
         "0"
        ],
        [
         "44",
         "1041620495391698945",
         "slowlypig флуоксетин ну хз у меня не такая тяжёлая форма чтобы пить что то сильнее этого",
         "0"
        ],
        [
         "45",
         "813645746079727617",
         "у меня в кармане пакетик оксикодона и я не знаю принять его или нет я отлично справляюсь со смертью дерека с",
         "0"
        ],
        [
         "46",
         "685423139073998849",
         "shizowizard это все тот же велафакс я по любому буду валяться пару часов без понимания реальности",
         "1"
        ],
        [
         "47",
         "707228174573281280",
         "piativisik остается довольствоваться растишкой сеалексом и рыбными пельменями от семи морей боюсь что и они отвернутся",
         "0"
        ],
        [
         "48",
         "930456443098140672",
         "xx ноября всемирный день борьбы с диабетом уралмаш опять отличился в поликлинике нет метформина лекарства для диабетиков",
         "0"
        ],
        [
         "49",
         "829289370607841280",
         "амурчанки продавали запрещенные препараты для похудения сибутрамин женщины заказали по почте",
         "0"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 9515
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>760402871867367424</td>\n",
       "      <td>настало время для ингаляторов дружок сальбутам...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1035908416869462016</td>\n",
       "      <td>xx на прошлой зимней олимпиаде большинство лыж...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1089839736427032577</td>\n",
       "      <td>не соглашусь с заменой зок на метопролол в так...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>779671488748224513</td>\n",
       "      <td>dixmx мезим смекта если отравление то лоперамид</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>738309299756240897</td>\n",
       "      <td>уберите микроволновки и имодиум действуют соул...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9510</th>\n",
       "      <td>669973915456925697</td>\n",
       "      <td>поставка лекарственных препаратов мнн формотер...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9511</th>\n",
       "      <td>1126889334735626240</td>\n",
       "      <td>единственная радость моей жизни прозак</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9512</th>\n",
       "      <td>1054522033151848449</td>\n",
       "      <td>anatoliisharii x героин класс a x кокаин класс...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9513</th>\n",
       "      <td>1112821163774918656</td>\n",
       "      <td>xx лет девочки разноцветные витаминки xx лет д...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9514</th>\n",
       "      <td>1044658562557980677</td>\n",
       "      <td>если желание бесконечно пихать в себя пищу пре...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9515 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                              tweet  \\\n",
       "0      760402871867367424  настало время для ингаляторов дружок сальбутам...   \n",
       "1     1035908416869462016  xx на прошлой зимней олимпиаде большинство лыж...   \n",
       "2     1089839736427032577  не соглашусь с заменой зок на метопролол в так...   \n",
       "3      779671488748224513    dixmx мезим смекта если отравление то лоперамид   \n",
       "4      738309299756240897  уберите микроволновки и имодиум действуют соул...   \n",
       "...                   ...                                                ...   \n",
       "9510   669973915456925697  поставка лекарственных препаратов мнн формотер...   \n",
       "9511  1126889334735626240             единственная радость моей жизни прозак   \n",
       "9512  1054522033151848449  anatoliisharii x героин класс a x кокаин класс...   \n",
       "9513  1112821163774918656  xx лет девочки разноцветные витаминки xx лет д...   \n",
       "9514  1044658562557980677  если желание бесконечно пихать в себя пищу пре...   \n",
       "\n",
       "      class  \n",
       "0         0  \n",
       "1         1  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "...     ...  \n",
       "9510      0  \n",
       "9511      0  \n",
       "9512      0  \n",
       "9513      0  \n",
       "9514      0  \n",
       "\n",
       "[9515 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.tsv', sep=',', encoding=\"utf-8\")\n",
    "train_df['tweet'] = train_df['tweet'].apply(lambda x: ' '.join(clean_text(x).lower().split()))\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "0    0.912559\n",
       "1    0.087441\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['class'].value_counts(normalize=True).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85446024, 1.14553976])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = ((1 / train_df['class'].value_counts(normalize=True).sort_index()) ** (1 / 8)).to_numpy()\n",
    "class_weights = class_weights * len(class_weights) / np.sum(class_weights)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.09581942, 11.43629808])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infreqs = (1 / train_df['class'].value_counts(normalize=True).sort_index()).to_numpy()\n",
    "infreqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, targets, tokenizer, max_length=128, \n",
    "                 augmenter=None, aug_prob=0.2):\n",
    "        self.texts = list(texts)\n",
    "        self.targets = list(targets) if targets is not None else None\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.augmenter = augmenter\n",
    "        self.aug_prob = aug_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def _apply_augmentation(self, text):\n",
    "        try:\n",
    "            augmented = self.augmenter.augment(text)\n",
    "            return augmented[0] if isinstance(augmented, list) else augmented\n",
    "        except Exception as e:\n",
    "            print(f\"Error in augmentation: {e}\")\n",
    "        return text\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        if self.augmenter and random.random() < self.aug_prob:\n",
    "            text = self._apply_augmentation(text)\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text, \n",
    "            padding=\"max_length\",\n",
    "            truncation=True, \n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        target = torch.tensor(self.targets[idx], dtype=torch.long) if self.targets else torch.nan\n",
    "\n",
    "        return [[input_ids, attention_mask], target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, backbone_model):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(backbone_model)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.regressor = nn.Linear(self.bert.config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x = x.pooler_output\n",
    "        x = self.dropout(x)\n",
    "        x = self.regressor(x)\n",
    "        return x\n",
    "\n",
    "    def _forward(self, input_ids, attention_mask):\n",
    "        return self.forward(input_ids=input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(y_true, y_pred):\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred = np.argmax(y_pred, axis=-1)\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred = np.argmax(y_pred, axis=-1)\n",
    "    return f1_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_model = 'ai-forever/ruBert-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(backbone_model)\n",
    "model = BERTClassifier(backbone_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texts = train_df[['tweet', 'class']].copy().dropna()\n",
    "\n",
    "train_texts, val_texts, train_targets, val_targets = \\\n",
    "    train_test_split(df_texts['tweet'], df_texts['class'].values,\n",
    "                     test_size=0.1, shuffle=True, random_state=SEED)\n",
    "\n",
    "augmenter = naw.ContextualWordEmbsAug(\n",
    "    model_path='ai-forever/ruBert-base',\n",
    "    action='substitute',\n",
    "    aug_p=0.1,\n",
    "    device='cuda',\n",
    ")\n",
    "\n",
    "train_dataset = TextClassificationDataset(train_texts, train_targets, tokenizer, augmenter=augmenter, aug_prob=0.1)\n",
    "val_dataset = TextClassificationDataset(val_texts, val_targets, tokenizer)\n",
    "\n",
    "samples_weights = pd.Series(train_dataset.targets).apply(lambda x: infreqs[x]).to_numpy()\n",
    "\n",
    "train_sampler = WeightedRandomSampler(\n",
    "    weights=samples_weights,\n",
    "    num_samples=len(train_dataset),\n",
    "    replacement=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'надо и юле предложить сдать анализы в евролаб у неё там давние связи тамифлю в украине продвигали а если серьезно то это самый реалистический сценарий она очень подозрительно смирилась со своим третьим местом как говорится без суда и следствия наверняка подстава'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'надо и ему предложить сдать анализы в евролаб у нее там давние связи тамифлю в украине но а если серьезно то это самыи реалистическии бы она как подозрительно смирилась со своим физическим состоянием как говорится без суда и следствия наверняка подстава'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset._apply_augmentation(train_dataset.texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedLabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, smoothing=0.01, class_weights=None):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.class_weights = torch.tensor(class_weights, dtype=torch.float) if class_weights is not None else None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        log_probs = F.log_softmax(x, dim=-1)\n",
    "        num_classes = x.size(-1)\n",
    "\n",
    "        # NLL Loss\n",
    "        nll_loss = -log_probs.gather(dim=-1, index=target.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        # Smooth Loss (original label smoothing)\n",
    "        smooth_target = torch.full_like(log_probs, self.smoothing / (num_classes - 1))\n",
    "        smooth_target.scatter_(-1, target.unsqueeze(-1), 1 - self.smoothing)\n",
    "        smooth_loss = - (log_probs * smooth_target).sum(dim=-1)\n",
    "\n",
    "        # Combine losses\n",
    "        loss = (1 - self.smoothing) * nll_loss + self.smoothing * smooth_loss\n",
    "\n",
    "        # Apply class weights\n",
    "        if self.class_weights is not None:\n",
    "            device = x.device\n",
    "            weights = self.class_weights.to(device)[target]\n",
    "            loss *= weights\n",
    "\n",
    "        # Normalize\n",
    "        if self.class_weights is not None:\n",
    "            return loss.sum() / weights.sum()\n",
    "        else:\n",
    "            return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup set to 134 batches (33.50 steps).\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train      | loss=0.4793 | current_lr=9.795e-6 | step_mean_grad_norm=0.000321: 100%|██████████| 536/536 [01:27<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: acc=0.7440; f1-macro=0.7408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation | loss=0.5132: 100%|██████████| 60/60 [00:03<00:00, 17.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: acc=0.7931; f1-macro=0.6253\n",
      "\n",
      "Epoch 2/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train      | loss=0.2310 | current_lr=8.917e-6 | step_mean_grad_norm=0.000220: 100%|██████████| 536/536 [01:28<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: acc=0.9125; f1-macro=0.9124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation | loss=0.4934: 100%|██████████| 60/60 [00:03<00:00, 16.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: acc=0.8330; f1-macro=0.6558\n",
      "\n",
      "Epoch 3/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train      | loss=0.1489 | current_lr=7.485e-6 | step_mean_grad_norm=0.000446: 100%|██████████| 536/536 [01:27<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: acc=0.9507; f1-macro=0.9507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation | loss=0.3550: 100%|██████████| 60/60 [00:03<00:00, 16.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: acc=0.8897; f1-macro=0.6857\n",
      "\n",
      "Epoch 4/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train      | loss=0.1016 | current_lr=5.731e-6 | step_mean_grad_norm=0.000303: 100%|██████████| 536/536 [01:26<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: acc=0.9686; f1-macro=0.9686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation | loss=0.3779: 100%|██████████| 60/60 [00:03<00:00, 16.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: acc=0.9055; f1-macro=0.7030\n",
      "\n",
      "Epoch 5/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train      | loss=0.0861 | current_lr=3.940e-6 | step_mean_grad_norm=0.000210: 100%|██████████| 536/536 [01:27<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: acc=0.9717; f1-macro=0.9717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation | loss=0.3991: 100%|██████████| 60/60 [00:03<00:00, 16.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: acc=0.9002; f1-macro=0.6980\n",
      "\n",
      "Epoch 6/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train      | loss=0.0753 | current_lr=2.402e-6 | step_mean_grad_norm=0.001748: 100%|██████████| 536/536 [01:27<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: acc=0.9777; f1-macro=0.9777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation | loss=0.4125: 100%|██████████| 60/60 [00:03<00:00, 16.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: acc=0.9034; f1-macro=0.6931\n",
      "\n",
      "Epoch 7/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train      | loss=0.0630 | current_lr=1.366e-6 | step_mean_grad_norm=0.000486: 100%|██████████| 536/536 [01:26<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: acc=0.9815; f1-macro=0.9815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation | loss=0.4366: 100%|██████████| 60/60 [00:03<00:00, 16.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: acc=0.9023; f1-macro=0.7012\n",
      "\n",
      "Epoch 8/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train      | loss=0.0583 | current_lr=1.000e-6 | step_mean_grad_norm=0.000052: 100%|██████████| 536/536 [01:26<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: acc=0.9841; f1-macro=0.9841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation | loss=0.4315: 100%|██████████| 60/60 [00:03<00:00, 16.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: acc=0.9107; f1-macro=0.7149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = train(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    epochs=8,\n",
    "    batch_size=(16, 16),\n",
    "    train_sampler=train_sampler,\n",
    "    criterion=WeightedLabelSmoothingCrossEntropy(smoothing=0.01, class_weights=class_weights),\n",
    "    lr=(1e-8, 1e-5, 1e-6),\n",
    "    warmup_epochs=0.25,\n",
    "    weight_decay=1e-2,\n",
    "    grad_accum_steps=4,\n",
    "    clip_grad_norm=1.0,\n",
    "    metrics={'acc': acc, 'f1-macro': f1},\n",
    "    device=device\n",
    ")\n",
    "\n",
    "torch.save(model.state_dict(), 'checkpoints.pth')\n",
    "model.bert.save_pretrained('checkpoints')\n",
    "with open('checkpoints.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tweet",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "2f947a26-b9af-4e72-940a-561a9924976b",
       "rows": [
        [
         "0",
         "1200838666136018946",
         "о возможно терапии баклофеном алкоголизма с эффективностью xx надо проинформировать каждого нарколога в стране и каждого алкоголика"
        ],
        [
         "1",
         "1202167857615912961",
         "homkanizli начало развиваться привыкание к сальбутамолу организм не очень хочет по утрам дышать без него"
        ],
        [
         "2",
         "1202511585744498690",
         "soloxxxxxxxxxx используем беродуал в комплексе с пульмикортом не вонючь и для ингаляций мягок"
        ],
        [
         "3",
         "1202519545501667334",
         "bunedemon у меня тоже были ады ламотриджин но бесчувственности не было и нет"
        ],
        [
         "4",
         "1200503613023555584",
         "мне кажется мне уже разряд по художественной нарезке сероквеля должны выдать"
        ],
        [
         "5",
         "1201891253362843650",
         "ксанакс избавляет вас от жизни а не стресса"
        ],
        [
         "6",
         "1200526724238397440",
         "alexeykocat ламотрин флуоксетин топиромакс ещё есть но не помню"
        ],
        [
         "7",
         "1200885839502434306",
         "купила российский а не венгерский кветиапин а он синенький и я чет прям беспокоюсь и гуглю отзывы на эту фирму да бля с"
        ],
        [
         "8",
         "1199758777643343876",
         "roriliechten кветиапин но один хер одно и то же хд"
        ],
        [
         "9",
         "1200693598045589507",
         "сожрал много баклофена и мне стало плохо упало давление прорыгался а потом прочитал что его столько нельзя можно заболеть комой"
        ],
        [
         "10",
         "1201889993519710209",
         "nskfkrf а как быть с флуоксетином на него рецепты x месяца хранят в аптеке"
        ],
        [
         "11",
         "1177488230024769536",
         "это почему то от марки зависит от велафакса меня не тошнит а от венлаксора просто ад"
        ],
        [
         "12",
         "1155406014805663746",
         "по собственной забывчивости сижу без венлаксора второй день синдром отмены начался так быстро и так жестко в аптеку доставят только вечером понедельника один помоги интересно прошлогодняя симбалта хоть немного облегчит жизнь"
        ],
        [
         "13",
         "1148370249839984640",
         "хорошое самочувствие конечно круто но вот бессонница от венлаксора не айс никакого баланса"
        ],
        [
         "14",
         "1176990084811427840",
         "ето тоже вкусно там больше про старичков но было очень мило всё это почитать и понять какой я суканахуй оптимист если без л тироксина рассчитывал жить ещё месяца три в прошлом году ага бля я начал постоянно спать уже через x недели без колёс"
        ],
        [
         "15",
         "1176548845158903810",
         "особое веселье это когда твой врач говорит что ну хер знает почему это от тироксина не должно и я так ъуъ"
        ],
        [
         "16",
         "1171085090656587777",
         "из за повышения дозы тироксина меня совершенно ужасно колбасит эмоционально не знаю что у меня там с гормонами но мне очень тяжело хочется плакать и плакать а ещё плакать говорить всем как я их люблю и плакать здец"
        ],
        [
         "17",
         "1169640190710824962",
         "если эта хуйня из за того что мне поменяли дозу л тироксина то я не хочу менять ее вообще больше никогда нахуй"
        ],
        [
         "18",
         "1168552709299023873",
         "ну здравствуй тахикардия из за тироксина"
        ],
        [
         "19",
         "1159068259095928833",
         "я извиняюсь а в верхней половине референсных значений это сколько тx x x тx xx x это ведь повышенные ттг у меня повышенный x x но был выше снизился на фоне тироксина"
        ],
        [
         "20",
         "1154083035219943425",
         "ещё раз благодарю с щитовидной буду думать что делать бо эндокринологи к которым обращалась ничего кроме л тироксина не дают"
        ],
        [
         "21",
         "1152465360874102785",
         "endocrinolog здравствуйте такой вопрос можно ли уменьшить дозировку приема л тироксина"
        ],
        [
         "22",
         "1147190433514172416",
         "основные гормоны щитовидной железы в своем составе имеют аминокислоты тирозин и йод для производства трийодтиронина тx из тироксина тx нужен минерал селен"
        ],
        [
         "23",
         "1146805046987956228",
         "бля у меня тоже походу а как мне понизить дозу тироксина d"
        ],
        [
         "24",
         "1143970818919206912",
         "недурно p s не подумайте передозировка всего лишь тироксина да и то совсем легкая"
        ],
        [
         "25",
         "1133737082675978241",
         "не это миф йод да так как входит в состав гормона щитовидной железы тироксина который контролирует скорость обмена веществ а водка это миф романтичный конечно но миф"
        ],
        [
         "26",
         "1133681533850669056",
         "причём с момента аип при снижении дозы отвес в xx кг который на xxx мг тироксина не уходили никак"
        ],
        [
         "27",
         "1133681345325150212",
         "была доза тироксина xxx сейчас ndt из которых тироксина xxа тx xx т е я будучи с аит и на тироксине почти xx лет на аип и сапплементации снизил дозу тx почти в x раза но у меня щитовидка жива таки хоть убежали в обратном"
        ],
        [
         "28",
         "1127189743811399681",
         "вот вы смеётесь а есть целая группа в вк посвящённая лечению гипотиреоза без л тироксина только натуральные щж селен витамин д и безглютеновая диета"
        ],
        [
         "29",
         "1126232843330445317",
         "блин ну это так напышенно сказано я так понял у тебя тоже с гормонами беда она неверное имела ввиду то что из за нарушения выработки гормонов щитовидки гормона ттг а следовательно и трийодтиронина и тироксина ты имеешь трудности в характере"
        ],
        [
         "30",
         "1123521304337035265",
         "шёл xый день приёма тироксина при минимальной дозировке уже ухудшения высыпаний на лице стало больше губы сухие как у мумии физически не выходит нормально просыпаться сильная сонливость болит голова и хуже концентрация где тут помощь то оно стоило того"
        ],
        [
         "31",
         "1118803017896218624",
         "я покупаю на ибее жизнь до ндт и после ндт это две тотально разные жизни я вам скажу я на тироксине много лет сидел и доза росла а отёки вес и фибромиалгия оставались на ндт все прошло но там есть особенности перехода с тироксина почитайте на хроническаяусталость рф"
        ],
        [
         "32",
         "1118801295555616769",
         "у меня на xxx мкг тироксина был ттг x x а на xx мкг ндт ттг x xx и минус xx кг"
        ],
        [
         "33",
         "1118795323131006976",
         "я бы задался задачей перехода с тироксина на ndt намного лучше работает"
        ],
        [
         "34",
         "1118791802465681409",
         "я пока ещё в самом начале пути нашла нормального эндокринолога который пошёл дальше чем анализ на ттг и гемоглобин хороший значит нет анемии врач нашла очень низкий ферритин недостаток магния и дx то есть доза л тироксина рассчитана верно но не усваивается организмом"
        ],
        [
         "35",
         "1112279998843506688",
         "покупайте эутирокс он производится в германии у него большая линейка дозировок в отличии от л тироксина я сама на заместительной терапии с xx года про лиотиронин вообще не слышала"
        ],
        [
         "36",
         "1112266385634271237",
         "я писала статью об ухудшении качества l тироксина еще в xxxx году"
        ],
        [
         "37",
         "1110187010940116994",
         "да видимо снова нужна коррекция тироксина ох уж эта щитовидка"
        ],
        [
         "38",
         "1108708473154519040",
         "кстати еще один приятный факт таблетки которые мне нужно принимать всю жизнь иначе моя щитовидка захерачит сама себя не продаются в россии более того российские врачи их не выписывают а минздрав уверен что людям вполне хватит эутирокса и л тироксина спойлер не хватит"
        ],
        [
         "39",
         "1105150208038653952",
         "когда смотришь на свои залежи тироксина в тумбочке и думаешь о смысле жизни"
        ],
        [
         "40",
         "1095357735380701184",
         "и о важно приятном доктор сказал что у меня есть шанс слезть с тироксина у меня есть шанс стать на самом деле здоровым"
        ],
        [
         "41",
         "1089977269400031232",
         "для нормальной работы щитовидки необходим йод этот микроэлемент входит в состав гормонов тироксина трийодтиронина обеспечивая их основные биологические функции вместе с тем при некоторых горманальных заболеваниях он противопаказан а нам собираются всю соль в продаже с йодом"
        ],
        [
         "42",
         "1089391878125076480",
         "внезапно нашла статью о том как похудеть с помощью тироксина увидела дозировки и не поверила глазам я человек без щитовидки принимаю xxxмкг и это очень много а тут для всратого похудения предлагают пить xxx два месяца но виноваты все равно будут врачи"
        ],
        [
         "43",
         "1069585942393040896",
         "и если человек болен и у него мало свободного тироксина то тогда преимущество у биотина с ним и связывается большинство магнитных наночастиц а дальше специальный прибор подсчитывает на мембране с рекордно высокой точностью число магнитных наночастиц и ставит диагноз"
        ],
        [
         "44",
         "1066358679438589952",
         "формула женщины гармония полноценная жизнь к сожалению после сорока лет в организме женщины снижается уровень эстрогена окситоцина тироксина и других гормонов эффективно бады"
        ],
        [
         "45",
         "1061308453539508236",
         "гнев в его чистом виде это практически один норадреналин иногда небольшие вкрапления тестостерона и тироксина последние в большей мере это спутники раздражения а не гнева кортизол это быстрее к неконтролируемой ярости бездумной агрессии"
        ],
        [
         "46",
         "1061241600196702208",
         "по сути поведение крайне безответственное у меня например гипотериоз заболевание не такое страшное как диабет но запас л тироксина у меня всегда есть хоть он и отпускается без рецепта и стоит рублей xxx в зависимости от дозировки но кто знает что может быть"
        ],
        [
         "47",
         "1057284268177661953",
         "я ттт пока в идеальном состоянии если в анализы не смотреть единственная жалоба рассосалась как только снизили дозу l тироксина и перестали доставать по сто раз на дню"
        ],
        [
         "48",
         "1055004479995760641",
         "идеальный протеин от steel power для употребления в любое время богат bcaa усиливает выработку тиреоидного гормона тироксина тx который является одним из основных регуляторов метаболического темпа xxx гр за xxxx рублей в атлетxx п"
        ],
        [
         "49",
         "1050398024076337152",
         "в аптеке посоветовалась насчет бессонницы от л тироксина мне показали снотворное я перетрухала и попросила новопассит"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 1504
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1200838666136018946</td>\n",
       "      <td>о возможно терапии баклофеном алкоголизма с эф...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1202167857615912961</td>\n",
       "      <td>homkanizli начало развиваться привыкание к сал...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1202511585744498690</td>\n",
       "      <td>soloxxxxxxxxxx используем беродуал в комплексе...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1202519545501667334</td>\n",
       "      <td>bunedemon у меня тоже были ады ламотриджин но ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1200503613023555584</td>\n",
       "      <td>мне кажется мне уже разряд по художественной н...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>1135162030636355585</td>\n",
       "      <td>аноны срочно требуется помощь пожалуйста кто е...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>1137990280483868673</td>\n",
       "      <td>дети и подростки с сдвг которым были назначены...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>1050945039189401605</td>\n",
       "      <td>однажды поднимаясь на гору я думала что ну уж ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>808645125115117568</td>\n",
       "      <td>мое новогоднее настроение застряло между турбу...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>855320089456857089</td>\n",
       "      <td>у меня какая то хуйня с тироксином и пролактином</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1504 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                              tweet\n",
       "0     1200838666136018946  о возможно терапии баклофеном алкоголизма с эф...\n",
       "1     1202167857615912961  homkanizli начало развиваться привыкание к сал...\n",
       "2     1202511585744498690  soloxxxxxxxxxx используем беродуал в комплексе...\n",
       "3     1202519545501667334  bunedemon у меня тоже были ады ламотриджин но ...\n",
       "4     1200503613023555584  мне кажется мне уже разряд по художественной н...\n",
       "...                   ...                                                ...\n",
       "1499  1135162030636355585  аноны срочно требуется помощь пожалуйста кто е...\n",
       "1500  1137990280483868673  дети и подростки с сдвг которым были назначены...\n",
       "1501  1050945039189401605  однажды поднимаясь на гору я думала что ну уж ...\n",
       "1502   808645125115117568  мое новогоднее настроение застряло между турбу...\n",
       "1503   855320089456857089   у меня какая то хуйня с тироксином и пролактином\n",
       "\n",
       "[1504 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('test.tsv', sep=',', encoding=\"utf-8\")\n",
    "test_df['tweet'] = test_df['tweet'].apply(lambda x: ' '.join(clean_text(x).lower().split()))\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TextClassificationDataset(test_df['tweet'], None, tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...: 100%|██████████| 188/188 [00:03<00:00, 61.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for (x, y) in tqdm(test_dataloader, desc='Processing...'):\n",
    "        for i in range(len(x)):\n",
    "            x[i] = x[i].to(device)\n",
    "        ans = model(*x)\n",
    "        preds += ans.tolist()\n",
    "preds = np.array(preds)\n",
    "preds = np.argmax(preds, axis=-1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tweet",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "class",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "8f11e3d5-2f23-4252-9481-6e0b20b310c8",
       "rows": [
        [
         "0",
         "1200838666136018946",
         "о возможно терапии баклофеном алкоголизма с эффективностью xx надо проинформировать каждого нарколога в стране и каждого алкоголика",
         "0"
        ],
        [
         "1",
         "1202167857615912961",
         "homkanizli начало развиваться привыкание к сальбутамолу организм не очень хочет по утрам дышать без него",
         "1"
        ],
        [
         "2",
         "1202511585744498690",
         "soloxxxxxxxxxx используем беродуал в комплексе с пульмикортом не вонючь и для ингаляций мягок",
         "0"
        ],
        [
         "3",
         "1202519545501667334",
         "bunedemon у меня тоже были ады ламотриджин но бесчувственности не было и нет",
         "0"
        ],
        [
         "4",
         "1200503613023555584",
         "мне кажется мне уже разряд по художественной нарезке сероквеля должны выдать",
         "0"
        ],
        [
         "5",
         "1201891253362843650",
         "ксанакс избавляет вас от жизни а не стресса",
         "0"
        ],
        [
         "6",
         "1200526724238397440",
         "alexeykocat ламотрин флуоксетин топиромакс ещё есть но не помню",
         "0"
        ],
        [
         "7",
         "1200885839502434306",
         "купила российский а не венгерский кветиапин а он синенький и я чет прям беспокоюсь и гуглю отзывы на эту фирму да бля с",
         "0"
        ],
        [
         "8",
         "1199758777643343876",
         "roriliechten кветиапин но один хер одно и то же хд",
         "0"
        ],
        [
         "9",
         "1200693598045589507",
         "сожрал много баклофена и мне стало плохо упало давление прорыгался а потом прочитал что его столько нельзя можно заболеть комой",
         "1"
        ],
        [
         "10",
         "1201889993519710209",
         "nskfkrf а как быть с флуоксетином на него рецепты x месяца хранят в аптеке",
         "0"
        ],
        [
         "11",
         "1177488230024769536",
         "это почему то от марки зависит от велафакса меня не тошнит а от венлаксора просто ад",
         "1"
        ],
        [
         "12",
         "1155406014805663746",
         "по собственной забывчивости сижу без венлаксора второй день синдром отмены начался так быстро и так жестко в аптеку доставят только вечером понедельника один помоги интересно прошлогодняя симбалта хоть немного облегчит жизнь",
         "0"
        ],
        [
         "13",
         "1148370249839984640",
         "хорошое самочувствие конечно круто но вот бессонница от венлаксора не айс никакого баланса",
         "1"
        ],
        [
         "14",
         "1176990084811427840",
         "ето тоже вкусно там больше про старичков но было очень мило всё это почитать и понять какой я суканахуй оптимист если без л тироксина рассчитывал жить ещё месяца три в прошлом году ага бля я начал постоянно спать уже через x недели без колёс",
         "1"
        ],
        [
         "15",
         "1176548845158903810",
         "особое веселье это когда твой врач говорит что ну хер знает почему это от тироксина не должно и я так ъуъ",
         "0"
        ],
        [
         "16",
         "1171085090656587777",
         "из за повышения дозы тироксина меня совершенно ужасно колбасит эмоционально не знаю что у меня там с гормонами но мне очень тяжело хочется плакать и плакать а ещё плакать говорить всем как я их люблю и плакать здец",
         "1"
        ],
        [
         "17",
         "1169640190710824962",
         "если эта хуйня из за того что мне поменяли дозу л тироксина то я не хочу менять ее вообще больше никогда нахуй",
         "0"
        ],
        [
         "18",
         "1168552709299023873",
         "ну здравствуй тахикардия из за тироксина",
         "1"
        ],
        [
         "19",
         "1159068259095928833",
         "я извиняюсь а в верхней половине референсных значений это сколько тx x x тx xx x это ведь повышенные ттг у меня повышенный x x но был выше снизился на фоне тироксина",
         "0"
        ],
        [
         "20",
         "1154083035219943425",
         "ещё раз благодарю с щитовидной буду думать что делать бо эндокринологи к которым обращалась ничего кроме л тироксина не дают",
         "0"
        ],
        [
         "21",
         "1152465360874102785",
         "endocrinolog здравствуйте такой вопрос можно ли уменьшить дозировку приема л тироксина",
         "0"
        ],
        [
         "22",
         "1147190433514172416",
         "основные гормоны щитовидной железы в своем составе имеют аминокислоты тирозин и йод для производства трийодтиронина тx из тироксина тx нужен минерал селен",
         "0"
        ],
        [
         "23",
         "1146805046987956228",
         "бля у меня тоже походу а как мне понизить дозу тироксина d",
         "0"
        ],
        [
         "24",
         "1143970818919206912",
         "недурно p s не подумайте передозировка всего лишь тироксина да и то совсем легкая",
         "1"
        ],
        [
         "25",
         "1133737082675978241",
         "не это миф йод да так как входит в состав гормона щитовидной железы тироксина который контролирует скорость обмена веществ а водка это миф романтичный конечно но миф",
         "0"
        ],
        [
         "26",
         "1133681533850669056",
         "причём с момента аип при снижении дозы отвес в xx кг который на xxx мг тироксина не уходили никак",
         "1"
        ],
        [
         "27",
         "1133681345325150212",
         "была доза тироксина xxx сейчас ndt из которых тироксина xxа тx xx т е я будучи с аит и на тироксине почти xx лет на аип и сапплементации снизил дозу тx почти в x раза но у меня щитовидка жива таки хоть убежали в обратном",
         "0"
        ],
        [
         "28",
         "1127189743811399681",
         "вот вы смеётесь а есть целая группа в вк посвящённая лечению гипотиреоза без л тироксина только натуральные щж селен витамин д и безглютеновая диета",
         "0"
        ],
        [
         "29",
         "1126232843330445317",
         "блин ну это так напышенно сказано я так понял у тебя тоже с гормонами беда она неверное имела ввиду то что из за нарушения выработки гормонов щитовидки гормона ттг а следовательно и трийодтиронина и тироксина ты имеешь трудности в характере",
         "1"
        ],
        [
         "30",
         "1123521304337035265",
         "шёл xый день приёма тироксина при минимальной дозировке уже ухудшения высыпаний на лице стало больше губы сухие как у мумии физически не выходит нормально просыпаться сильная сонливость болит голова и хуже концентрация где тут помощь то оно стоило того",
         "1"
        ],
        [
         "31",
         "1118803017896218624",
         "я покупаю на ибее жизнь до ндт и после ндт это две тотально разные жизни я вам скажу я на тироксине много лет сидел и доза росла а отёки вес и фибромиалгия оставались на ндт все прошло но там есть особенности перехода с тироксина почитайте на хроническаяусталость рф",
         "0"
        ],
        [
         "32",
         "1118801295555616769",
         "у меня на xxx мкг тироксина был ттг x x а на xx мкг ндт ттг x xx и минус xx кг",
         "0"
        ],
        [
         "33",
         "1118795323131006976",
         "я бы задался задачей перехода с тироксина на ndt намного лучше работает",
         "0"
        ],
        [
         "34",
         "1118791802465681409",
         "я пока ещё в самом начале пути нашла нормального эндокринолога который пошёл дальше чем анализ на ттг и гемоглобин хороший значит нет анемии врач нашла очень низкий ферритин недостаток магния и дx то есть доза л тироксина рассчитана верно но не усваивается организмом",
         "0"
        ],
        [
         "35",
         "1112279998843506688",
         "покупайте эутирокс он производится в германии у него большая линейка дозировок в отличии от л тироксина я сама на заместительной терапии с xx года про лиотиронин вообще не слышала",
         "0"
        ],
        [
         "36",
         "1112266385634271237",
         "я писала статью об ухудшении качества l тироксина еще в xxxx году",
         "0"
        ],
        [
         "37",
         "1110187010940116994",
         "да видимо снова нужна коррекция тироксина ох уж эта щитовидка",
         "1"
        ],
        [
         "38",
         "1108708473154519040",
         "кстати еще один приятный факт таблетки которые мне нужно принимать всю жизнь иначе моя щитовидка захерачит сама себя не продаются в россии более того российские врачи их не выписывают а минздрав уверен что людям вполне хватит эутирокса и л тироксина спойлер не хватит",
         "0"
        ],
        [
         "39",
         "1105150208038653952",
         "когда смотришь на свои залежи тироксина в тумбочке и думаешь о смысле жизни",
         "0"
        ],
        [
         "40",
         "1095357735380701184",
         "и о важно приятном доктор сказал что у меня есть шанс слезть с тироксина у меня есть шанс стать на самом деле здоровым",
         "0"
        ],
        [
         "41",
         "1089977269400031232",
         "для нормальной работы щитовидки необходим йод этот микроэлемент входит в состав гормонов тироксина трийодтиронина обеспечивая их основные биологические функции вместе с тем при некоторых горманальных заболеваниях он противопаказан а нам собираются всю соль в продаже с йодом",
         "0"
        ],
        [
         "42",
         "1089391878125076480",
         "внезапно нашла статью о том как похудеть с помощью тироксина увидела дозировки и не поверила глазам я человек без щитовидки принимаю xxxмкг и это очень много а тут для всратого похудения предлагают пить xxx два месяца но виноваты все равно будут врачи",
         "0"
        ],
        [
         "43",
         "1069585942393040896",
         "и если человек болен и у него мало свободного тироксина то тогда преимущество у биотина с ним и связывается большинство магнитных наночастиц а дальше специальный прибор подсчитывает на мембране с рекордно высокой точностью число магнитных наночастиц и ставит диагноз",
         "0"
        ],
        [
         "44",
         "1066358679438589952",
         "формула женщины гармония полноценная жизнь к сожалению после сорока лет в организме женщины снижается уровень эстрогена окситоцина тироксина и других гормонов эффективно бады",
         "0"
        ],
        [
         "45",
         "1061308453539508236",
         "гнев в его чистом виде это практически один норадреналин иногда небольшие вкрапления тестостерона и тироксина последние в большей мере это спутники раздражения а не гнева кортизол это быстрее к неконтролируемой ярости бездумной агрессии",
         "0"
        ],
        [
         "46",
         "1061241600196702208",
         "по сути поведение крайне безответственное у меня например гипотериоз заболевание не такое страшное как диабет но запас л тироксина у меня всегда есть хоть он и отпускается без рецепта и стоит рублей xxx в зависимости от дозировки но кто знает что может быть",
         "0"
        ],
        [
         "47",
         "1057284268177661953",
         "я ттт пока в идеальном состоянии если в анализы не смотреть единственная жалоба рассосалась как только снизили дозу l тироксина и перестали доставать по сто раз на дню",
         "0"
        ],
        [
         "48",
         "1055004479995760641",
         "идеальный протеин от steel power для употребления в любое время богат bcaa усиливает выработку тиреоидного гормона тироксина тx который является одним из основных регуляторов метаболического темпа xxx гр за xxxx рублей в атлетxx п",
         "0"
        ],
        [
         "49",
         "1050398024076337152",
         "в аптеке посоветовалась насчет бессонницы от л тироксина мне показали снотворное я перетрухала и попросила новопассит",
         "0"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 1504
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1200838666136018946</td>\n",
       "      <td>о возможно терапии баклофеном алкоголизма с эф...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1202167857615912961</td>\n",
       "      <td>homkanizli начало развиваться привыкание к сал...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1202511585744498690</td>\n",
       "      <td>soloxxxxxxxxxx используем беродуал в комплексе...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1202519545501667334</td>\n",
       "      <td>bunedemon у меня тоже были ады ламотриджин но ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1200503613023555584</td>\n",
       "      <td>мне кажется мне уже разряд по художественной н...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>1135162030636355585</td>\n",
       "      <td>аноны срочно требуется помощь пожалуйста кто е...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>1137990280483868673</td>\n",
       "      <td>дети и подростки с сдвг которым были назначены...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>1050945039189401605</td>\n",
       "      <td>однажды поднимаясь на гору я думала что ну уж ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>808645125115117568</td>\n",
       "      <td>мое новогоднее настроение застряло между турбу...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>855320089456857089</td>\n",
       "      <td>у меня какая то хуйня с тироксином и пролактином</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1504 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                              tweet  \\\n",
       "0     1200838666136018946  о возможно терапии баклофеном алкоголизма с эф...   \n",
       "1     1202167857615912961  homkanizli начало развиваться привыкание к сал...   \n",
       "2     1202511585744498690  soloxxxxxxxxxx используем беродуал в комплексе...   \n",
       "3     1202519545501667334  bunedemon у меня тоже были ады ламотриджин но ...   \n",
       "4     1200503613023555584  мне кажется мне уже разряд по художественной н...   \n",
       "...                   ...                                                ...   \n",
       "1499  1135162030636355585  аноны срочно требуется помощь пожалуйста кто е...   \n",
       "1500  1137990280483868673  дети и подростки с сдвг которым были назначены...   \n",
       "1501  1050945039189401605  однажды поднимаясь на гору я думала что ну уж ...   \n",
       "1502   808645125115117568  мое новогоднее настроение застряло между турбу...   \n",
       "1503   855320089456857089   у меня какая то хуйня с тироксином и пролактином   \n",
       "\n",
       "      class  \n",
       "0         0  \n",
       "1         1  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "...     ...  \n",
       "1499      1  \n",
       "1500      1  \n",
       "1501      0  \n",
       "1502      0  \n",
       "1503      0  \n",
       "\n",
       "[1504 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['class'] = preds\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[['id', 'class']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
