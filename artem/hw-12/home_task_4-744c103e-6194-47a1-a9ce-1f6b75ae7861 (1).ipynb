{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2FDVspYdIl4"
   },
   "source": [
    "**Homework main task**: overcome Mistral hallucinations in chat mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nALviN0PetMT"
   },
   "source": [
    "### Plan:\n",
    "\n",
    "- Test the pre-trained model as an online store assistant.\n",
    "- Completely train the model using the LoRA method on data from chatbot communication with clients on various products.\n",
    "- Test the completed model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8mkjNUdebSE"
   },
   "source": [
    ":You need to fill in all the blankes in the code (there are 7 spaces in total)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XK8WVBwzhy6d"
   },
   "source": [
    "**Note!** Please safe all cells outputs, otherwise the task will not be accepted!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RMBKcUhhl7i"
   },
   "source": [
    "# Fine-tune Mistral 7B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tYLWI6YDsuXU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mlflow 2.11.3 requires packaging<24, but you have packaging 24.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install peft transformers bitsandbytes accelerate trl datasets sentencepiece -U -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "u__5Qp_frMGA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJqIm1QqhxmB"
   },
   "source": [
    "Let's first test how the Mistral 7b, pre-trained in Russian (the original model was trained only in English), will cope with this task. Let the model play the role of an assistant for an online smartphone store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hYmD4GYPs6EW"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"IlyaGusev/saiga_mistral_7b\"\n",
    "MESSAGE_TEMPLATE = \"<s>{role}\\n{content}</s>\"\n",
    "RESPONSE_TEMPLATE = \"<s>bot\\n\"\n",
    "SYSTEM_PROMPT = \"\"\"Ты – чат-бот технической поддержки Xiaomi Store, который помогает клиенту выбрать наиболее подходящий для него смартфон. Опираясь на описание смартфонов, помоги клиенту выбрать наиболее подходящий для него смартфон. Если ответа на вопрос клиента нет в приведенном описании, ответь \"У меня не достаточно информации для ответа на ваш вопрос. Обратитесь пожалуйста к менеджеру в telegram\".\n",
    "Описание смартфонов из интернет-магазина сотовой связи Xiaomi Store:\n",
    "1. Смартфон Xiaomi Redmi Note 10 Pro: Этот смартфон оснащен дисплеем Super AMOLED с разрешением 1080 x 2400 пикселей, что обеспечивает четкую и яркую картинку. Он также имеет камеру на 64 Мп с возможностью записи видео в 4K. Процессор Snapdragon 732G обеспечивает отличную производительность, а аккумулятор на 5020 мАч обеспечивает долгое время работы.\n",
    "2. Смартфон Xiaomi Mi 11 Lite: Этот смартфон имеет ультратонкий и легкий дизайн, который удобно держать в руке. Он оснащен 6.55-дюймовым AMOLED-дисплеем с разрешением 1080 x 2400 пикселей. Камера на 64 Мп способна делать качественные фотографии, а быстрый процессор Qualcomm Snapdragon 732G позволяет работать с приложениями плавно.\n",
    "3. Смартфон Xiaomi Redmi 9T: Этот смартфон оснащен 6.53-дюймовым IPS-дисплеем с разрешением 1080 x 2340 пикселей, который обеспечивает реалистичную цветопередачу. Камера на 48 Мп делает четкие фото, а процессор Snapdragon 662 обеспечивает быструю работу. Аккумулятор на 6000 мАч позволяет использовать устройство долгое время без подзарядки.\n",
    "4. Смартфон Xiaomi Mi 10T Pro: Этот смартфон оснащен 6.67-дюймовым IPS-дисплеем с разрешением 1080 x 2400 пикселей. Он имеет камеру на 108 Мп, что позволяет делать потрясающие фотографии. Процессор Qualcomm Snapdragon 865 обеспечивает высокую производительность, а аккумулятор на 5000 мАч гарантирует долгое время работы.\n",
    "5. Смартфон Xiaomi Redmi Note 9 Pro: Этот смартфон оснащен 6.67-дюймовым IPS-дисплеем с разрешением 1080 x 2400 пикселей. Камера на 64 Мп с поддержкой искусственного интеллекта позволяет делать яркие и четкие фотографии. Процессор Snapdragon 720G обеспечивает отличную производительность, а аккумулятор на 5020 мАч обеспечивает долгое время работы.\n",
    "6. Смартфон Xiaomi Mi 10T Lite: Этот смартфон оснащен 6.67-дюймовым IPS-дисплеем с разрешением 1080 x 2400 пикселей. Он имеет камеру на 64 Мп, что позволяет делать яркие и детальные фотографии. Процессор Qualcomm Snapdragon 750G ускоряет работу с приложениями, а аккумулятор на 4820 мАч обеспечивает долгую автономность.\n",
    "7. Смартфон Xiaomi Redmi Note 8 Pro: Этот смартфон оснащен 6.53-дюймовым IPS-дисплеем с разрешением 1080 x 2340 пикселей. Камера на 64 Мп делает четкие и яркие фото, а процессор MediaTek Helio G90T обеспечивает плавную работу. Аккумулятор на 4500 мАч достаточно емкий для долгого использования.\n",
    "8. Смартфон Xiaomi Mi 10 Pro: Этот смартфон оснащен 6.67-дюймовым AMOLED-дисплеем с разрешением 1080 x 2340 пикселей. Камера на 108 Мп и оптическая стабилизация изображения позволяют делать высококачественные фотографии. Процессор Snapdragon 865 ускоряет работу с приложениями, а аккумулятор на 4500 мАч достаточно емкий.\n",
    "9. Смартфон Xiaomi Redmi 9C: Этот смартфон оснащен 6.53-дюймовым IPS-дисплеем с разрешением 720 x 1600 пикселей. Он имеет камеру на 13 Мп, которая делает четкие фото в хороших условиях освещения. Процессор MediaTek Helio G35 обеспечивает достаточную производительность для базовых задач, а аккумулятор на 5000 мАч обеспечивает долгое время работы.\n",
    "10. Смартфон Xiaomi Mi 11 Ultra: Этот смартфон оснащен 6.81-дюймовым AMOLED-дисплеем с разрешением 1440 x 3200 пикселей. Он имеет камеру на 50 Мп, а также вспомогательные камеры для различных эффектов съемки. Процессор Snapdragon 888 обеспечивает высокую производительность, а аккумулятор на 5000 мАч гарантирует долгое время работы.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "D6ceOj-RtDpH"
   },
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_template=MESSAGE_TEMPLATE,\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        response_template=RESPONSE_TEMPLATE\n",
    "    ):\n",
    "        # Инициализация класса Conversation с шаблонами сообщений и системным запросом.\n",
    "        # message_template - шаблон для каждого сообщения (например, форматирование текста).\n",
    "        # system_prompt - начальный запрос, задающий контекст разговора.\n",
    "        # response_template - шаблон для ответа, который будет добавлен в конце текста.\n",
    "\n",
    "        self.message_template = message_template  # Сохранение шаблона сообщений.\n",
    "        self.response_template = response_template  # Сохранение шаблона ответа.\n",
    "        # Инициализация списка сообщений с первым сообщением от системы, определяющим контекст беседы.\n",
    "        self.messages = [{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        }]\n",
    "\n",
    "    def add_user_message(self, message):\n",
    "        # Метод для добавления сообщения пользователя в список сообщений.\n",
    "        self.messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message\n",
    "        })\n",
    "\n",
    "    def add_bot_message(self, message):\n",
    "        # Метод для добавления сообщения бота в список сообщений.\n",
    "        self.messages.append({\n",
    "            \"role\": \"bot\",\n",
    "            \"content\": message\n",
    "        })\n",
    "\n",
    "    def get_prompt(self, tokenizer):\n",
    "        # Метод для формирования финального текста запроса, который будет отправлен для генерации ответа.\n",
    "        final_text = \"\"  # Инициализация переменной для хранения окончательного текста.\n",
    "        for message in self.messages:\n",
    "            # Форматирование каждого сообщения в соответствии с message_template.\n",
    "            message_text = self.message_template.format(**message)\n",
    "            final_text += message_text  # Добавление отформатированного текста к итоговому.\n",
    "\n",
    "        final_text += RESPONSE_TEMPLATE  # Добавление шаблона ответа в конце текста.\n",
    "        return final_text.strip()  # Возвращение итогового текста без лишних пробелов в начале и в конце.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0Tfx9wT3tIGp"
   },
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, generation_config):\n",
    "    # Функция для генерации текста на основе заданного промпта с использованием модели и токенизатора.\n",
    "\n",
    "    # Токенизация промпта: превращаем текстовый промпт в числовые представления (токены),\n",
    "    # которые понимает модель. Параметр return_tensors=\"pt\" означает, что данные\n",
    "    # будут возвращены в формате PyTorch тензоров. add_special_tokens=False указывает,\n",
    "    # что специальные токены (например, [CLS], [SEP]) не будут добавлены.\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "    # Переносим данные на устройство, на котором работает модель (например, GPU или CPU).\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "\n",
    "    # Генерация выходных идентификаторов (токенов) с использованием модели.\n",
    "    # Параметр generation_config задает конфигурацию генерации, например,\n",
    "    # максимальную длину ответа, температуру и т.д.\n",
    "    output_ids = model.generate(\n",
    "        **data,\n",
    "        generation_config=generation_config\n",
    "    )[0]\n",
    "\n",
    "    # Обрезаем выходные идентификаторы, чтобы исключить исходные токены промпта.\n",
    "    # Таким образом, оставляем только сгенерированный моделью ответ.\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
    "\n",
    "    # Декодируем токены обратно в текст, исключая специальные токены, такие как [PAD] или [SEP].\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Возвращаем итоговый сгенерированный текст, убирая лишние пробелы в начале и в конце.\n",
    "    return output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IcfEtzF2tSBI",
    "outputId": "b368f646-d74d-46ca-aeda-dc22ad22b053"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 1,\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"max_new_tokens\": 1536,\n",
       "  \"no_repeat_ngram_size\": 15,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"repetition_penalty\": 1.1,\n",
       "  \"temperature\": 0.2,\n",
       "  \"top_k\": 40,\n",
       "  \"top_p\": 0.9\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "1b881efe662a497db0575fef49f56948",
      "60c36f6f3aa847e29ac218f9b42d0e7c",
      "aba4667d8dac4e818e14480f3a60d310",
      "08d5b0058b734f1daf2ce1e30d09755e",
      "2d85631ce3e04420b8ecc8001a1beff8",
      "731468c4fc3849009c4413d3aa18a6a0",
      "d30de0e9f2d140e0893de1512cf1f00d",
      "6698060f628f4f22a2239883c8ca9424",
      "e6ab5f2c404b48749569590870117aef",
      "3dd64ffb2f9a4e069a97b6ff26458816",
      "87d179d339e44e578faca140d7aac828"
     ]
    },
    "id": "NyManLv7tUFv",
    "outputId": "1d0e2706-ab7f-471b-e6e6-ce0c006b5ba6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.33s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32002, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): MistralRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загружаем конфигурацию модели PeftConfig из предобученной модели, используя ее имя.\n",
    "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Загружаем предобученную модель для задачи автозавершения текста (Causal Language Modeling).\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"./offload\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    offload_folder=\"./offload\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "\n",
    "# Переводим модель в режим оценки, чтобы отключить функции, используемые только при обучении,\n",
    "# такие как dropout, и настроить модель на генерацию предсказаний.\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_HlOjMwMtW04"
   },
   "outputs": [],
   "source": [
    "conversation = Conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "EmGXnnEatZvp",
    "outputId": "b8b317ad-ebb6-4b42-d5bb-9bbdb8ead03f"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Клиент:  Мне нужен планшет для просмотра 4K видео.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Чат-бот: Выбор планшета для просмотра 4K видео зависит от многих факторов, таких как размер экрана, мощности процессора, объем памяти и других характеристик. Однако, если вы ищете планшет с максимально возможным разрешением экрана, то можно рассмотреть следующие модели:\n",
      "\n",
      "1. Apple iPad Pro (12.9-дюймовый экран) - это один из самых мощных планшетов на рынке, который поддерживает просмотр 4K видео.\n",
      "2. Samsung Galaxy Tab S7+ (12.4-дюймовый экран) - это другая мощная модель с поддержкой 4K видео.\n",
      "3. Microsoft Surface Pro 7 (12.3-дюймовый экран) - это универсальный планшет, который также поддерживает просмотр 4K видео, но его мощность может быть меньше, чем у предыдущих двух моделей.\n",
      "\n",
      "Однако, стоит учесть, что просмотр 4K видео на планшете требует большого объема памяти и мощного процессора, чтобы обеспечить плавное воспроизведение видео. Также стоит учитывать, что просмотр 4K видео на пленшет может потребовать больше энергии, что может снизить время работы планшета.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Клиент:  \n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_uttr = input(\"Клиент: \")\n",
    "    conversation.add_user_message(user_uttr)\n",
    "    prompt = conversation.get_prompt(tokenizer)\n",
    "    output = generate(model, tokenizer, prompt, generation_config)\n",
    "    output = output.split(\"bot\")[0].strip()\n",
    "    print(f\"Чат-бот: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9VYwBwkjP8A"
   },
   "source": [
    "Let's start a conversation. Try asking to tell about tablets that have 4K video quality. We see that the bot starts to invent non-existent information, that is, it starts to hallucinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbFbuumkju4_"
   },
   "source": [
    "Downloading data for additional fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oHn7o0rn1Qjg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-04-11 01:32:32--  https://docs.google.com/uc?export=download&id=1_xKwkNFisKzDQ1CPPDqvKWdPRzZvO4MT\n",
      "Resolving docs.google.com (docs.google.com)... 142.251.1.194, 2a00:1450:4010:c0d::c2\n",
      "connected. to docs.google.com (docs.google.com)|142.251.1.194|:443... \n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://drive.usercontent.google.com/download?id=1_xKwkNFisKzDQ1CPPDqvKWdPRzZvO4MT&export=download [following]\n",
      "--2025-04-11 01:32:32--  https://drive.usercontent.google.com/download?id=1_xKwkNFisKzDQ1CPPDqvKWdPRzZvO4MT&export=download\n",
      "173.194.221.132, 2a00:1450:4001:801::2001rive.usercontent.google.com)... \n",
      "connected. to drive.usercontent.google.com (drive.usercontent.google.com)|173.194.221.132|:443... \n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 70497894 (67M) [application/octet-stream]\n",
      "Saving to: ‘company_cases.json’\n",
      "\n",
      "company_cases.json  100%[===================>]  67.23M  52.0MB/s    in 1.3s    \n",
      "\n",
      "2025-04-11 01:32:39 (52.0 MB/s) - ‘company_cases.json’ saved [70497894/70497894]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1_xKwkNFisKzDQ1CPPDqvKWdPRzZvO4MT' -O company_cases.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fX-eE4Dx1pAF"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTTrainer, SFTConfig, DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "f6t0HfEOu_b0"
   },
   "outputs": [],
   "source": [
    "with open(\"company_cases.json\", 'r') as inp:\n",
    "    raw_dataset = json.load(inp)\n",
    "\n",
    "train_raw_dataset, test_raw_dataset = train_test_split(raw_dataset)\n",
    "train_test_raw_dataset = {\"train\": train_raw_dataset, \"test\": test_raw_dataset}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeoNwnUcj9Nt"
   },
   "source": [
    "Let's convert the dataset into the format used in the Dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "auxOuusLvfae"
   },
   "outputs": [],
   "source": [
    "raw_dataset_dict = {}\n",
    "for data_type in [\"train\", \"test\"]:\n",
    "    raw_dataset_dict[data_type] = {\n",
    "        \"instruction\": [element['instruction'] for element in train_test_raw_dataset[data_type]],\n",
    "        \"personality\": [element['personality'] for element in train_test_raw_dataset[data_type]],\n",
    "        \"context\": [element['context'] for element in train_test_raw_dataset[data_type]],\n",
    "        \"dialog_start_line\": [element['instruction'] for element in train_test_raw_dataset[data_type]],\n",
    "        \"dialog\": [element['dialog'] for element in train_test_raw_dataset[data_type]]\n",
    "    }\n",
    "\n",
    "train_dataset = Dataset.from_dict(raw_dataset_dict[\"train\"])\n",
    "test_dataset = Dataset.from_dict(raw_dataset_dict[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "el_YrV5fj4Ez"
   },
   "source": [
    "Function for formatting the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "buvCQ5jrvi7A"
   },
   "outputs": [],
   "source": [
    "def formatting_prompt_func(example):\n",
    "    prompt = f\"<s>system\\n{example['instruction']}\"\n",
    "    if example['personality']:\n",
    "        prompt += f\"\\n{example['personality']}\"\n",
    "    prompt += f\"\\n{example['context']}\"\n",
    "    if example['dialog_start_line']:\n",
    "        prompt += f\"\\n{example['dialog_start_line']}\"\n",
    "    prompt += \"</s>\"\n",
    "    for element in example['dialog']:\n",
    "        prompt += f\"<s>{element['role']}\\n{element['content']}</s>\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjA-Xy5IkLug"
   },
   "source": [
    "For additional training, we will take not the ready Russian-language Mistral, but the pre-trained Mistral on the Open-Orca dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZcE7QObvvk3z"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Open-Orca/Mistral-7B-OpenOrca\")\n",
    "tokenizer.pad_token_id = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDDDT6cgkJKp"
   },
   "source": [
    "Let's make the gradient during training flow only through the tokens of the last replica of the chatbot, and let's make the labels for the remaining tokens equal to -100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "cb6Jmqr4vmm4"
   },
   "outputs": [],
   "source": [
    "response_template = \"bot\\n\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Sb3ptnSLvo4v"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.76s/it]\n"
     ]
    }
   ],
   "source": [
    "# Будем обучать модель в int4 для уменьшения требуемой видеопамяти\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Open-Orca/Mistral-7B-OpenOrca\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yV8Xfkhkg_3"
   },
   "source": [
    "Initializing LoraConfig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9DFYOoXovs1H"
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnpWIfYykmv6"
   },
   "source": [
    "Setting parameters for additional training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PBp2aMHKvvT2"
   },
   "outputs": [],
   "source": [
    "training_arguments = SFTConfig(\n",
    "    output_dir=\"checkpoints\",\n",
    "    report_to=\"none\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    max_length=2048\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqruUzVThaMk"
   },
   "source": [
    "Disable tokenizer parallelism to avoid deadlocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "K1f8cTn-hZYk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "il3hvsNrkrQy"
   },
   "source": [
    "Starting fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLkxESXcvxkC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying formatting function to train dataset: 100%|██████████| 8354/8354 [00:01<00:00, 4819.66 examples/s]\n",
      "Converting train dataset to ChatML: 100%|██████████| 8354/8354 [00:00<00:00, 10062.53 examples/s]\n",
      "Applying chat template to train dataset: 100%|██████████| 8354/8354 [00:00<00:00, 13568.03 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 8354/8354 [00:20<00:00, 408.33 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 8354/8354 [00:00<00:00, 11046.60 examples/s]\n",
      "Applying formatting function to eval dataset: 100%|██████████| 2785/2785 [00:00<00:00, 4502.06 examples/s]\n",
      "Converting eval dataset to ChatML: 100%|██████████| 2785/2785 [00:00<00:00, 12792.61 examples/s]\n",
      "Applying chat template to eval dataset: 100%|██████████| 2785/2785 [00:00<00:00, 13088.34 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 2785/2785 [00:06<00:00, 424.28 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 2785/2785 [00:00<00:00, 12271.70 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='983' max='1044' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 983/1044 2:26:21 < 09:06, 0.11 it/s, Epoch 0.94/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.611100</td>\n",
       "      <td>0.610746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    formatting_func=formatting_prompt_func,\n",
    "    data_collator=collator,\n",
    "    peft_config=peft_config,\n",
    "    args=training_arguments,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tARaNoT7kwLN"
   },
   "source": [
    "Test the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bIXcbzhjQmA-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2_IdbgOmA1U"
   },
   "source": [
    "Loading the adapter checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eNBbYnW0QpdG"
   },
   "outputs": [],
   "source": [
    "BASE_MODEL = \"Open-Orca/Mistral-7B-OpenOrca\"\n",
    "ADAPTER_MODEL = \"checkpoints/checkpoint-1044\"\n",
    "MESSAGE_TEMPLATE = \"<s>{role}\\n{content}</s>\"\n",
    "RESPONSE_TEMPLATE = \"<s>bot\\n\"\n",
    "SYSTEM_PROMPT = \"\"\"Ты – чат-бот технической поддержки Xiaomi Store, который помогает клиенту выбрать наиболее подходящий для него смартфон. Опираясь на описание смартфонов, помоги клиенту выбрать наиболее подходящий для него смартфон. Если ответа на вопрос клиента нет в приведенном описании, ответь \"У меня не достаточно информации для ответа на ваш вопрос. Обратитесь пожалуйста к менеджеру в telegram\".{personality}\n",
    "Описание смартфонов из интернет-магазина сотовой связи Xiaomi Store:\n",
    "1. Смартфон Xiaomi Redmi Note 10 Pro: Этот смартфон оснащен дисплеем Super AMOLED с разрешением 1080 x 2400 пикселей, что обеспечивает четкую и яркую картинку. Он также имеет камеру на 64 Мп с возможностью записи видео в 4K. Процессор Snapdragon 732G обеспечивает отличную производительность, а аккумулятор на 5020 мАч обеспечивает долгое время работы.\n",
    "2. Смартфон Xiaomi Mi 11 Lite: Этот смартфон имеет ультратонкий и легкий дизайн, который удобно держать в руке. Он оснащен 6.55-дюймовым AMOLED-дисплеем с разрешением 1080 x 2400 пикселей. Камера на 64 Мп способна делать качественные фотографии, а быстрый процессор Qualcomm Snapdragon 732G позволяет работать с приложениями плавно.\n",
    "3. Смартфон Xiaomi Redmi 9T: Этот смартфон оснащен 6.53-дюймовым IPS-дисплеем с разрешением 1080 x 2340 пикселей, который обеспечивает реалистичную цветопередачу. Камера на 48 Мп делает четкие фото, а процессор Snapdragon 662 обеспечивает быструю работу. Аккумулятор на 6000 мАч позволяет использовать устройство долгое время без подзарядки.\n",
    "4. Смартфон Xiaomi Mi 10T Pro: Этот смартфон оснащен 6.67-дюймовым IPS-дисплеем с разрешением 1080 x 2400 пикселей. Он имеет камеру на 108 Мп, что позволяет делать потрясающие фотографии. Процессор Qualcomm Snapdragon 865 обеспечивает высокую производительность, а аккумулятор на 5000 мАч гарантирует долгое время работы.\n",
    "5. Смартфон Xiaomi Redmi Note 9 Pro: Этот смартфон оснащен 6.67-дюймовым IPS-дисплеем с разрешением 1080 x 2400 пикселей. Камера на 64 Мп с поддержкой искусственного интеллекта позволяет делать яркие и четкие фотографии. Процессор Snapdragon 720G обеспечивает отличную производительность, а аккумулятор на 5020 мАч обеспечивает долгое время работы.\n",
    "6. Смартфон Xiaomi Mi 10T Lite: Этот смартфон оснащен 6.67-дюймовым IPS-дисплеем с разрешением 1080 x 2400 пикселей. Он имеет камеру на 64 Мп, что позволяет делать яркие и детальные фотографии. Процессор Qualcomm Snapdragon 750G ускоряет работу с приложениями, а аккумулятор на 4820 мАч обеспечивает долгую автономность.\n",
    "7. Смартфон Xiaomi Redmi Note 8 Pro: Этот смартфон оснащен 6.53-дюймовым IPS-дисплеем с разрешением 1080 x 2340 пикселей. Камера на 64 Мп делает четкие и яркие фото, а процессор MediaTek Helio G90T обеспечивает плавную работу. Аккумулятор на 4500 мАч достаточно емкий для долгого использования.\n",
    "8. Смартфон Xiaomi Mi 10 Pro: Этот смартфон оснащен 6.67-дюймовым AMOLED-дисплеем с разрешением 1080 x 2340 пикселей. Камера на 108 Мп и оптическая стабилизация изображения позволяют делать высококачественные фотографии. Процессор Snapdragon 865 ускоряет работу с приложениями, а аккумулятор на 4500 мАч достаточно емкий.\n",
    "9. Смартфон Xiaomi Redmi 9C: Этот смартфон оснащен 6.53-дюймовым IPS-дисплеем с разрешением 720 x 1600 пикселей. Он имеет камеру на 13 Мп, которая делает четкие фото в хороших условиях освещения. Процессор MediaTek Helio G35 обеспечивает достаточную производительность для базовых задач, а аккумулятор на 5000 мАч обеспечивает долгое время работы.\n",
    "10. Смартфон Xiaomi Mi 11 Ultra: Этот смартфон оснащен 6.81-дюймовым AMOLED-дисплеем с разрешением 1440 x 3200 пикселей. Он имеет камеру на 50 Мп, а также вспомогательные камеры для различных эффектов съемки. Процессор Snapdragon 888 обеспечивает высокую производительность, а аккумулятор на 5000 мАч гарантирует долгое время работы.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xftgOaEpQtnH"
   },
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_template=MESSAGE_TEMPLATE,\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        response_template=RESPONSE_TEMPLATE,\n",
    "        personality=None\n",
    "    ):\n",
    "        self.message_template = message_template\n",
    "        self.response_template = response_template\n",
    "        if personality:\n",
    "            system_prompt = system_prompt.format(personality=personality)\n",
    "        else:\n",
    "            system_prompt = system_prompt.format(personality=\"\")\n",
    "        self.messages = [{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        }]\n",
    "\n",
    "    def add_user_message(self, message):\n",
    "        self.messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message\n",
    "        })\n",
    "\n",
    "    def add_bot_message(self, message):\n",
    "        self.messages.append({\n",
    "            \"role\": \"bot\",\n",
    "            \"content\": message\n",
    "        })\n",
    "\n",
    "    def get_prompt(self, tokenizer):\n",
    "        final_text = \"\"\n",
    "        for message in self.messages:\n",
    "            message_text = self.message_template.format(**message)\n",
    "            final_text += message_text\n",
    "        final_text += RESPONSE_TEMPLATE\n",
    "        return final_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "EB4bh1U1QyIN"
   },
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, generation_config):\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "    output_ids = model.generate(\n",
    "        **data,\n",
    "        generation_config=generation_config\n",
    "    )[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    return output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "W7V0kJ2MQ1CL"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=False)\n",
    "tokenizer.pad_token_id = 0\n",
    "generation_config = GenerationConfig(\n",
    "    pad_token_id=0,\n",
    "    bos_token_id=1,\n",
    "    eos_token_id=2,\n",
    "    temperature=0.2,\n",
    "    top_p=0.9,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "p9wUZBSgQ64L"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.30s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32002, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): MistralRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, ADAPTER_MODEL, torch_dtype=torch.float16)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7y0XOd29RBG_"
   },
   "outputs": [],
   "source": [
    "conversation = Conversation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTVEBA7QmZgc"
   },
   "source": [
    "We start the dialogue again and ask about the tablets. Were you able to overcome the hallucinations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uEbsJX89RGOV"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Клиент:  Мне нужен планшет для просмотра 4K видео.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Клиент: Мне нужен планшет для просмотра 4K видео.\n",
      "Чат-бот: К сожалению, у меня нет информации о планшетах, которые могут просматривать 4K видео. Могу ли я помочь вам с выбором смартфона?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Клиент:  \n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_uttr = input(\"Клиент: \")\n",
    "    print(f\"Клиент: {user_uttr}\")\n",
    "    conversation.add_user_message(user_uttr)\n",
    "    prompt = conversation.get_prompt(tokenizer)\n",
    "    output = generate(model, tokenizer, prompt, generation_config)\n",
    "    output = output.split(\"bot\")[0].replace(\"<s>\", \"\").replace(\"</s>\", \"\").strip()\n",
    "    print(f\"Чат-бот: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "08d5b0058b734f1daf2ce1e30d09755e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3dd64ffb2f9a4e069a97b6ff26458816",
      "placeholder": "​",
      "style": "IPY_MODEL_87d179d339e44e578faca140d7aac828",
      "value": " 1/2 [00:42&lt;00:42, 43.00s/it]"
     }
    },
    "1b881efe662a497db0575fef49f56948": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_60c36f6f3aa847e29ac218f9b42d0e7c",
       "IPY_MODEL_aba4667d8dac4e818e14480f3a60d310",
       "IPY_MODEL_08d5b0058b734f1daf2ce1e30d09755e"
      ],
      "layout": "IPY_MODEL_2d85631ce3e04420b8ecc8001a1beff8"
     }
    },
    "2d85631ce3e04420b8ecc8001a1beff8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3dd64ffb2f9a4e069a97b6ff26458816": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "60c36f6f3aa847e29ac218f9b42d0e7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_731468c4fc3849009c4413d3aa18a6a0",
      "placeholder": "​",
      "style": "IPY_MODEL_d30de0e9f2d140e0893de1512cf1f00d",
      "value": "Loading checkpoint shards:  50%"
     }
    },
    "6698060f628f4f22a2239883c8ca9424": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "731468c4fc3849009c4413d3aa18a6a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87d179d339e44e578faca140d7aac828": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aba4667d8dac4e818e14480f3a60d310": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6698060f628f4f22a2239883c8ca9424",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e6ab5f2c404b48749569590870117aef",
      "value": 1
     }
    },
    "d30de0e9f2d140e0893de1512cf1f00d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e6ab5f2c404b48749569590870117aef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
