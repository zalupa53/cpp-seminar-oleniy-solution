{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "64d60d94",
      "metadata": {
        "id": "64d60d94"
      },
      "source": [
        "### Task: Sentiment Classification of Movie Reviews  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MnFtJo_vQ7fd",
      "metadata": {
        "id": "MnFtJo_vQ7fd"
      },
      "source": [
        "Alice is a time traveler who visits different eras in the past to solve important missions. While there, she must always be careful to disguise herself so that no one will know she is from the future. This time, she joined an NLP company in 2014 year and was assigned the task of sentiment analysis on user reviews for movies. Help Alice with this task."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09CmnLTit1a0",
      "metadata": {
        "id": "09CmnLTit1a0"
      },
      "source": [
        "You need to solve sentiment classification task using the imdb movie review dataset. Each review is labeled as either positive (1) or negative (0), indicating its sentiment. You will be provided by basic LinearSVC classifier with TF-IDF features.\n",
        "\n",
        "You need to solve 3 tasks:\n",
        "\n",
        "1.   Task1: Text Preprocessing with spaCy (this is your baseline)\n",
        "2.   Task 2: Adding Part-of-Speech (POS) Features as a TF-IDF for Each POS Category\n",
        "3.   Task 3: Development of new features to improve classification accuracy\n",
        "\n",
        "**Note!** Do not change the classifier. Change only cells with TODO mark.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "995ac04c",
      "metadata": {
        "id": "995ac04c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.feature_extraction.text import (\n",
        "    TfidfVectorizer,\n",
        "    CountVectorizer,\n",
        ")\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e83541ef",
      "metadata": {
        "id": "e83541ef"
      },
      "outputs": [],
      "source": [
        "os.environ[\"PYTHONHASHSEED\"] = str(42)\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e40afc00",
      "metadata": {
        "id": "e40afc00"
      },
      "source": [
        "### Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4e_6CBdQXOD1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e_6CBdQXOD1",
        "outputId": "9c083343-3f94-4cce-d074-b582e53d3e36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/gotheartem/.local/lib/python3.11/site-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1C6TIP8c33fHM6dxs6DoxJeKY6ZXGWpBx\n",
            "To: /home/gotheartem/Projects/ioai-hw/hw-9/imdb_train_hw1.csv\n",
            "100%|██████████████████████████████████████| 8.25M/8.25M [00:01<00:00, 6.04MB/s]\n",
            "/home/gotheartem/.local/lib/python3.11/site-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1K8WBFVVvVlsvIMRG8HiaFkldiyuNkLD2\n",
            "To: /home/gotheartem/Projects/ioai-hw/hw-9/imdb_test_hw1.csv\n",
            "100%|██████████████████████████████████████| 2.10M/2.10M [00:00<00:00, 10.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "! gdown --id 1C6TIP8c33fHM6dxs6DoxJeKY6ZXGWpBx\n",
        "! gdown --id 1K8WBFVVvVlsvIMRG8HiaFkldiyuNkLD2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "808c6df4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "808c6df4",
        "outputId": "70b1bec8-b442-4ba3-aaf6-628199bee0ad"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "Unnamed: 0",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "label",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "text",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "conversionMethod": "pd.DataFrame",
              "ref": "888cfd44-f0bc-4ba8-babd-0ba23d2c8632",
              "rows": [
                [
                  "8681",
                  "8681",
                  "1",
                  "I noticed this movie was getting trashed well before it hit the theaters and I too didn't have high hopes for it. I figured it was another \"You Got Served\" type of movie with some nice dance moves and horrid acting. I was at the theater and deciding between this and Meet the Spartans and picked this. To my surprise the acting wasn't bad at all and the movie was actually pretty good. The fact that it has a lower rating than You Got Served is absolutely ridiculous."
                ],
                [
                  "2362",
                  "2362",
                  "1",
                  "When it comes to creating a universe George Lucas is the undisputed master and his final Star Wars film is very, very good (and more appropriately rated in comparison to the two previous films in the original saga). Having recently seen Revenge of the Sith really puts this movie in perspective. The final battle seems even more climactic knowing what Anakin Skywalker went through at the manipulative hands of the Emperor. It also makes the final battle between Luke and Vader more bitter considering the love he felt for Padmé and the love she felt for her children. Actually while the new films (especially Episode II) are inferior to the original films they are good for one reason only."
                ],
                [
                  "6232",
                  "6232",
                  "0",
                  "\"National Treasure\" (2004) is a thoroughly misguided hodge-podge of plot entanglements that borrow from nearly every cloak and dagger government conspiracy cliché that has ever been written. The film stars Nicholas Cage as Benjamin Franklin Gates (how precious is that, I ask you? ); a seemingly normal fellow who, for no other reason than being of a lineage of like-minded misguided fortune hunters, decides to steal a 'national treasure' that has been hidden by the United States founding fathers. After a bit of subtext and background that plays laughably (unintentionally) like Indiana Jones meets The Patriot, the film degenerates into one misguided whimsy after another  attempting to create a 'Stanley Goodspeed' regurgitation of Nicholas Cage and launch the whole convoluted mess forward with a series of high octane, but disconnected misadventures.<br /><br />The relevancy and logic to having George Washington and his motley crew of patriots burying a king's ransom someplace on native soil, and then, going through the meticulous plan of leaving clues scattered throughout U.S. currency art work, is something that director Jon Turteltaub never quite gets around to explaining. Couldn't Washington found better usage for such wealth during the start up of the country? Hence, we are left with a mystery built on top of an enigma that is already on shaky ground by the time Ben appoints himself the new custodian of this untold wealth. Ben's intentions are noble  if confusing."
                ],
                [
                  "1318",
                  "1318",
                  "1",
                  "I must admit - the only reason I bought this movie was because I am a big fan of Gackt and a *huge* fan of Hyde. I was expecting a good movie with a lot of shots that were, shall we say, pleasing to the feminine eye but a slightly cheesy story. I mean, the synopsis sounded really out there. And now that I have just finished watching it - I feel the need to tell the world of its brilliance! Hyde and Gackt both gave heart-wrenching performances, and my eyes are still hot from the crying that lasted throughout the last half of the movie."
                ],
                [
                  "543",
                  "543",
                  "1",
                  "Ten out of the 11 short films in this movie are masterpieces (I found only the Egyptian one disappointing). Stragely, all but the Mexican director chose to portray the problems of individuals or groups in connection with 9-11: the Afghan refugees, deaf people, Palestinians, the widows of Srebrenica, AIDS and poverty and corruption in Africa, Pinochets coup and ensuing bloodbath, suicide bombings in Israel, paranoia-hit and state-persecuted Muslim Americans in the USA, old people living alone, and the aftermath of WWII in the hearts of Asian soldiers. This might say something sad about the limits of empathy, in both ways: the directors might feel that Americans ignore the pains of the rest of the world and only care about their own tragedies, while they effectively do the same with their short films.<br /><br />Surprising myself, I found Sean Penn's piece one of the very best in the collection, and ***SPOILER AHEAD*** I also guess his portrayal of Ernest Borgnine as a half-crazy old man vegetating in a New York flat experiencing his widow life's happiest moment when the Sun shines through his window after the WTC \"collapsed out of light's way\", I guess this might also be one of the most offending as the general American audience would see it."
                ]
              ],
              "shape": {
                "columns": 3,
                "rows": 5
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8681</th>\n",
              "      <td>8681</td>\n",
              "      <td>1</td>\n",
              "      <td>I noticed this movie was getting trashed well ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2362</th>\n",
              "      <td>2362</td>\n",
              "      <td>1</td>\n",
              "      <td>When it comes to creating a universe George Lu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6232</th>\n",
              "      <td>6232</td>\n",
              "      <td>0</td>\n",
              "      <td>\"National Treasure\" (2004) is a thoroughly mis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1318</th>\n",
              "      <td>1318</td>\n",
              "      <td>1</td>\n",
              "      <td>I must admit - the only reason I bought this m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>543</th>\n",
              "      <td>543</td>\n",
              "      <td>1</td>\n",
              "      <td>Ten out of the 11 short films in this movie ar...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0  label                                               text\n",
              "8681        8681      1  I noticed this movie was getting trashed well ...\n",
              "2362        2362      1  When it comes to creating a universe George Lu...\n",
              "6232        6232      0  \"National Treasure\" (2004) is a thoroughly mis...\n",
              "1318        1318      1  I must admit - the only reason I bought this m...\n",
              "543          543      1  Ten out of the 11 short films in this movie ar..."
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train = pd.read_csv(\"imdb_train_hw1.csv\")\n",
        "df_test = pd.read_csv(\"imdb_test_hw1.csv\")\n",
        "df_train.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "MkhanC2f0J5t",
      "metadata": {
        "id": "MkhanC2f0J5t"
      },
      "outputs": [],
      "source": [
        "y_train = df_train[\"label\"]\n",
        "y_test = df_test[\"label\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe84f9a2",
      "metadata": {
        "id": "fe84f9a2"
      },
      "source": [
        "Since the classes in our dataset are nearly balanced, we can use accuracy as the evaluation metric. Accuracy provides a straightforward measure of how well the model classifies reviews correctly across both sentiment classes.  \n",
        "\n",
        "However, we will consider the F1-score for a more detailed performance assessment. Even with balanced classes, the model might still be biased towards one class due to feature distributions (e.g., it may predict negative reviews more confidently than positive ones).  \n",
        "\n",
        "The F1-score, which is the harmonic mean of precision and recall, helps us identify such imbalances. It ensures that both false positives and false negatives are accounted for, providing a better understanding of how well the model performs on each sentiment class."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cb026b8",
      "metadata": {
        "id": "8cb026b8"
      },
      "source": [
        "## 0. LinearSVC with TF-IDF Features  \n",
        "\n",
        "We will now train a LinearSVC model using TF-IDF (Term Frequency-Inverse Document Frequency) as features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ad2e6298",
      "metadata": {
        "id": "ad2e6298"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(df_train[\"text\"])\n",
        "X_test_tfidf = vectorizer.transform(df_test[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9967a971",
      "metadata": {
        "id": "9967a971"
      },
      "outputs": [],
      "source": [
        "y_train = df_train[\"label\"]\n",
        "y_test = df_test[\"label\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fcdbbec5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcdbbec5",
        "outputId": "71343aa4-7b84-418a-c9c0-2c31da49d71c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy (TF-IDF): 0.841747984726347\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.84      0.85      1213\n",
            "           1       0.83      0.84      0.84      1144\n",
            "\n",
            "    accuracy                           0.84      2357\n",
            "   macro avg       0.84      0.84      0.84      2357\n",
            "weighted avg       0.84      0.84      0.84      2357\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = LinearSVC(random_state=42)\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "print(\"Accuracy (TF-IDF):\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "411df717",
      "metadata": {
        "id": "411df717"
      },
      "source": [
        "The model's accuracy using TF-IDF is 0.8417 (84.17%) this our **baseline result**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1028a21b",
      "metadata": {
        "id": "1028a21b"
      },
      "source": [
        "## Task1: Text Preprocessing with spaCy\n",
        "\n",
        "Lemmatize original review texts with [spacy ](https://spacy.io/usage/linguistic-features#lemmatization)library.\n",
        "With spacy remove:\n",
        "\n",
        "*   stop words\n",
        "*   punctuation\n",
        "*   digits\n",
        "*   emails\n",
        "*   numbers\n",
        "*   empty word\n",
        "\n",
        "Train classifier with a new tf-idf representation of text. Obtain baseline classification metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9c29ba1d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "bc0fe2f6",
      "metadata": {
        "id": "bc0fe2f6"
      },
      "outputs": [],
      "source": [
        "#TODO function take text as an argument and return cleaned text\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "def clean_text(text):\n",
        "    doc = nlp(text)\n",
        "    cleaned_tokens = []\n",
        "    \n",
        "    for token in doc:\n",
        "        if token.is_stop or token.is_punct or token.is_digit or token.like_email or token.like_num or token.text.strip() == '':\n",
        "            continue\n",
        "        cleaned_tokens.append(token.lemma_.lower().strip())\n",
        "    \n",
        "    return \" \".join(cleaned_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8bb510b1",
      "metadata": {
        "id": "8bb510b1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "007d81e35a7248d09289edeca3b86fab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/9427 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4de8fd25512d49d7ae2d4133823175ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2357 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df_train[\"text_lemmatized\"] = df_train[\"text\"].progress_apply(clean_text)\n",
        "df_test[\"text_lemmatized\"] = df_test[\"text\"].progress_apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c29cf065",
      "metadata": {
        "id": "c29cf065"
      },
      "outputs": [],
      "source": [
        "# TODO get tf-idf vectors for your lemmatized texts\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf_lemmatized = vectorizer.fit_transform(df_train[\"text_lemmatized\"])\n",
        "X_test_tfidf_lemmatized = vectorizer.transform(df_test[\"text_lemmatized\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e1a5e75a",
      "metadata": {
        "id": "e1a5e75a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy (TF-IDF): 0.8413237165888842\n"
          ]
        }
      ],
      "source": [
        "model = LinearSVC(random_state=42)\n",
        "model.fit(X_train_tfidf_lemmatized, y_train)\n",
        "y_pred = model.predict(X_test_tfidf_lemmatized)\n",
        "print(\"Accuracy (TF-IDF):\", accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5itZ2DUtzUJ5",
      "metadata": {
        "id": "5itZ2DUtzUJ5"
      },
      "source": [
        "This is your **baseline** metrics!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72baffa9",
      "metadata": {
        "id": "72baffa9"
      },
      "source": [
        "## Task 2: Adding Part-of-Speech (POS) Features as a TF-IDF for Each POS Category\n",
        "\n",
        "For each text add part-of-speach (pos) tags as feature in TF-IDF manner. Use Spacy to get pos tag features. Combine them with lemmatized tf-idf features, obtained in the Task1.\n",
        "\n",
        "For example, if you have two sentences with following tf-idf vectors:\n",
        "\n",
        "1.   sent1: \"The cat sat on the mat.\" -> [0.63, 0.44, 0.31, 0.31, 0.44, 0, 0]\n",
        "2.   sent2: \"The dog sat on the floor. \" -> [0.63, 0, 0.31, 0.31, 0, 0.44, 0.44]\n",
        "\n",
        "And you obtained the following pos tag features (with dictionary {'det': 1, 'noun': 2, 'verb': 3, 'adp': 0}):\n",
        "\n",
        "*   sent1: [0.63, 0.63, 0.31, 0.31]\n",
        "*   sent2: [0.63, 0.63, 0.31, 0.31]\n",
        "\n",
        "\n",
        "Then final representation should be:\n",
        "\n",
        "*   sent1: [0.63, 0.44, 0.31, 0.31, 0.44, 0, 0, 0.63, 0.63, 0.31, 0.31]\n",
        "*   sent2: [0.63, 0, 0.31, 0.31, 0, 0.44, 0.44, 0.63, 0.63, 0.31, 0.31]\n",
        "\n",
        "**Note!** Do not use pos tags punctuation and empty words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "KjuTut0a1Y9H",
      "metadata": {
        "id": "KjuTut0a1Y9H"
      },
      "outputs": [],
      "source": [
        "# TODO function takes text as input and return string with pos tags joined by a space.\n",
        "\n",
        "def extract_pos_tags(text):\n",
        "    doc = nlp(text)\n",
        "    pos_tags = [token.pos_ for token in doc if not token.is_punct and token.text.strip() != '']\n",
        "    return \" \".join(pos_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1f0d0af0",
      "metadata": {
        "id": "1f0d0af0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e7c9a52d64747949480ecb8e0003a28",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/9427 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3de4bf72b51046fc9fd69f1537eda75f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2357 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df_train[\"pos_text\"] = df_train[\"text\"].progress_apply(extract_pos_tags)\n",
        "df_test[\"pos_text\"] = df_test[\"text\"].progress_apply(extract_pos_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "674b790b",
      "metadata": {
        "id": "674b790b"
      },
      "source": [
        "We need to bring the features obtained by CountVectorizer for POS tags to the same scale as TF-IDF. The easiest way is to apply TfidfTransformer to the CountVectorizer result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "410ed982",
      "metadata": {
        "id": "410ed982"
      },
      "outputs": [],
      "source": [
        "#TODO train bag of words with pos tag features, then normalize them with TfidfTransformer, combine with X_train_tfidf_lemmatized\n",
        "# and X_test_tfidf_lemmatized features, save resulted features to the following variables:\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "pos_vectorizer = CountVectorizer()\n",
        "X_train_pos = pos_vectorizer.fit_transform(df_train[\"pos_text\"])\n",
        "X_test_pos = pos_vectorizer.transform(df_test[\"pos_text\"])\n",
        "\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_pos_tfidf = tfidf_transformer.fit_transform(X_train_pos)\n",
        "X_test_pos_tfidf = tfidf_transformer.transform(X_test_pos)\n",
        "\n",
        "X_train_combined = hstack([X_train_tfidf_lemmatized, X_train_pos_tfidf])\n",
        "X_test_combined = hstack([X_test_tfidf_lemmatized, X_test_pos_tfidf])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "3e5b908f",
      "metadata": {
        "id": "3e5b908f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy (tf-idf + POS): 0.8447178616885872\n"
          ]
        }
      ],
      "source": [
        "lr_combined = LinearSVC(random_state=42)\n",
        "lr_combined.fit(X_train_combined, y_train)\n",
        "y_pred_combined = lr_combined.predict(X_test_combined)\n",
        "\n",
        "print(\"Accuracy (tf-idf + POS):\", accuracy_score(y_test, y_pred_combined))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "378c0cd7",
      "metadata": {
        "id": "378c0cd7"
      },
      "source": [
        "## Task 3: Development of new features to improve classification accuracy\n",
        "\n",
        "Come up with another feature or set of features and help Alice improve the quality. Remember that Alice is in the past and does not have access to any . Additional training data cannot be used either. You can use third-party resources to generate features.\n",
        "\n",
        "Compare with result of your **baseline** from the Task 1. Any improvement will be counted. Use X_train_tfidf_lemmatized and X_test_tfidf_lemmatized, add combine your features with them as in task 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "vSXv61eH4G9e",
      "metadata": {
        "id": "vSXv61eH4G9e"
      },
      "outputs": [],
      "source": [
        "# TODO create your features function here, add feature explanation\n",
        "\n",
        "def get_custom_feature(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    num_nums = 0\n",
        "    for token in doc:\n",
        "        if token.like_num or token.is_digit:\n",
        "            num_nums += 1\n",
        "    \n",
        "    num_exclamations = text.count('!')\n",
        "    num_questions = text.count('?')\n",
        "    \n",
        "    return [num_nums, num_exclamations, num_questions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "2cc902b8",
      "metadata": {
        "id": "2cc902b8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3f11b99358594a50a07d5bf5b6ef6139",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/9427 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75c56aba8ff640b99f3da62e9684c2d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2357 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Combine your features with X_train_tfidf_lemmatized and X_test_tfidf_lemmatized\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "train_custom_features = np.array(df_train[\"text\"].progress_apply(get_custom_feature).tolist())\n",
        "test_custom_features = np.array(df_test[\"text\"].progress_apply(get_custom_feature).tolist())\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "train_custom_features = scaler.fit_transform(train_custom_features)\n",
        "test_custom_features = scaler.transform(test_custom_features)\n",
        "\n",
        "X_train_combined = hstack([X_train_tfidf_lemmatized, train_custom_features])\n",
        "X_test_combined = hstack([X_test_tfidf_lemmatized, test_custom_features])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "eb4d8a6b",
      "metadata": {
        "id": "eb4d8a6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy (tf-idf + Custom feature): 0.8468392023759016\n"
          ]
        }
      ],
      "source": [
        "lr_combined = LinearSVC(random_state=42)\n",
        "lr_combined.fit(X_train_combined, y_train)\n",
        "y_pred_combined = lr_combined.predict(X_test_combined)\n",
        "\n",
        "print(\"Accuracy (tf-idf + Custom feature):\", accuracy_score(y_test, y_pred_combined))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2c02e74",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "eb71b2d7",
        "3dbeab72",
        "98934cf6"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
