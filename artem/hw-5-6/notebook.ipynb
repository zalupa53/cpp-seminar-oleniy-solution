{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62cb4c8e",
   "metadata": {
    "papermill": {
     "duration": 0.00362,
     "end_time": "2024-12-15T18:27:20.785766",
     "exception": false,
     "start_time": "2024-12-15T18:27:20.782146",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3da5f302",
   "metadata": {
    "_cell_guid": "60614544-f6cf-421e-bd0b-54138ec447cf",
    "_uuid": "25cd17fe-3867-4fdc-a507-a81d5d43108a",
    "papermill": {
     "duration": 8.288624,
     "end_time": "2024-12-15T18:27:29.078346",
     "exception": false,
     "start_time": "2024-12-15T18:27:20.789722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "from skimage import io\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from collections import Counter\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from albumentations import (\n",
    "    Compose, OneOf, Resize, CenterCrop, RandomCrop, Rotate,\n",
    "    HorizontalFlip, RandomBrightnessContrast, HueSaturationValue,\n",
    "    CoarseDropout, Normalize, GaussNoise\n",
    ")\n",
    "import cv2\n",
    "from multiprocessing import cpu_count\n",
    "from termcolor import colored\n",
    "from matplotlib import colors, pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "767706df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    global SEED\n",
    "    SEED = seed\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78f9b001",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f22a682a",
   "metadata": {
    "papermill": {
     "duration": 0.092359,
     "end_time": "2024-12-15T18:27:29.175949",
     "exception": false,
     "start_time": "2024-12-15T18:27:29.083590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f6de0c",
   "metadata": {
    "papermill": {
     "duration": 0.003738,
     "end_time": "2024-12-15T18:27:29.183934",
     "exception": false,
     "start_time": "2024-12-15T18:27:29.180196",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "###  Преобразование входных данных и настройка аугментации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b62b9d0",
   "metadata": {
    "papermill": {
     "duration": 0.015491,
     "end_time": "2024-12-15T18:27:29.203453",
     "exception": false,
     "start_time": "2024-12-15T18:27:29.187962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "RESCALE_SIZE = 224\n",
    "\n",
    "# Определяем блок дополнительной аугментации, аналогичный RandomChoice\n",
    "augmentation = OneOf([\n",
    "    # Вариант 1: Resize до 300, затем CenterCrop и случайное обрезание до 250\n",
    "    Compose([\n",
    "        Resize(300 * 1.5, 300 * 1.5, interpolation=cv2.INTER_LINEAR, p=1.0),\n",
    "        CenterCrop(300 * 1.5, 300 * 1.5, p=1.0),\n",
    "        RandomCrop(250 * 1.5, 250 * 1.5, p=1.0)\n",
    "    ], p=1.0),\n",
    "    # Вариант 2: Случайное вращение в диапазоне (-25, 25) градусов\n",
    "    Rotate(limit=25, border_mode=cv2.BORDER_REFLECT_101, p=1.0),\n",
    "    # Вариант 3: Обязательный горизонтальный флип\n",
    "    HorizontalFlip(p=1.0)\n",
    "], p=1.0)\n",
    "\n",
    "# Общий pipeline для обучения\n",
    "train_transform = Compose([\n",
    "    augmentation,\n",
    "    Resize(RESCALE_SIZE, RESCALE_SIZE, interpolation=cv2.INTER_LINEAR, p=1.0),\n",
    "    # Нормализация: albumentations ожидает входные значения в диапазоне 0-255\n",
    "    Normalize(mean=(0.485, 0.456, 0.406), \n",
    "              std=(0.229, 0.224, 0.225), \n",
    "              max_pixel_value=255.0, p=1.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = Compose([\n",
    "    Resize(RESCALE_SIZE, RESCALE_SIZE),\n",
    "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "test_transform = Compose([\n",
    "    HorizontalFlip(p=1.0),\n",
    "    Resize(RESCALE_SIZE, RESCALE_SIZE, interpolation=cv2.INTER_LINEAR, p=1.0),\n",
    "    # Нормализация: albumentations ожидает входные значения в диапазоне 0-255\n",
    "    Normalize(mean=(0.485, 0.456, 0.406), \n",
    "              std=(0.229, 0.224, 0.225), \n",
    "              max_pixel_value=255.0, p=1.0),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fd7fbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_aug_transform(img, n=1):\n",
    "    \"\"\"\n",
    "    Преобразует PIL.Image в массив и возвращает:\n",
    "      - original: изображение, обработанное по val_transform,\n",
    "      - aug_stack: тензор из n аугментированных изображений (train_transform)\n",
    "    \"\"\"\n",
    "    img_np = np.array(img)\n",
    "    original = val_transform(image=img_np)[\"image\"]\n",
    "    # Создаем список из n аугментированных изображений\n",
    "    augmented_list = [train_transform(image=img_np)[\"image\"] for _ in range(n)]\n",
    "    # Склеиваем в один тензор размером (n, C, H, W)\n",
    "    aug_stack = torch.stack(augmented_list)\n",
    "    return original, aug_stack\n",
    "\n",
    "def multi_aug_transform_test(img, n=1):\n",
    "    \"\"\"\n",
    "    Преобразует PIL.Image в массив и возвращает:\n",
    "      - original: изображение, обработанное по val_transform,\n",
    "      - aug_stack: тензор из n аугментированных изображений (train_transform)\n",
    "    \"\"\"\n",
    "    img_np = np.array(img)\n",
    "    original = val_transform(image=img_np)[\"image\"]\n",
    "    # Создаем список из n аугментированных изображений\n",
    "    augmented_list = [test_transform(image=img_np)[\"image\"] for _ in range(n)]\n",
    "    # Склеиваем в один тензор размером (n, C, H, W)\n",
    "    aug_stack = torch.stack(augmented_list)\n",
    "    return original, aug_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc45ccf",
   "metadata": {
    "papermill": {
     "duration": 0.003738,
     "end_time": "2024-12-15T18:27:29.211272",
     "exception": false,
     "start_time": "2024-12-15T18:27:29.207534",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e22de94",
   "metadata": {
    "papermill": {
     "duration": 20.407499,
     "end_time": "2024-12-15T18:27:49.622506",
     "exception": false,
     "start_time": "2024-12-15T18:27:29.215007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Датасеты\n",
    "dataset = ImageFolder('data/train/simpsons_dataset', \n",
    "                      transform=multi_aug_transform)\n",
    "\n",
    "labels = [sample[1] for sample in dataset.samples]\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    list(range(len(dataset))),\n",
    "    test_size=0.003,\n",
    "    stratify=labels,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Формируем списки образцов и меток для каждого датасета\n",
    "train_samples = [dataset.samples[i] for i in train_idx]\n",
    "train_targets = [dataset.targets[i] for i in train_idx]\n",
    "val_samples = [dataset.samples[i] for i in val_idx]\n",
    "val_targets = [dataset.targets[i] for i in val_idx]\n",
    "\n",
    "# Создаем два отдельных экземпляра датасета с одинаковыми корневыми данными и преобразованиями\n",
    "train_dataset = ImageFolder('data/train/simpsons_dataset', transform=multi_aug_transform)\n",
    "val_dataset = ImageFolder('data/train/simpsons_dataset', transform=lambda img: val_transform(image=np.array(img))[\"image\"])\n",
    "\n",
    "# Переопределяем выборки и метки в каждом датасете\n",
    "train_dataset.samples = train_samples\n",
    "train_dataset.targets = train_targets\n",
    "\n",
    "val_dataset.samples = val_samples\n",
    "val_dataset.targets = val_targets\n",
    "\n",
    "test_dataset = ImageFolder('data/testset/', \n",
    "                           transform=multi_aug_transform_test)#lambda img: val_transform(image=np.array(img))[\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "202b99ec",
   "metadata": {
    "papermill": {
     "duration": 0.014509,
     "end_time": "2024-12-15T18:27:49.641463",
     "exception": false,
     "start_time": "2024-12-15T18:27:49.626954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution: [4.36, 0.2, 2.98, 0.51, 6.41, 0.47, 5.7, 4.71, 0.23, 2.24, 0.04, 2.18, 0.13, 0.13, 0.58, 10.73, 2.38, 5.76, 1.48, 0.01, 6.47, 0.61, 6.17, 0.34, 1.17, 5.16, 0.08, 6.94, 6.95, 1.71, 0.15, 0.34, 5.7, 0.31, 0.22, 0.43, 0.49, 4.19, 0.19, 0.26, 0.04, 0.86]%\n"
     ]
    }
   ],
   "source": [
    "# Статистика по классам\n",
    "class_counts = np.bincount([sample[1] for sample in train_dataset.samples])\n",
    "print(f\"Class distribution: {[round(c / sum(class_counts) * 100, 2) for c in class_counts]}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b94edca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 42 artists>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(x=range(42), height=[round(c / sum(class_counts) * 100, 2) for c in class_counts])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4ccaae",
   "metadata": {
    "papermill": {
     "duration": 0.003783,
     "end_time": "2024-12-15T18:27:49.649140",
     "exception": false,
     "start_time": "2024-12-15T18:27:49.645357",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Обработка дисбаланса классов в данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c06f9fb",
   "metadata": {
    "papermill": {
     "duration": 0.046502,
     "end_time": "2024-12-15T18:27:49.699499",
     "exception": false,
     "start_time": "2024-12-15T18:27:49.652997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample weights (min, max): (0.00045001172, 0.33585876)\n",
      "Class weights (min, max): (0.00045001173513922033, 0.33585875832557144)\n"
     ]
    }
   ],
   "source": [
    "# взвешивание классов и нормализация\n",
    "from torch import batch_norm\n",
    "\n",
    "\n",
    "class_counts = np.bincount([sample[1] for sample in train_dataset.samples])\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights /= class_weights.sum()  # Нормализация\n",
    "\n",
    "# присвоение изображению веса на основе его класса\n",
    "sample_weights = np.array([class_weights[label] for _, label in train_dataset.samples], dtype=np.float32)\n",
    "\n",
    "# статистика о семплировании\n",
    "print(f\"Sample weights (min, max): {sample_weights.min(), sample_weights.max()}\")\n",
    "print(f\"Class weights (min, max): {class_weights.min(), class_weights.max()}\")\n",
    "\n",
    "# выборка с учётом весов\n",
    "train_sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "batch_size = 32\n",
    "accumulation_steps = 1024 // batch_size\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, shuffle=False, pin_memory=True, num_workers=min(8, cpu_count()))\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=min(8, cpu_count()))\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=min(8, cpu_count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e01105",
   "metadata": {
    "papermill": {
     "duration": 0.003884,
     "end_time": "2024-12-15T18:27:49.707445",
     "exception": false,
     "start_time": "2024-12-15T18:27:49.703561",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Реализация модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "135643b4",
   "metadata": {
    "papermill": {
     "duration": 0.011566,
     "end_time": "2024-12-15T18:27:49.722944",
     "exception": false,
     "start_time": "2024-12-15T18:27:49.711378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     def __init__(self, num_classes=42):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.resnet = models.resnet101(weights=models.ResNet101_Weights.IMAGENET1K_V1)\n",
    "\n",
    "#         # Замораживание весов\n",
    "#         for param in self.resnet.parameters():\n",
    "#             param.requires_grad = False\n",
    "        \n",
    "#         in_features = self.resnet.fc.in_features\n",
    "#         self.resnet.fc = nn.Sequential(\n",
    "#             nn.Linear(in_features, 512),\n",
    "#             nn.BatchNorm1d(512),\n",
    "#             nn.ReLU(),\n",
    "\n",
    "#             nn.Linear(512, num_classes),\n",
    "#         )\n",
    "\n",
    "#         # разморозим параметры последнего блока layer4 и полносвязного слоя\n",
    "#         for param in self.resnet.layer3.parameters():\n",
    "#             param.requires_grad = True\n",
    "#         for param in self.resnet.layer4.parameters():\n",
    "#             param.requires_grad = True\n",
    "#         for param in self.resnet.fc.parameters():\n",
    "#             param.requires_grad = True\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5ac9ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     def __init__(self, num_classes=42):\n",
    "#         super(Net, self).__init__()\n",
    "#         # Загружаем предобученную модель DenseNet201\n",
    "#         self.densenet = models.densenet201(weights=models.DenseNet201_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "#         # Замораживаем все параметры\n",
    "#         for param in self.densenet.parameters():\n",
    "#             param.requires_grad = False\n",
    "        \n",
    "#         # Получаем число входных признаков классификатора\n",
    "#         in_features = self.densenet.classifier.in_features\n",
    "        \n",
    "#         # Заменяем классификатор на новый блок\n",
    "#         # Здесь: сначала линейный слой расширяет размерность в 4 раза,\n",
    "#         # затем BatchNorm, ReLU, MaxPool1d (уменьшает размерность вдвое),\n",
    "#         # затем второй линейный слой, BatchNorm, ReLU и финальный линейный слой,\n",
    "#         # выдающий num_classes выходов.\n",
    "#         self.densenet.classifier = nn.Sequential(\n",
    "#             nn.Linear(in_features, in_features * 4),\n",
    "#             nn.BatchNorm1d(in_features * 4),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool1d(kernel_size=2),  # После этого размерность станет in_features*2\n",
    "\n",
    "#             nn.Linear(in_features * 2, in_features),\n",
    "#             nn.BatchNorm1d(in_features),\n",
    "#             nn.ReLU(),\n",
    "\n",
    "#             nn.Linear(in_features, num_classes),\n",
    "#         )\n",
    "        \n",
    "#         # Разморозим параметры последнего блока признаков (denseblock4) и классификатора\n",
    "#         for param in self.densenet.features.denseblock4.parameters():\n",
    "#             param.requires_grad = True\n",
    "#         for param in self.densenet.classifier.parameters():\n",
    "#             param.requires_grad = True\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.densenet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40c6571f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     def __init__(self, num_classes=42):\n",
    "#         super(Net, self).__init__()\n",
    "#         # Загружаем предобученную модель ViT128\n",
    "#         for param in self.vit.parameters():\n",
    "#             param.requires_grad = True#False\n",
    "\n",
    "#         # Разморозим параметры последнего блока энкодера\n",
    "#         for param in self.vit.encoder.layers[-1].parameters():\n",
    "#             param.requires_grad = True\n",
    "\n",
    "#         # Разморозим параметры классификатора (heads)\n",
    "#         # Если heads является nn.Sequential, то заменим последний слой\n",
    "#         if isinstance(self.vit.heads, nn.Sequential):\n",
    "#             # Предполагаем, что последний слой — это Linear\n",
    "#             in_features = self.vit.heads[-1].in_features\n",
    "#             self.vit.heads[-1] = nn.Linear(in_features, num_classes)\n",
    "#         else:\n",
    "#             # Если heads не является последовательностью\n",
    "#             in_features = self.vit.heads.in_features\n",
    "#             self.vit.heads = nn.Linear(in_features, num_classes)\n",
    "\n",
    "#         # Если требуется разморозить параметры нового классификатора:\n",
    "#         for param in self.vit.heads.parameters():\n",
    "#             param.requires_grad = True\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.vit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bdd346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_channels, kernel_size, p):\n",
    "        super(ResBlock, self).__init__()\n",
    "        assert kernel_size % 2 == 1\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size, padding=kernel_size // 2),\n",
    "            # nn.BatchNorm2d(num_features=n_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=p),\n",
    "\n",
    "            nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size, padding=kernel_size // 2),\n",
    "            nn.BatchNorm2d(num_features=n_channels),\n",
    "        )\n",
    "        self.merge = nn.Sequential(\n",
    "            nn.BatchNorm2d(num_features=n_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_ = x\n",
    "        x = self.convs(x)\n",
    "        x += x_\n",
    "        x = self.merge(x)\n",
    "        return x\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, conv_kernel_size, pool_kernel_size, p):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=conv_kernel_size, padding=conv_kernel_size // 2),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel_size),\n",
    "            nn.Dropout2d(p=p),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=42):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            ConvBlock(in_channels=3, out_channels=16, \n",
    "                      conv_kernel_size=7, pool_kernel_size=2, p=0.0),\n",
    "\n",
    "            ResBlock(n_channels=16, kernel_size=3, p=0.0),\n",
    "            ResBlock(n_channels=16, kernel_size=3, p=0.0),\n",
    "\n",
    "            ConvBlock(in_channels=16, out_channels=32, \n",
    "                      conv_kernel_size=3, pool_kernel_size=2, p=0.0),\n",
    "\n",
    "            ResBlock(n_channels=32, kernel_size=3, p=0.0),\n",
    "            ResBlock(n_channels=32, kernel_size=3, p=0.0),\n",
    "\n",
    "            ConvBlock(in_channels=32, out_channels=64, \n",
    "                      conv_kernel_size=3, pool_kernel_size=2, p=0.0),\n",
    "\n",
    "            ResBlock(n_channels=64, kernel_size=3, p=0.0),\n",
    "            ResBlock(n_channels=64, kernel_size=3, p=0.0),\n",
    "\n",
    "            ConvBlock(in_channels=64, out_channels=128, \n",
    "                      conv_kernel_size=3, pool_kernel_size=2, p=0.0),\n",
    "            ConvBlock(in_channels=128, out_channels=256, \n",
    "                      conv_kernel_size=3, pool_kernel_size=2, p=0.0),\n",
    "            ConvBlock(in_channels=256, out_channels=512, \n",
    "                      conv_kernel_size=3, pool_kernel_size=2, p=0.0),\n",
    "            ConvBlock(in_channels=512, out_channels=1024, \n",
    "                      conv_kernel_size=3, pool_kernel_size=2, p=0.0),\n",
    "        )\n",
    "\n",
    "        self.globavgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=1024 * 1 * 1, out_features=2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(p=0.4),\n",
    "\n",
    "            nn.Linear(in_features=2048, out_features=1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(p=0.0),\n",
    "        )\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(in_features=1024, out_features=num_classes),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "\n",
    "        x = self.globavgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed3282e",
   "metadata": {
    "papermill": {
     "duration": 0.004093,
     "end_time": "2024-12-15T18:27:49.731505",
     "exception": false,
     "start_time": "2024-12-15T18:27:49.727412",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Обучение модели и предсказание на тестовом наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af23245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath, model, optimizer=None, scheduler=None):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    \n",
    "    # Загрузка состояния модели\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Если требуется, загружаем состояние оптимизатора\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    # Если используется scheduler и он сохранён в чекпоинте, загружаем его состояние\n",
    "    if scheduler is not None and checkpoint.get('scheduler_state_dict') is not None:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    epoch = checkpoint.get('epoch', None)\n",
    "    train_loss = checkpoint.get('train_loss', None)\n",
    "    val_loss = checkpoint.get('val_loss', None)\n",
    "    \n",
    "    print(f\"Checkpoint loaded: Epoch {epoch}; Train loss: {train_loss}; Val loss: {val_loss}\")\n",
    "    \n",
    "    return model, optimizer, scheduler, epoch, train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50f4ef8d",
   "metadata": {
    "papermill": {
     "duration": 0.014455,
     "end_time": "2024-12-15T18:27:49.749958",
     "exception": false,
     "start_time": "2024-12-15T18:27:49.735503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def training_epoch(model, optimizer, criterion, train_loader, epoch, accumulation_steps):\n",
    "    train_loss = 0.0\n",
    "    train_accuracy = 0.0\n",
    "    all_train_preds = []\n",
    "    all_train_targets = []\n",
    "    model.train()\n",
    "\n",
    "    n_samples = 0\n",
    "    pb = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    loss_hist, accuracy_hist = [], []\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch_idx, ((orig_imgs, aug_imgs), labels) in pb:\n",
    "        # orig_imgs: (B, C, H, W)\n",
    "        # aug_imgs: (B, n, C, H, W) – n аугментированных изображений для каждого примера\n",
    "        orig_imgs = orig_imgs.to(device)\n",
    "        aug_imgs = aug_imgs.to(device)  # shape: (B, n, C, H, W)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Прямой проход для оригинальных изображений\n",
    "        logits_orig = model(orig_imgs)  # (B, num_classes)\n",
    "        \n",
    "        # Для аугментированных изображений: объединяем батч и n\n",
    "        B, n, C, H, W = aug_imgs.shape\n",
    "        aug_imgs_flat = aug_imgs.view(B * n, C, H, W)  # (B*n, C, H, W)\n",
    "        logits_aug_all = model(aug_imgs_flat)  # (B*n, num_classes)\n",
    "        # Восстанавливаем размерность и усредняем по n\n",
    "        logits_aug_all = logits_aug_all.view(B, n, -1)  # (B, n, num_classes)\n",
    "        logits_aug_avg = logits_aug_all.mean(dim=1)      # (B, num_classes)\n",
    "        \n",
    "        # Вычисляем потери для оригинала и усредненного аугмента, затем усредняем\n",
    "        loss_orig = criterion(logits_orig, labels)\n",
    "        loss_aug = criterion(logits_aug_avg, labels)\n",
    "        loss = (loss_orig + loss_aug) / 2\n",
    "        \n",
    "        # Scale loss for gradient accumulation\n",
    "        loss = loss / accumulation_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        # Accumulate gradients: update when we have processed accumulation_steps mini-batches\n",
    "        if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        train_loss += loss.item() * labels.size(0)\n",
    "        n_samples += labels.size(0)\n",
    "        \n",
    "        # Предсказания для оригинала\n",
    "        preds_orig = logits_orig.argmax(dim=1)\n",
    "        # Предсказания для аугментированных (усредненные)\n",
    "        preds_aug = logits_aug_avg.argmax(dim=1)\n",
    "        \n",
    "        # Считаем корректные предсказания и усредняем\n",
    "        correct_orig = (preds_orig == labels).sum().item()\n",
    "        correct_aug = (preds_aug == labels).sum().item()\n",
    "        train_accuracy += (correct_orig + correct_aug) / 2\n",
    "\n",
    "        pb.set_description(f'Training:      loss = [' + colored(f'{(train_loss / n_samples):.4f}', 'green') + '];    accuracy = [' + colored(f'{(train_accuracy / n_samples):.4f}', 'green') + ']')\n",
    "        loss_hist.append(loss.item())#train_loss / n_samples\n",
    "        accuracy_hist.append(((correct_orig + correct_aug) / 2) / labels.size(0))#train_accuracy / n_samples\n",
    "\n",
    "        # Для метрики F1 используем предсказания оригинала\n",
    "        all_train_preds.extend(preds_orig.cpu().numpy())\n",
    "        all_train_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "    np.array(loss_hist).dump(f'logs/train_loss_hist_epoch_{epoch}.npy')\n",
    "    print(f'[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_{epoch}.npy]')\n",
    "    np.array(accuracy_hist).dump(f'logs/train_accuracy_hist_epoch_{epoch}.npy')\n",
    "    print(f'[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_{epoch}.npy]')\n",
    "\n",
    "    n_samples = len(train_loader.dataset)\n",
    "    avg_loss = train_loss / n_samples\n",
    "    avg_acc = train_accuracy / n_samples\n",
    "    avg_f1 = f1_score(all_train_targets, all_train_preds, average='micro')\n",
    "    return avg_loss, avg_acc, avg_f1\n",
    "\n",
    "def validation_epoch(model, criterion, val_loader):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_accuracy = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        n_samples = 0\n",
    "        pb = tqdm(val_loader)\n",
    "\n",
    "        # Валидационный датасет возвращает (image, label)\n",
    "        for images, labels in pb:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item() * labels.size(0)\n",
    "            n_samples += labels.size(0)\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct = (preds == labels).sum().item()\n",
    "            val_accuracy += correct\n",
    "\n",
    "            pb.set_description(f'Validation:    loss = [' + colored(f'{(val_loss / n_samples):.4f}', 'green') + '];    accuracy = [' + colored(f'{(val_accuracy / n_samples):.4f}', 'green') + ']')\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "    \n",
    "    n_samples = len(val_loader.dataset)\n",
    "    avg_loss = val_loss / n_samples\n",
    "    avg_acc = val_accuracy / n_samples\n",
    "    # Вычисляем макро F1 score\n",
    "    avg_f1 = f1_score(all_targets, all_preds, average='micro')\n",
    "    return avg_loss, avg_acc, avg_f1\n",
    "\n",
    "def train(model, optimizer, criterion, train_loader, val_loader, accumulation_steps, num_epochs, init_epoch=1, save_period=4, scheduler=None):\n",
    "    best_model = None\n",
    "    best_val_loss = 1e+9\n",
    "    \n",
    "    for epoch in range(init_epoch, num_epochs + 1):\n",
    "        print(f'Epoch {epoch}/{num_epochs}:')\n",
    "        print(f'lr = {scheduler.get_last_lr()}')\n",
    "\n",
    "        train_loss, train_acc, train_f1 = training_epoch(\n",
    "            model, optimizer, criterion, train_loader, epoch, accumulation_steps\n",
    "        )\n",
    "        val_loss, val_acc, val_f1 = validation_epoch(\n",
    "            model, criterion, val_loader\n",
    "        )\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}')\n",
    "        print(f'  Val   Loss: {val_loss:.4f}, Val   Acc: {val_acc:.4f}, Val   F1: {val_f1:.4f}')\n",
    "        \n",
    "        # Сохраняем лучшую модель по валидации\n",
    "        if val_loss <= best_val_loss:\n",
    "            best_model = deepcopy(model)\n",
    "            best_val_loss = val_loss\n",
    "\n",
    "        # Бэкап модели на каждую эпоху\n",
    "        if epoch % save_period == 0:\n",
    "            backup_filename = f'checkpoints/model_epoch_{epoch}.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None,\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss\n",
    "            }, backup_filename)\n",
    "            print(f'[BACKUP] Backup saved in [{backup_filename}]')\n",
    "\n",
    "        print()\n",
    "            \n",
    "    return best_model\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, test_loader):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    for (orig_imgs, aug_imgs), labels in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        # orig_imgs: (B, C, H, W)\n",
    "        # aug_imgs: (B, n, C, H, W) – n аугментированных изображений для каждого примера\n",
    "        orig_imgs = orig_imgs.to(device)\n",
    "        aug_imgs = aug_imgs.to(device)  # shape: (B, n, C, H, W)\n",
    "        \n",
    "        # Прямой проход для оригинальных изображений\n",
    "        logits_orig = model(orig_imgs)\n",
    "        B, n, C, H, W = aug_imgs.shape\n",
    "        aug_imgs_flat = aug_imgs.view(B * n, C, H, W)  # (B*n, C, H, W)\n",
    "        logits_aug_all = model(aug_imgs_flat)  # (B*n, num_classes)\n",
    "        # Восстанавливаем размерность и усредняем по n\n",
    "        logits_aug_all = logits_aug_all.view(B, n, -1)  # (B, n, num_classes)\n",
    "        logits_aug_avg = logits_aug_all.mean(dim=1)      # (B, num_classes)\n",
    "        outputs = (logits_orig * 0.5 + logits_aug_avg * 0.5) / (0.5 + 0.5)\n",
    "\n",
    "\n",
    "        # images = images.to(device)\n",
    "        # outputs = model(images)\n",
    "        all_preds.append(outputs.argmax(dim=1).cpu().numpy())\n",
    "        all_labels.append(labels.numpy())\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    return all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d6e37cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv): Sequential(\n",
       "    (0): ConvBlock(\n",
       "      (layer): Sequential(\n",
       "        (0): Conv2d(3, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (4): Dropout2d(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): ResBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout2d(p=0.0, inplace=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (merge): Sequential(\n",
       "        (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (2): ResBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout2d(p=0.0, inplace=False)\n",
       "        (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (merge): Sequential(\n",
       "        (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (3): ConvBlock(\n",
       "      (layer): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (4): Dropout2d(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): ResBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout2d(p=0.0, inplace=False)\n",
       "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (merge): Sequential(\n",
       "        (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (5): ResBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout2d(p=0.0, inplace=False)\n",
       "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (merge): Sequential(\n",
       "        (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (6): ConvBlock(\n",
       "      (layer): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (4): Dropout2d(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): ResBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout2d(p=0.0, inplace=False)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (merge): Sequential(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (8): ResBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout2d(p=0.0, inplace=False)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (merge): Sequential(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (9): ConvBlock(\n",
       "      (layer): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (4): Dropout2d(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): ConvBlock(\n",
       "      (layer): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (4): Dropout2d(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): ConvBlock(\n",
       "      (layer): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (4): Dropout2d(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (12): ConvBlock(\n",
       "      (layer): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (4): Dropout2d(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (globavgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "    (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=42, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net(num_classes=len(train_dataset.classes)).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "396a0f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10,739,818 parameters\n"
     ]
    }
   ],
   "source": [
    "print(f'{np.sum([p.numel() for p in model.parameters()]):,} parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "111fa729",
   "metadata": {
    "papermill": {
     "duration": 1679.011367,
     "end_time": "2024-12-15T18:55:48.765355",
     "exception": false,
     "start_time": "2024-12-15T18:27:49.753988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma set to 0.6493816315762113\n",
      "gradient accumulation steps set to 32\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 56\n",
    "init_epoch = 1\n",
    "save_period = 4\n",
    "lr1 = 1e-3\n",
    "lr2 = 1e-3 * (1e-3 ** (7 / 8))#1e-6\n",
    "step_size = 4\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr1, amsgrad=True)#torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=4e-3)\n",
    "# T_max = num_epochs // 2\n",
    "gamma = (lr2 / lr1) ** (1 / (num_epochs // step_size))\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)#torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max)\n",
    "print(f'gamma set to {gamma}')\n",
    "print(f'gradient accumulation steps set to {accumulation_steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "501199ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3713737056616556e-06"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-3 * (1e-3 ** (7 / 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6750efe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if init_epoch > 1:\n",
    "    checkpoint_path = f'checkpoints/model_epoch_{init_epoch - 1}.pth'\n",
    "    model, optimizer, scheduler, epoch, train_loss, val_loss = load_checkpoint(\n",
    "        checkpoint_path, model, optimizer, scheduler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b4a8a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/56:\n",
      "lr = [0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:      loss = [\u001b[32m0.0849\u001b[0m];    accuracy = [\u001b[32m0.2943\u001b[0m]: 100%|██████████| 653/653 [00:49<00:00, 13.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_1.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_1.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m2.2199\u001b[0m];    accuracy = [\u001b[32m0.3810\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0849, Train Acc: 0.2943, Train F1: 0.3123\n",
      "  Val   Loss: 2.2199, Val   Acc: 0.3810, Val   F1: 0.3810\n",
      "\n",
      "Epoch 2/56:\n",
      "lr = [0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0412\u001b[0m];    accuracy = [\u001b[32m0.6501\u001b[0m]: 100%|██████████| 653/653 [00:49<00:00, 13.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_2.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_2.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m1.3975\u001b[0m];    accuracy = [\u001b[32m0.5238\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0412, Train Acc: 0.6501, Train F1: 0.6806\n",
      "  Val   Loss: 1.3975, Val   Acc: 0.5238, Val   F1: 0.5238\n",
      "\n",
      "Epoch 3/56:\n",
      "lr = [0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0228\u001b[0m];    accuracy = [\u001b[32m0.8005\u001b[0m]: 100%|██████████| 653/653 [00:51<00:00, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_3.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_3.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.9218\u001b[0m];    accuracy = [\u001b[32m0.7302\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0228, Train Acc: 0.8005, Train F1: 0.8264\n",
      "  Val   Loss: 0.9218, Val   Acc: 0.7302, Val   F1: 0.7302\n",
      "\n",
      "Epoch 4/56:\n",
      "lr = [0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0145\u001b[0m];    accuracy = [\u001b[32m0.8723\u001b[0m]: 100%|██████████| 653/653 [00:51<00:00, 12.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_4.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_4.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.5687\u001b[0m];    accuracy = [\u001b[32m0.7937\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0145, Train Acc: 0.8723, Train F1: 0.8975\n",
      "  Val   Loss: 0.5687, Val   Acc: 0.7937, Val   F1: 0.7937\n",
      "[BACKUP] Backup saved in [checkpoints/model_epoch_4.pth]\n",
      "\n",
      "Epoch 5/56:\n",
      "lr = [0.0006493816315762113]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:      loss = [\u001b[32m0.0100\u001b[0m];    accuracy = [\u001b[32m0.9135\u001b[0m]: 100%|██████████| 653/653 [00:49<00:00, 13.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_5.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_5.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.3784\u001b[0m];    accuracy = [\u001b[32m0.8730\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0100, Train Acc: 0.9135, Train F1: 0.9329\n",
      "  Val   Loss: 0.3784, Val   Acc: 0.8730, Val   F1: 0.8730\n",
      "\n",
      "Epoch 6/56:\n",
      "lr = [0.0006493816315762113]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0073\u001b[0m];    accuracy = [\u001b[32m0.9379\u001b[0m]: 100%|██████████| 653/653 [00:49<00:00, 13.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_6.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_6.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.1401\u001b[0m];    accuracy = [\u001b[32m0.9683\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0073, Train Acc: 0.9379, Train F1: 0.9537\n",
      "  Val   Loss: 0.1401, Val   Acc: 0.9683, Val   F1: 0.9683\n",
      "\n",
      "Epoch 7/56:\n",
      "lr = [0.0006493816315762113]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0060\u001b[0m];    accuracy = [\u001b[32m0.9474\u001b[0m]: 100%|██████████| 653/653 [00:49<00:00, 13.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_7.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_7.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.2654\u001b[0m];    accuracy = [\u001b[32m0.8730\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0060, Train Acc: 0.9474, Train F1: 0.9622\n",
      "  Val   Loss: 0.2654, Val   Acc: 0.8730, Val   F1: 0.8730\n",
      "\n",
      "Epoch 8/56:\n",
      "lr = [0.0006493816315762113]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0055\u001b[0m];    accuracy = [\u001b[32m0.9518\u001b[0m]: 100%|██████████| 653/653 [00:49<00:00, 13.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_8.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_8.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.1443\u001b[0m];    accuracy = [\u001b[32m0.9524\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0055, Train Acc: 0.9518, Train F1: 0.9672\n",
      "  Val   Loss: 0.1443, Val   Acc: 0.9524, Val   F1: 0.9524\n",
      "[BACKUP] Backup saved in [checkpoints/model_epoch_8.pth]\n",
      "\n",
      "Epoch 9/56:\n",
      "lr = [0.0004216965034285823]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:      loss = [\u001b[32m0.0045\u001b[0m];    accuracy = [\u001b[32m0.9609\u001b[0m]: 100%|██████████| 653/653 [00:49<00:00, 13.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_9.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_9.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.2070\u001b[0m];    accuracy = [\u001b[32m0.9524\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0045, Train Acc: 0.9609, Train F1: 0.9750\n",
      "  Val   Loss: 0.2070, Val   Acc: 0.9524, Val   F1: 0.9524\n",
      "\n",
      "Epoch 10/56:\n",
      "lr = [0.0004216965034285823]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0035\u001b[0m];    accuracy = [\u001b[32m0.9695\u001b[0m]: 100%|██████████| 653/653 [00:47<00:00, 13.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_10.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_10.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.1603\u001b[0m];    accuracy = [\u001b[32m0.9365\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0035, Train Acc: 0.9695, Train F1: 0.9806\n",
      "  Val   Loss: 0.1603, Val   Acc: 0.9365, Val   F1: 0.9365\n",
      "\n",
      "Epoch 11/56:\n",
      "lr = [0.0004216965034285823]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0031\u001b[0m];    accuracy = [\u001b[32m0.9739\u001b[0m]: 100%|██████████| 653/653 [00:47<00:00, 13.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_11.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_11.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0541\u001b[0m];    accuracy = [\u001b[32m0.9841\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0031, Train Acc: 0.9739, Train F1: 0.9833\n",
      "  Val   Loss: 0.0541, Val   Acc: 0.9841, Val   F1: 0.9841\n",
      "\n",
      "Epoch 12/56:\n",
      "lr = [0.0004216965034285823]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0027\u001b[0m];    accuracy = [\u001b[32m0.9772\u001b[0m]: 100%|██████████| 653/653 [00:48<00:00, 13.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_12.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_12.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.1438\u001b[0m];    accuracy = [\u001b[32m0.9524\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0027, Train Acc: 0.9772, Train F1: 0.9872\n",
      "  Val   Loss: 0.1438, Val   Acc: 0.9524, Val   F1: 0.9524\n",
      "[BACKUP] Backup saved in [checkpoints/model_epoch_12.pth]\n",
      "\n",
      "Epoch 13/56:\n",
      "lr = [0.00027384196342643616]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:      loss = [\u001b[32m0.0024\u001b[0m];    accuracy = [\u001b[32m0.9801\u001b[0m]: 100%|██████████| 653/653 [00:47<00:00, 13.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_13.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_13.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0615\u001b[0m];    accuracy = [\u001b[32m0.9841\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0024, Train Acc: 0.9801, Train F1: 0.9895\n",
      "  Val   Loss: 0.0615, Val   Acc: 0.9841, Val   F1: 0.9841\n",
      "\n",
      "Epoch 14/56:\n",
      "lr = [0.00027384196342643616]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0022\u001b[0m];    accuracy = [\u001b[32m0.9825\u001b[0m]: 100%|██████████| 653/653 [00:47<00:00, 13.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_14.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_14.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.1029\u001b[0m];    accuracy = [\u001b[32m0.9524\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  6.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0022, Train Acc: 0.9825, Train F1: 0.9910\n",
      "  Val   Loss: 0.1029, Val   Acc: 0.9524, Val   F1: 0.9524\n",
      "\n",
      "Epoch 15/56:\n",
      "lr = [0.00027384196342643616]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0018\u001b[0m];    accuracy = [\u001b[32m0.9856\u001b[0m]: 100%|██████████| 653/653 [00:48<00:00, 13.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_15.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_15.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0988\u001b[0m];    accuracy = [\u001b[32m0.9683\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  6.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0018, Train Acc: 0.9856, Train F1: 0.9936\n",
      "  Val   Loss: 0.0988, Val   Acc: 0.9683, Val   F1: 0.9683\n",
      "\n",
      "Epoch 16/56:\n",
      "lr = [0.00027384196342643616]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0015\u001b[0m];    accuracy = [\u001b[32m0.9879\u001b[0m]: 100%|██████████| 653/653 [00:47<00:00, 13.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_16.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_16.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.1274\u001b[0m];    accuracy = [\u001b[32m0.9841\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0015, Train Acc: 0.9879, Train F1: 0.9947\n",
      "  Val   Loss: 0.1274, Val   Acc: 0.9841, Val   F1: 0.9841\n",
      "[BACKUP] Backup saved in [checkpoints/model_epoch_16.pth]\n",
      "\n",
      "Epoch 17/56:\n",
      "lr = [0.0001778279410038923]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:      loss = [\u001b[32m0.0015\u001b[0m];    accuracy = [\u001b[32m0.9879\u001b[0m]: 100%|██████████| 653/653 [00:47<00:00, 13.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_17.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_17.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0873\u001b[0m];    accuracy = [\u001b[32m0.9841\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0015, Train Acc: 0.9879, Train F1: 0.9946\n",
      "  Val   Loss: 0.0873, Val   Acc: 0.9841, Val   F1: 0.9841\n",
      "\n",
      "Epoch 18/56:\n",
      "lr = [0.0001778279410038923]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0012\u001b[0m];    accuracy = [\u001b[32m0.9905\u001b[0m]: 100%|██████████| 653/653 [00:47<00:00, 13.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_18.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_18.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0682\u001b[0m];    accuracy = [\u001b[32m0.9841\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0012, Train Acc: 0.9905, Train F1: 0.9966\n",
      "  Val   Loss: 0.0682, Val   Acc: 0.9841, Val   F1: 0.9841\n",
      "\n",
      "Epoch 19/56:\n",
      "lr = [0.0001778279410038923]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0013\u001b[0m];    accuracy = [\u001b[32m0.9901\u001b[0m]: 100%|██████████| 653/653 [00:47<00:00, 13.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_19.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_19.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0539\u001b[0m];    accuracy = [\u001b[32m0.9841\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0013, Train Acc: 0.9901, Train F1: 0.9959\n",
      "  Val   Loss: 0.0539, Val   Acc: 0.9841, Val   F1: 0.9841\n",
      "\n",
      "Epoch 20/56:\n",
      "lr = [0.0001778279410038923]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0010\u001b[0m];    accuracy = [\u001b[32m0.9918\u001b[0m]: 100%|██████████| 653/653 [00:50<00:00, 12.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_20.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_20.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0775\u001b[0m];    accuracy = [\u001b[32m0.9841\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0010, Train Acc: 0.9918, Train F1: 0.9970\n",
      "  Val   Loss: 0.0775, Val   Acc: 0.9841, Val   F1: 0.9841\n",
      "[BACKUP] Backup saved in [checkpoints/model_epoch_20.pth]\n",
      "\n",
      "Epoch 21/56:\n",
      "lr = [0.00011547819846894584]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:      loss = [\u001b[32m0.0010\u001b[0m];    accuracy = [\u001b[32m0.9923\u001b[0m]: 100%|██████████| 653/653 [00:51<00:00, 12.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_21.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_21.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0630\u001b[0m];    accuracy = [\u001b[32m0.9841\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0010, Train Acc: 0.9923, Train F1: 0.9975\n",
      "  Val   Loss: 0.0630, Val   Acc: 0.9841, Val   F1: 0.9841\n",
      "\n",
      "Epoch 22/56:\n",
      "lr = [0.00011547819846894584]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0009\u001b[0m];    accuracy = [\u001b[32m0.9935\u001b[0m]: 100%|██████████| 653/653 [00:51<00:00, 12.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_22.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_22.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0702\u001b[0m];    accuracy = [\u001b[32m0.9841\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0009, Train Acc: 0.9935, Train F1: 0.9979\n",
      "  Val   Loss: 0.0702, Val   Acc: 0.9841, Val   F1: 0.9841\n",
      "\n",
      "Epoch 23/56:\n",
      "lr = [0.00011547819846894584]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0008\u001b[0m];    accuracy = [\u001b[32m0.9933\u001b[0m]: 100%|██████████| 653/653 [00:51<00:00, 12.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_23.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_23.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0434\u001b[0m];    accuracy = [\u001b[32m0.9841\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0008, Train Acc: 0.9933, Train F1: 0.9977\n",
      "  Val   Loss: 0.0434, Val   Acc: 0.9841, Val   F1: 0.9841\n",
      "\n",
      "Epoch 24/56:\n",
      "lr = [0.00011547819846894584]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0008\u001b[0m];    accuracy = [\u001b[32m0.9939\u001b[0m]: 100%|██████████| 653/653 [00:50<00:00, 12.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_24.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_24.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0473\u001b[0m];    accuracy = [\u001b[32m0.9841\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0008, Train Acc: 0.9939, Train F1: 0.9983\n",
      "  Val   Loss: 0.0473, Val   Acc: 0.9841, Val   F1: 0.9841\n",
      "[BACKUP] Backup saved in [checkpoints/model_epoch_24.pth]\n",
      "\n",
      "Epoch 25/56:\n",
      "lr = [7.49894209332456e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:      loss = [\u001b[32m0.0008\u001b[0m];    accuracy = [\u001b[32m0.9936\u001b[0m]: 100%|██████████| 653/653 [00:50<00:00, 12.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_25.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_25.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0571\u001b[0m];    accuracy = [\u001b[32m0.9683\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0008, Train Acc: 0.9936, Train F1: 0.9979\n",
      "  Val   Loss: 0.0571, Val   Acc: 0.9683, Val   F1: 0.9683\n",
      "\n",
      "Epoch 26/56:\n",
      "lr = [7.49894209332456e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0008\u001b[0m];    accuracy = [\u001b[32m0.9939\u001b[0m]: 100%|██████████| 653/653 [00:51<00:00, 12.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_26.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_26.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0328\u001b[0m];    accuracy = [\u001b[32m0.9841\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  5.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0008, Train Acc: 0.9939, Train F1: 0.9980\n",
      "  Val   Loss: 0.0328, Val   Acc: 0.9841, Val   F1: 0.9841\n",
      "\n",
      "Epoch 27/56:\n",
      "lr = [7.49894209332456e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0007\u001b[0m];    accuracy = [\u001b[32m0.9947\u001b[0m]: 100%|██████████| 653/653 [00:51<00:00, 12.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_27.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_27.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0709\u001b[0m];    accuracy = [\u001b[32m0.9683\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0007, Train Acc: 0.9947, Train F1: 0.9988\n",
      "  Val   Loss: 0.0709, Val   Acc: 0.9683, Val   F1: 0.9683\n",
      "\n",
      "Epoch 28/56:\n",
      "lr = [7.49894209332456e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0007\u001b[0m];    accuracy = [\u001b[32m0.9948\u001b[0m]: 100%|██████████| 653/653 [00:51<00:00, 12.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_28.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_28.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0621\u001b[0m];    accuracy = [\u001b[32m0.9841\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0007, Train Acc: 0.9948, Train F1: 0.9986\n",
      "  Val   Loss: 0.0621, Val   Acc: 0.9841, Val   F1: 0.9841\n",
      "[BACKUP] Backup saved in [checkpoints/model_epoch_28.pth]\n",
      "\n",
      "Epoch 29/56:\n",
      "lr = [4.869675251658632e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:      loss = [\u001b[32m0.0007\u001b[0m];    accuracy = [\u001b[32m0.9948\u001b[0m]: 100%|██████████| 653/653 [00:50<00:00, 12.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_29.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_29.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0619\u001b[0m];    accuracy = [\u001b[32m0.9683\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0007, Train Acc: 0.9948, Train F1: 0.9988\n",
      "  Val   Loss: 0.0619, Val   Acc: 0.9683, Val   F1: 0.9683\n",
      "\n",
      "Epoch 30/56:\n",
      "lr = [4.869675251658632e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0006\u001b[0m];    accuracy = [\u001b[32m0.9954\u001b[0m]: 100%|██████████| 653/653 [00:51<00:00, 12.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_30.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_30.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0687\u001b[0m];    accuracy = [\u001b[32m0.9683\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0006, Train Acc: 0.9954, Train F1: 0.9993\n",
      "  Val   Loss: 0.0687, Val   Acc: 0.9683, Val   F1: 0.9683\n",
      "\n",
      "Epoch 31/56:\n",
      "lr = [4.869675251658632e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0006\u001b[0m];    accuracy = [\u001b[32m0.9957\u001b[0m]: 100%|██████████| 653/653 [00:50<00:00, 12.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_31.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_31.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0501\u001b[0m];    accuracy = [\u001b[32m0.9683\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0006, Train Acc: 0.9957, Train F1: 0.9992\n",
      "  Val   Loss: 0.0501, Val   Acc: 0.9683, Val   F1: 0.9683\n",
      "\n",
      "Epoch 32/56:\n",
      "lr = [4.869675251658632e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0006\u001b[0m];    accuracy = [\u001b[32m0.9954\u001b[0m]: 100%|██████████| 653/653 [00:51<00:00, 12.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_32.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_32.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0560\u001b[0m];    accuracy = [\u001b[32m0.9841\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0006, Train Acc: 0.9954, Train F1: 0.9990\n",
      "  Val   Loss: 0.0560, Val   Acc: 0.9841, Val   F1: 0.9841\n",
      "[BACKUP] Backup saved in [checkpoints/model_epoch_32.pth]\n",
      "\n",
      "Epoch 33/56:\n",
      "lr = [3.1622776601683795e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:      loss = [\u001b[32m0.0006\u001b[0m];    accuracy = [\u001b[32m0.9961\u001b[0m]: 100%|██████████| 653/653 [00:50<00:00, 12.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_33.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_33.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0607\u001b[0m];    accuracy = [\u001b[32m0.9683\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0006, Train Acc: 0.9961, Train F1: 0.9993\n",
      "  Val   Loss: 0.0607, Val   Acc: 0.9683, Val   F1: 0.9683\n",
      "\n",
      "Epoch 34/56:\n",
      "lr = [3.1622776601683795e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0005\u001b[0m];    accuracy = [\u001b[32m0.9968\u001b[0m]: 100%|██████████| 653/653 [00:51<00:00, 12.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_34.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_34.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0722\u001b[0m];    accuracy = [\u001b[32m0.9683\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0005, Train Acc: 0.9968, Train F1: 0.9994\n",
      "  Val   Loss: 0.0722, Val   Acc: 0.9683, Val   F1: 0.9683\n",
      "\n",
      "Epoch 35/56:\n",
      "lr = [3.1622776601683795e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0005\u001b[0m];    accuracy = [\u001b[32m0.9963\u001b[0m]: 100%|██████████| 653/653 [00:50<00:00, 12.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_35.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_35.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0562\u001b[0m];    accuracy = [\u001b[32m0.9683\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0005, Train Acc: 0.9963, Train F1: 0.9994\n",
      "  Val   Loss: 0.0562, Val   Acc: 0.9683, Val   F1: 0.9683\n",
      "\n",
      "Epoch 36/56:\n",
      "lr = [3.1622776601683795e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0006\u001b[0m];    accuracy = [\u001b[32m0.9957\u001b[0m]: 100%|██████████| 653/653 [00:51<00:00, 12.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_36.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_36.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0617\u001b[0m];    accuracy = [\u001b[32m0.9841\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0006, Train Acc: 0.9957, Train F1: 0.9992\n",
      "  Val   Loss: 0.0617, Val   Acc: 0.9841, Val   F1: 0.9841\n",
      "[BACKUP] Backup saved in [checkpoints/model_epoch_36.pth]\n",
      "\n",
      "Epoch 37/56:\n",
      "lr = [2.0535250264571462e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:      loss = [\u001b[32m0.0005\u001b[0m];    accuracy = [\u001b[32m0.9965\u001b[0m]: 100%|██████████| 653/653 [00:50<00:00, 12.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_37.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_37.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0628\u001b[0m];    accuracy = [\u001b[32m0.9683\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0005, Train Acc: 0.9965, Train F1: 0.9995\n",
      "  Val   Loss: 0.0628, Val   Acc: 0.9683, Val   F1: 0.9683\n",
      "\n",
      "Epoch 38/56:\n",
      "lr = [2.0535250264571462e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0006\u001b[0m];    accuracy = [\u001b[32m0.9960\u001b[0m]: 100%|██████████| 653/653 [00:51<00:00, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_38.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_38.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0475\u001b[0m];    accuracy = [\u001b[32m0.9841\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  5.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0006, Train Acc: 0.9960, Train F1: 0.9991\n",
      "  Val   Loss: 0.0475, Val   Acc: 0.9841, Val   F1: 0.9841\n",
      "\n",
      "Epoch 39/56:\n",
      "lr = [2.0535250264571462e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0005\u001b[0m];    accuracy = [\u001b[32m0.9964\u001b[0m]: 100%|██████████| 653/653 [00:50<00:00, 12.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_39.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_39.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0332\u001b[0m];    accuracy = [\u001b[32m1.0000\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0005, Train Acc: 0.9964, Train F1: 0.9996\n",
      "  Val   Loss: 0.0332, Val   Acc: 1.0000, Val   F1: 1.0000\n",
      "\n",
      "Epoch 40/56:\n",
      "lr = [2.0535250264571462e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0005\u001b[0m];    accuracy = [\u001b[32m0.9963\u001b[0m]: 100%|██████████| 653/653 [00:51<00:00, 12.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_40.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_40.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0525\u001b[0m];    accuracy = [\u001b[32m0.9841\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0005, Train Acc: 0.9963, Train F1: 0.9995\n",
      "  Val   Loss: 0.0525, Val   Acc: 0.9841, Val   F1: 0.9841\n",
      "[BACKUP] Backup saved in [checkpoints/model_epoch_40.pth]\n",
      "\n",
      "Epoch 41/56:\n",
      "lr = [1.333521432163324e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:      loss = [\u001b[32m0.0005\u001b[0m];    accuracy = [\u001b[32m0.9963\u001b[0m]: 100%|██████████| 653/653 [00:50<00:00, 12.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_41.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_41.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0509\u001b[0m];    accuracy = [\u001b[32m0.9683\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0005, Train Acc: 0.9963, Train F1: 0.9993\n",
      "  Val   Loss: 0.0509, Val   Acc: 0.9683, Val   F1: 0.9683\n",
      "\n",
      "Epoch 42/56:\n",
      "lr = [1.333521432163324e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0005\u001b[0m];    accuracy = [\u001b[32m0.9963\u001b[0m]: 100%|██████████| 653/653 [00:50<00:00, 12.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_42.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_42.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0370\u001b[0m];    accuracy = [\u001b[32m0.9841\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  6.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0005, Train Acc: 0.9963, Train F1: 0.9995\n",
      "  Val   Loss: 0.0370, Val   Acc: 0.9841, Val   F1: 0.9841\n",
      "\n",
      "Epoch 43/56:\n",
      "lr = [1.333521432163324e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0004\u001b[0m];    accuracy = [\u001b[32m0.9967\u001b[0m]: 100%|██████████| 653/653 [00:50<00:00, 12.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_43.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_43.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0557\u001b[0m];    accuracy = [\u001b[32m0.9683\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0004, Train Acc: 0.9967, Train F1: 0.9996\n",
      "  Val   Loss: 0.0557, Val   Acc: 0.9683, Val   F1: 0.9683\n",
      "\n",
      "Epoch 44/56:\n",
      "lr = [1.333521432163324e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0005\u001b[0m];    accuracy = [\u001b[32m0.9969\u001b[0m]: 100%|██████████| 653/653 [00:51<00:00, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_44.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_44.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0550\u001b[0m];    accuracy = [\u001b[32m0.9841\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0005, Train Acc: 0.9969, Train F1: 0.9995\n",
      "  Val   Loss: 0.0550, Val   Acc: 0.9841, Val   F1: 0.9841\n",
      "[BACKUP] Backup saved in [checkpoints/model_epoch_44.pth]\n",
      "\n",
      "Epoch 45/56:\n",
      "lr = [8.659643233600654e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:      loss = [\u001b[32m0.0005\u001b[0m];    accuracy = [\u001b[32m0.9967\u001b[0m]: 100%|██████████| 653/653 [00:51<00:00, 12.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_45.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_45.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0370\u001b[0m];    accuracy = [\u001b[32m0.9841\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0005, Train Acc: 0.9967, Train F1: 0.9996\n",
      "  Val   Loss: 0.0370, Val   Acc: 0.9841, Val   F1: 0.9841\n",
      "\n",
      "Epoch 46/56:\n",
      "lr = [8.659643233600654e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0005\u001b[0m];    accuracy = [\u001b[32m0.9966\u001b[0m]: 100%|██████████| 653/653 [00:50<00:00, 12.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_46.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_46.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0415\u001b[0m];    accuracy = [\u001b[32m0.9841\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0005, Train Acc: 0.9966, Train F1: 0.9993\n",
      "  Val   Loss: 0.0415, Val   Acc: 0.9841, Val   F1: 0.9841\n",
      "\n",
      "Epoch 47/56:\n",
      "lr = [8.659643233600654e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0005\u001b[0m];    accuracy = [\u001b[32m0.9965\u001b[0m]: 100%|██████████| 653/653 [00:51<00:00, 12.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_47.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_47.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0432\u001b[0m];    accuracy = [\u001b[32m0.9841\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0005, Train Acc: 0.9965, Train F1: 0.9995\n",
      "  Val   Loss: 0.0432, Val   Acc: 0.9841, Val   F1: 0.9841\n",
      "\n",
      "Epoch 48/56:\n",
      "lr = [8.659643233600654e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0005\u001b[0m];    accuracy = [\u001b[32m0.9965\u001b[0m]: 100%|██████████| 653/653 [00:51<00:00, 12.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_48.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_48.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0847\u001b[0m];    accuracy = [\u001b[32m0.9524\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0005, Train Acc: 0.9965, Train F1: 0.9994\n",
      "  Val   Loss: 0.0847, Val   Acc: 0.9524, Val   F1: 0.9524\n",
      "[BACKUP] Backup saved in [checkpoints/model_epoch_48.pth]\n",
      "\n",
      "Epoch 49/56:\n",
      "lr = [5.623413251903491e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:      loss = [\u001b[32m0.0005\u001b[0m];    accuracy = [\u001b[32m0.9967\u001b[0m]: 100%|██████████| 653/653 [00:50<00:00, 12.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_49.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_49.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0373\u001b[0m];    accuracy = [\u001b[32m1.0000\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0005, Train Acc: 0.9967, Train F1: 0.9994\n",
      "  Val   Loss: 0.0373, Val   Acc: 1.0000, Val   F1: 1.0000\n",
      "\n",
      "Epoch 50/56:\n",
      "lr = [5.623413251903491e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0005\u001b[0m];    accuracy = [\u001b[32m0.9966\u001b[0m]: 100%|██████████| 653/653 [00:51<00:00, 12.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_50.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_50.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0736\u001b[0m];    accuracy = [\u001b[32m0.9524\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0005, Train Acc: 0.9966, Train F1: 0.9997\n",
      "  Val   Loss: 0.0736, Val   Acc: 0.9524, Val   F1: 0.9524\n",
      "\n",
      "Epoch 51/56:\n",
      "lr = [5.623413251903491e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0005\u001b[0m];    accuracy = [\u001b[32m0.9969\u001b[0m]: 100%|██████████| 653/653 [00:50<00:00, 12.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_51.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_51.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0663\u001b[0m];    accuracy = [\u001b[32m0.9841\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0005, Train Acc: 0.9969, Train F1: 0.9996\n",
      "  Val   Loss: 0.0663, Val   Acc: 0.9841, Val   F1: 0.9841\n",
      "\n",
      "Epoch 52/56:\n",
      "lr = [5.623413251903491e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0004\u001b[0m];    accuracy = [\u001b[32m0.9970\u001b[0m]: 100%|██████████| 653/653 [00:51<00:00, 12.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_52.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_52.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0646\u001b[0m];    accuracy = [\u001b[32m0.9683\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0004, Train Acc: 0.9970, Train F1: 0.9997\n",
      "  Val   Loss: 0.0646, Val   Acc: 0.9683, Val   F1: 0.9683\n",
      "[BACKUP] Backup saved in [checkpoints/model_epoch_52.pth]\n",
      "\n",
      "Epoch 53/56:\n",
      "lr = [3.6517412725483775e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:      loss = [\u001b[32m0.0005\u001b[0m];    accuracy = [\u001b[32m0.9968\u001b[0m]: 100%|██████████| 653/653 [00:51<00:00, 12.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_53.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_53.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0717\u001b[0m];    accuracy = [\u001b[32m0.9841\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0005, Train Acc: 0.9968, Train F1: 0.9993\n",
      "  Val   Loss: 0.0717, Val   Acc: 0.9841, Val   F1: 0.9841\n",
      "\n",
      "Epoch 54/56:\n",
      "lr = [3.6517412725483775e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0005\u001b[0m];    accuracy = [\u001b[32m0.9967\u001b[0m]: 100%|██████████| 653/653 [00:51<00:00, 12.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_54.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_54.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0632\u001b[0m];    accuracy = [\u001b[32m0.9683\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  4.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0005, Train Acc: 0.9967, Train F1: 0.9994\n",
      "  Val   Loss: 0.0632, Val   Acc: 0.9683, Val   F1: 0.9683\n",
      "\n",
      "Epoch 55/56:\n",
      "lr = [3.6517412725483775e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0004\u001b[0m];    accuracy = [\u001b[32m0.9971\u001b[0m]: 100%|██████████| 653/653 [00:50<00:00, 12.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_55.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_55.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0465\u001b[0m];    accuracy = [\u001b[32m0.9841\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0004, Train Acc: 0.9971, Train F1: 0.9999\n",
      "  Val   Loss: 0.0465, Val   Acc: 0.9841, Val   F1: 0.9841\n",
      "\n",
      "Epoch 56/56:\n",
      "lr = [3.6517412725483775e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:      loss = [\u001b[32m0.0005\u001b[0m];    accuracy = [\u001b[32m0.9967\u001b[0m]: 100%|██████████| 653/653 [00:51<00:00, 12.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BACKUP] Train loss hist data saved in [logs/train_loss_hist_epoch_56.npy]\n",
      "[BACKUP] Train accuracy hist data saved in [logs/train_accuracy_hist_epoch_56.npy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:    loss = [\u001b[32m0.0749\u001b[0m];    accuracy = [\u001b[32m0.9683\u001b[0m]: 100%|██████████| 2/2 [00:00<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.0005, Train Acc: 0.9967, Train F1: 0.9993\n",
      "  Val   Loss: 0.0749, Val   Acc: 0.9683, Val   F1: 0.9683\n",
      "[BACKUP] Backup saved in [checkpoints/model_epoch_56.pth]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train gives best model but I'll use usual model later\n",
    "best_model = train(model, optimizer, criterion, train_dataloader, val_dataloader, accumulation_steps, num_epochs, init_epoch, save_period, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491d82b1",
   "metadata": {
    "papermill": {
     "duration": 0.08974,
     "end_time": "2024-12-15T18:55:48.946035",
     "exception": false,
     "start_time": "2024-12-15T18:55:48.856295",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " ### Подготовка предсказаний для тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3f2082b",
   "metadata": {
    "papermill": {
     "duration": 3.050506,
     "end_time": "2024-12-15T18:55:52.317456",
     "exception": false,
     "start_time": "2024-12-15T18:55:49.266950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 31/31 [00:01<00:00, 27.17it/s]\n"
     ]
    }
   ],
   "source": [
    "preds, _ = predict(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "985ccdff",
   "metadata": {
    "papermill": {
     "duration": 0.112009,
     "end_time": "2024-12-15T18:55:52.518632",
     "exception": false,
     "start_time": "2024-12-15T18:55:52.406623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEST_DIR = Path('data/testset/')\n",
    "test_files = sorted(list(TEST_DIR.rglob('*.jpg')))\n",
    "file_names = [file.name for file in test_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53134346",
   "metadata": {
    "papermill": {
     "duration": 0.095811,
     "end_time": "2024-12-15T18:55:52.701726",
     "exception": false,
     "start_time": "2024-12-15T18:55:52.605915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx_to_class = {l: n for n, l in train_dataset.class_to_idx.items()}\n",
    "preds_class_names = [idx_to_class[pred] for pred in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e528998",
   "metadata": {
    "papermill": {
     "duration": 0.110228,
     "end_time": "2024-12-15T18:55:52.900748",
     "exception": false,
     "start_time": "2024-12-15T18:55:52.790520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Expected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img0.jpg</td>\n",
       "      <td>nelson_muntz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img1.jpg</td>\n",
       "      <td>bart_simpson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img10.jpg</td>\n",
       "      <td>ned_flanders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img100.jpg</td>\n",
       "      <td>chief_wiggum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img101.jpg</td>\n",
       "      <td>apu_nahasapeemapetilon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Id                Expected\n",
       "0    img0.jpg            nelson_muntz\n",
       "1    img1.jpg            bart_simpson\n",
       "2   img10.jpg            ned_flanders\n",
       "3  img100.jpg            chief_wiggum\n",
       "4  img101.jpg  apu_nahasapeemapetilon"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_submit = pd.DataFrame({'Id': file_names, 'Expected': preds_class_names})\n",
    "my_submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4917f4f5",
   "metadata": {
    "papermill": {
     "duration": 0.098059,
     "end_time": "2024-12-15T18:55:53.087878",
     "exception": false,
     "start_time": "2024-12-15T18:55:52.989819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_submit.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37d794e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Expected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>img488.jpg</td>\n",
       "      <td>edna_krabappel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id        Expected\n",
       "432  img488.jpg  edna_krabappel"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_submit[my_submit['Id'] == 'img488.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "683c6835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gotheartem/Projects/ioai-hw/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 42 artists>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzoAAAGsCAYAAAAVEdLDAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAHiFJREFUeJzt3XFsnPV5wPHHTrANJDYJKXYTTE0LKooQdmtj15s6uuHV27K2dJuUVdVseV3+aUCw06Yl2rDXsskpYTSFRKRjyyq1YomYmlVturDMg0xV3YY6ikZZG61TQ9zScxJt2KkZDvK9+4P1gpsYfI6TM798PtIr4dfve/ec+SXoy3t+ryLLsiwAAAASUlnuAQAAABaa0AEAAJIjdAAAgOQIHQAAIDlCBwAASI7QAQAAkiN0AACA5Cwt9wBzUSgU4sUXX4zly5dHRUVFuccBAADKJMuyOH36dKxevToqK2e/bvOWCJ0XX3wxGhsbyz0GAACwSIyOjsb1118/6/ffEqGzfPnyiHjtxdTW1pZ5GgAAoFwmJiaisbGx2AizeUuEzs/erlZbWyt0AACAN/2VFjcjAAAAkiN0AACA5AgdAAAgOUIHAABIjtABAACSI3QAAIDkCB0AACA5QgcAAEiO0AEAAJIjdAAAgOQIHQAAIDlCBwAASI7QAQAAkiN0AACA5AgdAAAgOUIHAABIztJyDwAsPk2b9s3rvGNb1i3wJAAA8+OKDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMmZV+js2LEjmpqaoqamJjo6OuLQoUOzHvuFL3whKioqZmw1NTXzHhgAAODNlBw6e/bsiVwuFwMDA3H48OFobm6O7u7uOHHixKzn1NbWxk9+8pPi9sILL1zQ0AAAAG+k5NB5+OGHY8OGDdHX1xdr166NnTt3xlVXXRW7du2a9ZyKiopoaGgobvX19Rc0NAAAwBspKXTOnDkTIyMj0dXVdfYBKiujq6srhoeHZz3vpz/9abzjHe+IxsbG+MhHPhLPP//8Gz7P1NRUTExMzNgAAADmqqTQOXXqVExPT59zRaa+vj7y+fx5z3n3u98du3btiq985SvxpS99KQqFQvzCL/xC/OhHP5r1eQYHB6Ourq64NTY2ljImAABwmbvod13r7OyMnp6eaGlpiTvuuCO+/OUvx9ve9rb4/Oc/P+s5mzdvjvHx8eI2Ojp6sccEAAASsrSUg1etWhVLliyJsbGxGfvHxsaioaFhTo9xxRVXxHve8574wQ9+MOsx1dXVUV1dXcpoAAAARSVd0amqqorW1tYYGhoq7isUCjE0NBSdnZ1zeozp6el47rnn4u1vf3tpkwIAAMxRSVd0IiJyuVz09vZGW1tbtLe3x7Zt22JycjL6+voiIqKnpyfWrFkTg4ODERHx6U9/Ot73vvfFTTfdFC+99FJs3bo1XnjhhfiDP/iDhX0lAAAA/6/k0Fm/fn2cPHky+vv7I5/PR0tLS+zfv794g4Ljx49HZeXZC0X/8z//Exs2bIh8Ph8rVqyI1tbW+OY3vxlr165duFcBAADwOhVZlmXlHuLNTExMRF1dXYyPj0dtbW25x4HkNW3aN6/zjm1Zt8CTAADMNNc2uOh3XQMAALjUhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcuYVOjt27IimpqaoqamJjo6OOHTo0JzO2717d1RUVMRdd901n6cFAACYk5JDZ8+ePZHL5WJgYCAOHz4czc3N0d3dHSdOnHjD844dOxZ/9Ed/FO9///vnPSwAAMBclBw6Dz/8cGzYsCH6+vpi7dq1sXPnzrjqqqti165ds54zPT0dH//4x+NTn/pUvPOd73zT55iamoqJiYkZGwAAwFyVFDpnzpyJkZGR6OrqOvsAlZXR1dUVw8PDs5736U9/Oq677rr4xCc+MafnGRwcjLq6uuLW2NhYypgAAMBlrqTQOXXqVExPT0d9ff2M/fX19ZHP5897zje+8Y3427/923j88cfn/DybN2+O8fHx4jY6OlrKmAAAwGVu6cV88NOnT8fv/d7vxeOPPx6rVq2a83nV1dVRXV19EScDAABSVlLorFq1KpYsWRJjY2Mz9o+NjUVDQ8M5x//Xf/1XHDt2LD70oQ8V9xUKhdeeeOnSOHr0aLzrXe+az9wAAACzKumta1VVVdHa2hpDQ0PFfYVCIYaGhqKzs/Oc42+55ZZ47rnn4siRI8Xtwx/+cPzyL/9yHDlyxO/eAAAAF0XJb13L5XLR29sbbW1t0d7eHtu2bYvJycno6+uLiIienp5Ys2ZNDA4ORk1NTdx6660zzr/mmmsiIs7ZDwAAsFBKDp3169fHyZMno7+/P/L5fLS0tMT+/fuLNyg4fvx4VFbO63NIAQAAFkRFlmVZuYd4MxMTE1FXVxfj4+NRW1tb7nEgeU2b9s3rvGNb1i3wJAAAM821DVx6AQAAkiN0AACA5AgdAAAgOUIHAABIjtABAACSI3QAAIDkCB0AACA5QgcAAEiO0AEAAJIjdAAAgOQIHQAAIDlCBwAASI7QAQAAkiN0AACA5AgdAAAgOUIHAABIjtABAACSI3QAAIDkCB0AACA5QgcAAEiO0AEAAJIjdAAAgOQIHQAAIDlCBwAASI7QAQAAkiN0AACA5AgdAAAgOUIHAABIjtABAACSI3QAAIDkCB0AACA5QgcAAEiO0AEAAJIjdAAAgOQIHQAAIDlCBwAASI7QAQAAkiN0AACA5AgdAAAgOUIHAABIjtABAACSI3QAAIDkCB0AACA5QgcAAEiO0AEAAJIjdAAAgOQIHQAAIDlCBwAASI7QAQAAkiN0AACA5AgdAAAgOUIHAABIjtABAACSI3QAAIDkCB0AACA5QgcAAEjOvEJnx44d0dTUFDU1NdHR0RGHDh2a9dgvf/nL0dbWFtdcc01cffXV0dLSEl/84hfnPTAAAMCbKTl09uzZE7lcLgYGBuLw4cPR3Nwc3d3dceLEifMev3LlyvjTP/3TGB4ejn//93+Pvr6+6Ovri6eeeuqChwcAADifiizLslJO6OjoiNtvvz22b98eERGFQiEaGxvjnnvuiU2bNs3pMd773vfGunXr4oEHHpjT8RMTE1FXVxfj4+NRW1tbyrjAPDRt2jev845tWbfAkwAAzDTXNijpis6ZM2diZGQkurq6zj5AZWV0dXXF8PDwm56fZVkMDQ3F0aNH45d+6ZdmPW5qaiomJiZmbAAAAHNVUuicOnUqpqeno76+fsb++vr6yOfzs543Pj4ey5Yti6qqqli3bl08+uij8au/+quzHj84OBh1dXXFrbGxsZQxAQCAy9wlueva8uXL48iRI/Hss8/GX/7lX0Yul4tnnnlm1uM3b94c4+PjxW10dPRSjAkAACRiaSkHr1q1KpYsWRJjY2Mz9o+NjUVDQ8Os51VWVsZNN90UEREtLS3xve99LwYHB+MDH/jAeY+vrq6O6urqUkYDAAAoKumKTlVVVbS2tsbQ0FBxX6FQiKGhoejs7Jzz4xQKhZiamirlqQEAAOaspCs6ERG5XC56e3ujra0t2tvbY9u2bTE5ORl9fX0REdHT0xNr1qyJwcHBiHjt923a2triXe96V0xNTcXXv/71+OIXvxiPPfbYwr4SAACA/1dy6Kxfvz5OnjwZ/f39kc/no6WlJfbv31+8QcHx48ejsvLshaLJycn45Cc/GT/60Y/iyiuvjFtuuSW+9KUvxfr16xfuVQAAALxOyZ+jUw4+RwcuLZ+jAwAsVhflc3QAAADeCoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHLmFTo7duyIpqamqKmpiY6Ojjh06NCsxz7++OPx/ve/P1asWBErVqyIrq6uNzweAADgQpUcOnv27IlcLhcDAwNx+PDhaG5uju7u7jhx4sR5j3/mmWfiYx/7WDz99NMxPDwcjY2N8cEPfjB+/OMfX/DwAAAA51ORZVlWygkdHR1x++23x/bt2yMiolAoRGNjY9xzzz2xadOmNz1/eno6VqxYEdu3b4+enp45PefExETU1dXF+Ph41NbWljIuMA9Nm/bN67xjW9Yt8CQAADPNtQ1KuqJz5syZGBkZia6urrMPUFkZXV1dMTw8PKfHePnll+PVV1+NlStXznrM1NRUTExMzNgAAADmqqTQOXXqVExPT0d9ff2M/fX19ZHP5+f0GH/yJ38Sq1evnhFLP29wcDDq6uqKW2NjYyljAgAAl7lLete1LVu2xO7du2Pv3r1RU1Mz63GbN2+O8fHx4jY6OnoJpwQAAN7qlpZy8KpVq2LJkiUxNjY2Y//Y2Fg0NDS84bkPPfRQbNmyJf7lX/4lbrvttjc8trq6Oqqrq0sZDQAAoKikKzpVVVXR2toaQ0NDxX2FQiGGhoais7Nz1vMefPDBeOCBB2L//v3R1tY2/2kBAADmoKQrOhERuVwuent7o62tLdrb22Pbtm0xOTkZfX19ERHR09MTa9asicHBwYiI+MxnPhP9/f3xxBNPRFNTU/F3eZYtWxbLli1bwJcCAADwmpJDZ/369XHy5Mno7++PfD4fLS0tsX///uINCo4fPx6VlWcvFD322GNx5syZ+J3f+Z0ZjzMwMBB//ud/fmHTAwAAnEfJn6NTDj5HBy4tn6MDACxWF+VzdAAAAN4KhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMlZWu4BALi8NG3aN6/zjm1Zt8CTAJAyV3QAAIDkCB0AACA5QgcAAEiO0AEAAJIjdAAAgOQIHQAAIDlCBwAASI7QAQAAkiN0AACA5AgdAAAgOUIHAABIjtABAACSI3QAAIDkCB0AACA5QgcAAEiO0AEAAJIjdAAAgOQIHQAAIDlCBwAASI7QAQAAkiN0AACA5AgdAAAgOUIHAABIjtABAACSI3QAAIDkCB0AACA5QgcAAEiO0AEAAJKzdD4n7dixI7Zu3Rr5fD6am5vj0Ucfjfb29vMe+/zzz0d/f3+MjIzECy+8EJ/97Gfjvvvuu5CZ+X9Nm/aVfM6xLesuwiQAALC4lHxFZ8+ePZHL5WJgYCAOHz4czc3N0d3dHSdOnDjv8S+//HK8853vjC1btkRDQ8MFDwwAAPBmSg6dhx9+ODZs2BB9fX2xdu3a2LlzZ1x11VWxa9eu8x5/++23x9atW+N3f/d3o7q6+oIHBgAAeDMlhc6ZM2diZGQkurq6zj5AZWV0dXXF8PDwgg01NTUVExMTMzYAAIC5Kil0Tp06FdPT01FfXz9jf319feTz+QUbanBwMOrq6opbY2Pjgj02AACQvkV517XNmzfH+Ph4cRsdHS33SAAAwFtISXddW7VqVSxZsiTGxsZm7B8bG1vQGw1UV1f7fR4AAGDeSrqiU1VVFa2trTE0NFTcVygUYmhoKDo7Oxd8OAAAgPko+XN0crlc9Pb2RltbW7S3t8e2bdticnIy+vr6IiKip6cn1qxZE4ODgxHx2g0M/uM//qP4zz/+8Y/jyJEjsWzZsrjpppsW8KUAAAC8puTQWb9+fZw8eTL6+/sjn89HS0tL7N+/v3iDguPHj0dl5dkLRS+++GK85z3vKX790EMPxUMPPRR33HFHPPPMMxf+CgAAAH5OyaETEXH33XfH3Xfffd7v/Xy8NDU1RZZl83kaAACAeVmUd10DAAC4EEIHAABIzrzeugaLTdOmffM679iWdQs8CQAAi4ErOgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkJyl5R4AACAiomnTvpLPObZl3UWYBEiBKzoAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcoQOAACQHKEDAAAkR+gAAADJEToAAEByhA4AAJAcoQMAACRH6AAAAMkROgAAQHKEDgAAkByhAwAAJEfoAAAAyRE6AABAcpaWewCAxa5p076Szzm2Zd1FmAQAmCtXdAAAgOS4ogMAb2HzueIY4aojkD5XdAAAgOS4ogMAXDBXloDFRuhQdv7jCADAQptX6OzYsSO2bt0a+Xw+mpub49FHH4329vZZj3/yySfj/vvvj2PHjsXNN98cn/nMZ+I3fuM35j008MbcJQwAuNyVHDp79uyJXC4XO3fujI6Ojti2bVt0d3fH0aNH47rrrjvn+G9+85vxsY99LAYHB+M3f/M344knnoi77rorDh8+HLfeeuuCvAiA1LnyCZcXf+YXnv8JeNbl8rMoOXQefvjh2LBhQ/T19UVExM6dO2Pfvn2xa9eu2LRp0znHf+5zn4tf+7Vfiz/+4z+OiIgHHnggDhw4ENu3b4+dO3ee9zmmpqZiamqq+PX4+HhERExMTJQ67kVx68BTJZ/z3U91L/gchamXSz5nsfwMX28+ryNi5mtZiMdYLP9eF2KOC10bC/HzTMli+LOW0r+TxfJaFsuf+Qu1WH6e5fq7fDGu8YVQrv+uRSzOdb4QrK+z3uo/i5/NkmXZGx+YlWBqaipbsmRJtnfv3hn7e3p6sg9/+MPnPaexsTH77Gc/O2Nff39/dtttt836PAMDA1lE2Gw2m81ms9lsNtt5t9HR0Tdsl5Ku6Jw6dSqmp6ejvr5+xv76+vr4/ve/f95z8vn8eY/P5/OzPs/mzZsjl8sVvy4UCvHf//3fce2110ZFRUUpI19SExMT0djYGKOjo1FbW1vucWAG65PFzhplMbM+WewupzWaZVmcPn06Vq9e/YbHLcq7rlVXV0d1dfWMfddcc015hpmH2tra5BcYb13WJ4udNcpiZn2y2F0ua7Suru5NjynpA0NXrVoVS5YsibGxsRn7x8bGoqGh4bznNDQ0lHQ8AADAhSopdKqqqqK1tTWGhoaK+wqFQgwNDUVnZ+d5z+ns7JxxfETEgQMHZj0eAADgQpX81rVcLhe9vb3R1tYW7e3tsW3btpicnCzeha2npyfWrFkTg4ODERFx7733xh133BF/9Vd/FevWrYvdu3fHd77znfjrv/7rhX0li0B1dXUMDAyc87Y7WAysTxY7a5TFzPpksbNGz1WRZW92X7Zzbd++vfiBoS0tLfHII49ER0dHRER84AMfiKampvjCF75QPP7JJ5+MP/uzPyt+YOiDDz7oA0MBAICLZl6hAwAAsJiV9Ds6AAAAbwVCBwAASI7QAQAAkiN0AACA5AidBbJjx45oamqKmpqa6OjoiEOHDpV7JC5T//Zv/xYf+tCHYvXq1VFRURH/+I//OOP7WZZFf39/vP3tb48rr7wyurq64j//8z/LMyyXncHBwbj99ttj+fLlcd1118Vdd90VR48enXHMK6+8Ehs3boxrr702li1bFr/92799zgdPw8Xy2GOPxW233Vb8dPnOzs74p3/6p+L3rU8Wky1btkRFRUXcd999xX3W6FlCZwHs2bMncrlcDAwMxOHDh6O5uTm6u7vjxIkT5R6Ny9Dk5GQ0NzfHjh07zvv9Bx98MB555JHYuXNnfPvb346rr746uru745VXXrnEk3I5OnjwYGzcuDG+9a1vxYEDB+LVV1+ND37wgzE5OVk85g//8A/jq1/9ajz55JNx8ODBePHFF+O3fuu3yjg1l5Prr78+tmzZEiMjI/Gd73wnfuVXfiU+8pGPxPPPPx8R1ieLx7PPPhuf//zn47bbbpux3xp9nYwL1t7enm3cuLH49fT0dLZ69epscHCwjFNBlkVEtnfv3uLXhUIha2hoyLZu3Vrc99JLL2XV1dXZ3//935dhQi53J06cyCIiO3jwYJZlr63HK664InvyySeLx3zve9/LIiIbHh4u15hc5lasWJH9zd/8jfXJonH69Ons5ptvzg4cOJDdcccd2b333ptlmb9Df54rOhfozJkzMTIyEl1dXcV9lZWV0dXVFcPDw2WcDM71wx/+MPL5/Iz1WldXFx0dHdYrZTE+Ph4REStXroyIiJGRkXj11VdnrNFbbrklbrjhBmuUS256ejp2794dk5OT0dnZaX2yaGzcuDHWrVs3Yy1G+Dv05y0t9wBvdadOnYrp6emor6+fsb++vj6+//3vl2kqOL98Ph8Rcd71+rPvwaVSKBTivvvui1/8xV+MW2+9NSJeW6NVVVVxzTXXzDjWGuVSeu6556KzszNeeeWVWLZsWezduzfWrl0bR44csT4pu927d8fhw4fj2WefPed7/g6dSegAUBYbN26M7373u/GNb3yj3KPADO9+97vjyJEjMT4+Hv/wD/8Qvb29cfDgwXKPBTE6Ohr33ntvHDhwIGpqaso9zqLnrWsXaNWqVbFkyZJz7mYxNjYWDQ0NZZoKzu9na9J6pdzuvvvu+NrXvhZPP/10XH/99cX9DQ0NcebMmXjppZdmHG+NcilVVVXFTTfdFK2trTE4OBjNzc3xuc99zvqk7EZGRuLEiRPx3ve+N5YuXRpLly6NgwcPxiOPPBJLly6N+vp6a/R1hM4FqqqqitbW1hgaGiruKxQKMTQ0FJ2dnWWcDM514403RkNDw4z1OjExEd/+9retVy6JLMvi7rvvjr1798a//uu/xo033jjj+62trXHFFVfMWKNHjx6N48ePW6OUTaFQiKmpKeuTsrvzzjvjueeeiyNHjhS3tra2+PjHP178Z2v0LG9dWwC5XC56e3ujra0t2tvbY9u2bTE5ORl9fX3lHo3L0E9/+tP4wQ9+UPz6hz/8YRw5ciRWrlwZN9xwQ9x3333xF3/xF3HzzTfHjTfeGPfff3+sXr067rrrrvINzWVj48aN8cQTT8RXvvKVWL58efE943V1dXHllVdGXV1dfOITn4hcLhcrV66M2trauOeee6KzszPe9773lXl6LgebN2+OX//1X48bbrghTp8+HU888UQ888wz8dRTT1mflN3y5cuLv9P4M1dffXVce+21xf3W6OuU+7ZvqXj00UezG264Iauqqsra29uzb33rW+UeicvU008/nUXEOVtvb2+WZa/dYvr+++/P6uvrs+rq6uzOO+/Mjh49Wt6huWycb21GRPZ3f/d3xWP+93//N/vkJz+ZrVixIrvqqquyj370o9lPfvKT8g3NZeX3f//3s3e84x1ZVVVV9ra3vS278847s3/+538uft/6ZLF5/e2ls8wafb2KLMuyMjUWAADAReF3dAAAgOQIHQAAIDlCBwAASI7QAQAAkiN0AACA5AgdAAAgOUIHAABIjtABAACSI3QAAIDkCB0AACA5QgcAAEjO/wG2oi8qwAC6hQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.functional as F\n",
    "with torch.no_grad():\n",
    "    l = nn.Softmax()(model(test_dataset[432][0][0].view([1, 3, 224, 224])) * 0.5 + \\\n",
    "                     model(test_dataset[432][0][1].view([1, 3, 224, 224])) * 0.5)[0].cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(x=range(42), height=l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc80103d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10593451, 0.54136795)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[2], l[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c09a4fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'abraham_grampa_simpson',\n",
       " 1: 'agnes_skinner',\n",
       " 2: 'apu_nahasapeemapetilon',\n",
       " 3: 'barney_gumble',\n",
       " 4: 'bart_simpson',\n",
       " 5: 'carl_carlson',\n",
       " 6: 'charles_montgomery_burns',\n",
       " 7: 'chief_wiggum',\n",
       " 8: 'cletus_spuckler',\n",
       " 9: 'comic_book_guy',\n",
       " 10: 'disco_stu',\n",
       " 11: 'edna_krabappel',\n",
       " 12: 'fat_tony',\n",
       " 13: 'gil',\n",
       " 14: 'groundskeeper_willie',\n",
       " 15: 'homer_simpson',\n",
       " 16: 'kent_brockman',\n",
       " 17: 'krusty_the_clown',\n",
       " 18: 'lenny_leonard',\n",
       " 19: 'lionel_hutz',\n",
       " 20: 'lisa_simpson',\n",
       " 21: 'maggie_simpson',\n",
       " 22: 'marge_simpson',\n",
       " 23: 'martin_prince',\n",
       " 24: 'mayor_quimby',\n",
       " 25: 'milhouse_van_houten',\n",
       " 26: 'miss_hoover',\n",
       " 27: 'moe_szyslak',\n",
       " 28: 'ned_flanders',\n",
       " 29: 'nelson_muntz',\n",
       " 30: 'otto_mann',\n",
       " 31: 'patty_bouvier',\n",
       " 32: 'principal_skinner',\n",
       " 33: 'professor_john_frink',\n",
       " 34: 'rainier_wolfcastle',\n",
       " 35: 'ralph_wiggum',\n",
       " 36: 'selma_bouvier',\n",
       " 37: 'sideshow_bob',\n",
       " 38: 'sideshow_mel',\n",
       " 39: 'snake_jailbird',\n",
       " 40: 'troy_mcclure',\n",
       " 41: 'waylon_smithers'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa2feef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Expected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img0.jpg</td>\n",
       "      <td>nelson_muntz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img1.jpg</td>\n",
       "      <td>bart_simpson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img10.jpg</td>\n",
       "      <td>ned_flanders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img100.jpg</td>\n",
       "      <td>chief_wiggum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img101.jpg</td>\n",
       "      <td>apu_nahasapeemapetilon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Id                Expected\n",
       "0    img0.jpg            nelson_muntz\n",
       "1    img1.jpg            bart_simpson\n",
       "2   img10.jpg            ned_flanders\n",
       "3  img100.jpg            chief_wiggum\n",
       "4  img101.jpg  apu_nahasapeemapetilon"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trues = pd.read_csv('best_submission.csv').sort_values(by='Id').reset_index(drop=True)\n",
    "trues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f74991d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Expected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img0.jpg</td>\n",
       "      <td>nelson_muntz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img1.jpg</td>\n",
       "      <td>bart_simpson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img10.jpg</td>\n",
       "      <td>ned_flanders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img100.jpg</td>\n",
       "      <td>chief_wiggum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img101.jpg</td>\n",
       "      <td>apu_nahasapeemapetilon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Id                Expected\n",
       "0    img0.jpg            nelson_muntz\n",
       "1    img1.jpg            bart_simpson\n",
       "2   img10.jpg            ned_flanders\n",
       "3  img100.jpg            chief_wiggum\n",
       "4  img101.jpg  apu_nahasapeemapetilon"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_submit = my_submit.sort_values(by='Id').reset_index(drop=True)\n",
    "my_submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f444d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(991, 991)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(my_submit['Expected'] == trues['Expected']).sum(), (my_submit['Expected'] == trues['Expected']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5510e314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Expected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Id, Expected]\n",
       "Index: []"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_submit[my_submit['Expected'] != trues['Expected']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3bd56c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Expected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Id, Expected]\n",
       "Index: []"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trues[my_submit['Expected'] != trues['Expected']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b657421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 99.99999999999997)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.random.rand(len(my_submit)) < 0.19\n",
    "mask[432] = 1\n",
    "f1s = f1_score(trues.loc[mask, 'Expected'], my_submit.loc[mask, 'Expected'], average='micro')\n",
    "f1s, (f1s - 0.8) * 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83badc1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 1134053,
     "sourceId": 20320,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1719.589167,
   "end_time": "2024-12-15T18:55:57.872153",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-15T18:27:18.282986",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
