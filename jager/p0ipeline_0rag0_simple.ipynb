{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba588aa",
   "metadata": {},
   "source": [
    "# Simple RAG Pipeline - Простой RAG без векторных БД\n",
    "\n",
    "Пайплайн для:\n",
    "- Retrieval без FAISS (TF-IDF, BM25)\n",
    "- Простое хранилище документов\n",
    "- Context injection в промпты\n",
    "- Question Answering с LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efb8f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch pandas numpy scikit-learn rank-bm25 sentence-transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2823dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Библиотеки загружены!\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5177c1f6",
   "metadata": {},
   "source": [
    "## 1. Загрузка документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117a8289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ВАШИ ДАННЫЕ ===\n",
    "# Вариант 1: CSV с документами\n",
    "docs_df = pd.read_csv('documents.csv')\n",
    "# Колонки: 'id', 'text', 'title' (опционально)\n",
    "\n",
    "# Вариант 2: Текстовые файлы\n",
    "# import os\n",
    "# documents = []\n",
    "# for filename in os.listdir('docs_folder/'):\n",
    "#     with open(f'docs_folder/{filename}', 'r', encoding='utf-8') as f:\n",
    "#         documents.append({'id': filename, 'text': f.read()})\n",
    "# docs_df = pd.DataFrame(documents)\n",
    "\n",
    "# Вариант 3: Простой список\n",
    "# documents = [\n",
    "#     \"Документ 1: Машинное обучение - это...\",\n",
    "#     \"Документ 2: Нейронные сети применяются...\",\n",
    "# ]\n",
    "# docs_df = pd.DataFrame({'id': range(len(documents)), 'text': documents})\n",
    "\n",
    "print(f\"Загружено документов: {len(docs_df)}\")\n",
    "print(f\"\\nПервые документы:\")\n",
    "print(docs_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470f325b",
   "metadata": {},
   "source": [
    "## 2. Simple Document Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673726eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDocumentStore:\n",
    "    \"\"\"\n",
    "    Простое хранилище документов в памяти\n",
    "    \"\"\"\n",
    "    def __init__(self, documents: pd.DataFrame):\n",
    "        self.documents = documents.to_dict('records')\n",
    "        self.texts = [doc['text'] for doc in self.documents]\n",
    "    \n",
    "    def get_document(self, idx: int) -> Dict:\n",
    "        return self.documents[idx]\n",
    "    \n",
    "    def get_documents_by_ids(self, ids: List[int]) -> List[Dict]:\n",
    "        return [self.documents[i] for i in ids]\n",
    "    \n",
    "    def get_all_texts(self) -> List[str]:\n",
    "        return self.texts\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.documents)\n",
    "\n",
    "# Создание хранилища\n",
    "doc_store = SimpleDocumentStore(docs_df)\n",
    "print(f\"✓ Document store создан! Документов: {len(doc_store)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1be765",
   "metadata": {},
   "source": [
    "## 3. Retriever - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68a7aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFRetriever:\n",
    "    \"\"\"\n",
    "    Retriever на основе TF-IDF\n",
    "    \"\"\"\n",
    "    def __init__(self, doc_store: SimpleDocumentStore):\n",
    "        self.doc_store = doc_store\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=5000,\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words='english'  # или None для русского\n",
    "        )\n",
    "        \n",
    "        # Индексация документов\n",
    "        self.doc_vectors = self.vectorizer.fit_transform(doc_store.get_all_texts())\n",
    "        print(f\"✓ TF-IDF индекс построен! Shape: {self.doc_vectors.shape}\")\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 3) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Поиск топ-k наиболее релевантных документов\n",
    "        \"\"\"\n",
    "        # Векторизация запроса\n",
    "        query_vector = self.vectorizer.transform([query])\n",
    "        \n",
    "        # Вычисление косинусного сходства\n",
    "        similarities = cosine_similarity(query_vector, self.doc_vectors)[0]\n",
    "        \n",
    "        # Топ-k документов\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            doc = self.doc_store.get_document(idx)\n",
    "            doc['score'] = float(similarities[idx])\n",
    "            results.append(doc)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Создание retriever\n",
    "tfidf_retriever = TFIDFRetriever(doc_store)\n",
    "print(\"✓ TF-IDF Retriever готов!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34f78f2",
   "metadata": {},
   "source": [
    "## 4. Retriever - BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93431b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25Retriever:\n",
    "    \"\"\"\n",
    "    Retriever на основе BM25 (улучшенная версия TF-IDF)\n",
    "    \"\"\"\n",
    "    def __init__(self, doc_store: SimpleDocumentStore):\n",
    "        self.doc_store = doc_store\n",
    "        \n",
    "        # Токенизация документов (простая по пробелам)\n",
    "        self.tokenized_docs = [doc.lower().split() for doc in doc_store.get_all_texts()]\n",
    "        \n",
    "        # Создание BM25 индекса\n",
    "        self.bm25 = BM25Okapi(self.tokenized_docs)\n",
    "        print(f\"✓ BM25 индекс построен!\")\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 3) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Поиск с BM25\n",
    "        \"\"\"\n",
    "        # Токенизация запроса\n",
    "        tokenized_query = query.lower().split()\n",
    "        \n",
    "        # Получение скоров\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        # Топ-k документов\n",
    "        top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            doc = self.doc_store.get_document(idx)\n",
    "            doc['score'] = float(scores[idx])\n",
    "            results.append(doc)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Создание BM25 retriever\n",
    "bm25_retriever = BM25Retriever(doc_store)\n",
    "print(\"✓ BM25 Retriever готов!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca667a6f",
   "metadata": {},
   "source": [
    "## 5. Retriever - Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dc0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticRetriever:\n",
    "    \"\"\"\n",
    "    Semantic retriever с sentence-transformers\n",
    "    \"\"\"\n",
    "    def __init__(self, doc_store: SimpleDocumentStore, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.doc_store = doc_store\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        \n",
    "        # Кодирование всех документов\n",
    "        print(\"Кодирование документов...\")\n",
    "        self.doc_embeddings = self.model.encode(\n",
    "            doc_store.get_all_texts(),\n",
    "            convert_to_tensor=True,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        print(f\"✓ Embeddings готовы! Shape: {self.doc_embeddings.shape}\")\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 3) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Semantic search\n",
    "        \"\"\"\n",
    "        # Кодирование запроса\n",
    "        query_embedding = self.model.encode(query, convert_to_tensor=True)\n",
    "        \n",
    "        # Вычисление косинусного сходства\n",
    "        similarities = torch.nn.functional.cosine_similarity(\n",
    "            query_embedding.unsqueeze(0),\n",
    "            self.doc_embeddings\n",
    "        )\n",
    "        \n",
    "        # Топ-k\n",
    "        top_scores, top_indices = torch.topk(similarities, k=min(top_k, len(similarities)))\n",
    "        \n",
    "        results = []\n",
    "        for idx, score in zip(top_indices.cpu().numpy(), top_scores.cpu().numpy()):\n",
    "            doc = self.doc_store.get_document(int(idx))\n",
    "            doc['score'] = float(score)\n",
    "            results.append(doc)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Создание semantic retriever\n",
    "semantic_retriever = SemanticRetriever(doc_store)\n",
    "print(\"✓ Semantic Retriever готов!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef49ba40",
   "metadata": {},
   "source": [
    "## 6. Тестирование Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91f343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ТЕСТОВЫЙ ЗАПРОС ===\n",
    "test_query = \"What is machine learning?\"\n",
    "\n",
    "print(f\"Запрос: {test_query}\\n\")\n",
    "\n",
    "# TF-IDF\n",
    "print(\"=\" * 60)\n",
    "print(\"TF-IDF Results:\")\n",
    "print(\"=\" * 60)\n",
    "tfidf_results = tfidf_retriever.retrieve(test_query, top_k=3)\n",
    "for i, doc in enumerate(tfidf_results, 1):\n",
    "    print(f\"{i}. Score: {doc['score']:.4f}\")\n",
    "    print(f\"   Text: {doc['text'][:200]}...\\n\")\n",
    "\n",
    "# BM25\n",
    "print(\"=\" * 60)\n",
    "print(\"BM25 Results:\")\n",
    "print(\"=\" * 60)\n",
    "bm25_results = bm25_retriever.retrieve(test_query, top_k=3)\n",
    "for i, doc in enumerate(bm25_results, 1):\n",
    "    print(f\"{i}. Score: {doc['score']:.4f}\")\n",
    "    print(f\"   Text: {doc['text'][:200]}...\\n\")\n",
    "\n",
    "# Semantic\n",
    "print(\"=\" * 60)\n",
    "print(\"Semantic Results:\")\n",
    "print(\"=\" * 60)\n",
    "semantic_results = semantic_retriever.retrieve(test_query, top_k=3)\n",
    "for i, doc in enumerate(semantic_results, 1):\n",
    "    print(f\"{i}. Score: {doc['score']:.4f}\")\n",
    "    print(f\"   Text: {doc['text'][:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c981f206",
   "metadata": {},
   "source": [
    "## 7. RAG Pipeline - Генерация ответов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e63439",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRAGPipeline:\n",
    "    \"\"\"\n",
    "    Простой RAG: Retrieval + Generation\n",
    "    \"\"\"\n",
    "    def __init__(self, retriever, llm_model_name: str = \"microsoft/phi-2\"):\n",
    "        self.retriever = retriever\n",
    "        \n",
    "        # Загрузка LLM\n",
    "        print(f\"Загрузка LLM: {llm_model_name}...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(llm_model_name, trust_remote_code=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            llm_model_name,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        self.generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.7,\n",
    "            do_sample=True\n",
    "        )\n",
    "        print(\"✓ LLM загружена!\")\n",
    "    \n",
    "    def create_prompt(self, query: str, context_docs: List[Dict]) -> str:\n",
    "        \"\"\"\n",
    "        Создание промпта с контекстом\n",
    "        \"\"\"\n",
    "        context = \"\\n\\n\".join([f\"Document {i+1}: {doc['text']}\" \n",
    "                               for i, doc in enumerate(context_docs)])\n",
    "        \n",
    "        prompt = f\"\"\"Use the following context to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def answer(self, query: str, top_k: int = 3) -> Dict:\n",
    "        \"\"\"\n",
    "        Полный RAG pipeline\n",
    "        \"\"\"\n",
    "        # 1. Retrieval\n",
    "        retrieved_docs = self.retriever.retrieve(query, top_k=top_k)\n",
    "        \n",
    "        # 2. Создание промпта\n",
    "        prompt = self.create_prompt(query, retrieved_docs)\n",
    "        \n",
    "        # 3. Generation\n",
    "        response = self.generator(prompt)[0]['generated_text']\n",
    "        \n",
    "        # Извлекаем только ответ (после \"Answer:\")\n",
    "        answer_start = response.find(\"Answer:\") + len(\"Answer:\")\n",
    "        answer = response[answer_start:].strip()\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'answer': answer,\n",
    "            'retrieved_docs': retrieved_docs,\n",
    "            'prompt': prompt\n",
    "        }\n",
    "\n",
    "# Создание RAG pipeline (выберите retriever)\n",
    "rag = SimpleRAGPipeline(retriever=semantic_retriever)  # или tfidf_retriever, bm25_retriever\n",
    "print(\"\\n✓ RAG Pipeline готов!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561d77e3",
   "metadata": {},
   "source": [
    "## 8. Тестирование RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3045cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ТЕСТОВЫЕ ВОПРОСЫ ===\n",
    "questions = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How do neural networks work?\",\n",
    "    \"Explain deep learning\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"ВОПРОС: {question}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    result = rag.answer(question, top_k=2)\n",
    "    \n",
    "    print(f\"\\nОТВЕТ: {result['answer']}\")\n",
    "    \n",
    "    print(f\"\\nИСПОЛЬЗОВАНЫ ДОКУМЕНТЫ:\")\n",
    "    for i, doc in enumerate(result['retrieved_docs'], 1):\n",
    "        print(f\"{i}. Score: {doc['score']:.4f} - {doc['text'][:150]}...\")\n",
    "    \n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97d1f97",
   "metadata": {},
   "source": [
    "## 9. Batch Processing для Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a76a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ЗАГРУЗКА ТЕСТОВЫХ ДАННЫХ ===\n",
    "# test_df = pd.read_csv('test.csv')\n",
    "# Колонки: 'id', 'query'\n",
    "\n",
    "# Пример\n",
    "test_df = pd.DataFrame({\n",
    "    'id': [1, 2, 3],\n",
    "    'query': [\n",
    "        \"What is AI?\",\n",
    "        \"Explain supervised learning\",\n",
    "        \"How to train neural networks?\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Генерация ответов\n",
    "answers = []\n",
    "for idx, row in test_df.iterrows():\n",
    "    result = rag.answer(row['query'], top_k=3)\n",
    "    answers.append(result['answer'])\n",
    "    \n",
    "    if (idx + 1) % 10 == 0:\n",
    "        print(f\"Обработано: {idx + 1}/{len(test_df)}\")\n",
    "\n",
    "print(f\"\\n✓ Все ответы сгенерированы!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a541e9",
   "metadata": {},
   "source": [
    "## 10. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33174cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'answer': answers\n",
    "})\n",
    "\n",
    "submission.to_csv('rag_submission.csv', index=False)\n",
    "print(\"\\n✓ Submission сохранен!\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13ecada",
   "metadata": {},
   "source": [
    "## 11. Evaluation (если есть ground truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36dc96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если есть правильные ответы\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from nltk.translate.bleu_score import sentence_bleu\n",
    "# from rouge import Rouge\n",
    "\n",
    "# val_df = pd.read_csv('val.csv')  # С ground truth ответами\n",
    "\n",
    "# rouge = Rouge()\n",
    "# rouge_scores = []\n",
    "\n",
    "# for idx, row in val_df.iterrows():\n",
    "#     result = rag.answer(row['query'])\n",
    "#     score = rouge.get_scores(result['answer'], row['ground_truth_answer'])[0]\n",
    "#     rouge_scores.append(score)\n",
    "\n",
    "# print(f\"Average ROUGE-L F1: {np.mean([s['rouge-l']['f'] for s in rouge_scores])}\")\n",
    "\n",
    "print(\"Добавьте свою метрику оценки!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
