{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d105d70d",
   "metadata": {},
   "source": [
    "# Audio Classification: Zero-Shot & Fine-Tuning\n",
    "\n",
    "Два подхода к классификации аудио:\n",
    "1. **Zero-Shot** - классификация без обучения с помощью CLAP\n",
    "2. **Fine-Tuning** - дообучение wav2vec2 / HuBERT на своих данных\n",
    "\n",
    "Применимо для:\n",
    "- Классификация звуков\n",
    "- Распознавание эмоций\n",
    "- Музыкальные жанры\n",
    "- Sound event detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7d0eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch torchaudio librosa soundfile pandas numpy scikit-learn datasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63acb7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoFeatureExtractor, \n",
    "    AutoModelForAudioClassification,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2ForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from datasets import Dataset, Audio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Библиотеки загружены!\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7a0aaa",
   "metadata": {},
   "source": [
    "## 1. Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f835cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ВАШИ ДАННЫЕ ===\n",
    "# Формат: CSV с колонками 'file_path', 'label'\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "SAMPLE_RATE = 16000  # Wav2Vec2 работает на 16kHz\n",
    "MAX_DURATION = 10  # секунд\n",
    "\n",
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "train_df['label_encoded'] = le.fit_transform(train_df['label'])\n",
    "num_labels = len(le.classes_)\n",
    "\n",
    "print(f\"Train samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"Количество классов: {num_labels}\")\n",
    "print(f\"Классы: {le.classes_}\")\n",
    "print(f\"\\nРаспределение классов:\")\n",
    "print(train_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50457d64",
   "metadata": {},
   "source": [
    "## ЧАСТЬ 1: ZERO-SHOT CLASSIFICATION с CLAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02815b7c",
   "metadata": {},
   "source": [
    "### 1.1 Загрузка CLAP модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7488c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLAP (Contrastive Language-Audio Pretraining)\n",
    "# Альтернатива: используем модель, которая понимает текстовые описания звуков\n",
    "\n",
    "# Для zero-shot нужна модель типа CLAP или AudioCLIP\n",
    "# К сожалению, готовых моделей в HuggingFace мало\n",
    "# Используем Wav2Vec2 + текстовые эмбеддинги как workaround\n",
    "\n",
    "print(\"\\n⚠️ CLAP модели пока ограничены в HuggingFace\")\n",
    "print(\"Для настоящего zero-shot используйте:\")\n",
    "print(\"- microsoft/CLAP (если доступна)\")\n",
    "print(\"- laion/clap-htsat-unfused\")\n",
    "print(\"\\nПереходим к Fine-Tuning подходу...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797ccc47",
   "metadata": {},
   "source": [
    "### 1.2 Zero-Shot с предобученной моделью (альтернатива)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90824ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем предобученную модель для быстрого baseline\n",
    "# Например, модель обученная на AudioSet или ESC-50\n",
    "\n",
    "zero_shot_model_name = \"MIT/ast-finetuned-audioset-10-10-0.4593\"  # Audio Spectrogram Transformer\n",
    "\n",
    "try:\n",
    "    zero_shot_model = AutoModelForAudioClassification.from_pretrained(zero_shot_model_name)\n",
    "    zero_shot_processor = AutoFeatureExtractor.from_pretrained(zero_shot_model_name)\n",
    "    \n",
    "    print(f\"✓ Zero-shot модель загружена: {zero_shot_model_name}\")\n",
    "    \n",
    "    # Пример предсказания\n",
    "    def zero_shot_predict(audio_path):\n",
    "        audio, sr = librosa.load(audio_path, sr=SAMPLE_RATE, duration=MAX_DURATION)\n",
    "        inputs = zero_shot_processor(audio, sampling_rate=SAMPLE_RATE, return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = zero_shot_model(**inputs).logits\n",
    "        \n",
    "        predicted_class_id = logits.argmax(-1).item()\n",
    "        predicted_label = zero_shot_model.config.id2label[predicted_class_id]\n",
    "        \n",
    "        return predicted_label, logits\n",
    "    \n",
    "    # Тестируем на первом примере\n",
    "    sample_audio = train_df.iloc[0]['file_path']\n",
    "    pred_label, logits = zero_shot_predict(sample_audio)\n",
    "    print(f\"\\nПример zero-shot предсказания:\")\n",
    "    print(f\"Файл: {sample_audio}\")\n",
    "    print(f\"Предсказанный класс: {pred_label}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Не удалось загрузить zero-shot модель: {e}\")\n",
    "    print(\"Переходим к fine-tuning...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c907f7dd",
   "metadata": {},
   "source": [
    "## ЧАСТЬ 2: FINE-TUNING Wav2Vec2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b88124",
   "metadata": {},
   "source": [
    "### 2.1 Загрузка предобученной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8125ce8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выбор модели\n",
    "MODEL_NAME = \"facebook/wav2vec2-base\"  # базовая модель\n",
    "# MODEL_NAME = \"facebook/wav2vec2-large-xlsr-53\"  # мультиязычная, более мощная\n",
    "# MODEL_NAME = \"microsoft/wavlm-base\"  # альтернатива\n",
    "# MODEL_NAME = \"facebook/hubert-base-ls960\"  # HuBERT\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(MODEL_NAME)\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "print(f\"✓ Модель загружена: {MODEL_NAME}\")\n",
    "print(f\"Параметры модели: {sum(p.numel() for p in model.parameters())/1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ebc728",
   "metadata": {},
   "source": [
    "### 2.2 Подготовка данных для Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34ff3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Val split\n",
    "train_df_split, val_df_split = train_test_split(\n",
    "    train_df, test_size=0.15, random_state=42, stratify=train_df['label']\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_df_split)}, Val: {len(val_df_split)}\")\n",
    "\n",
    "# Создание HuggingFace Dataset\n",
    "def create_dataset(df):\n",
    "    dataset_dict = {\n",
    "        'audio': df['file_path'].tolist(),\n",
    "        'label': df['label_encoded'].tolist()\n",
    "    }\n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "    dataset = dataset.cast_column('audio', Audio(sampling_rate=SAMPLE_RATE))\n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_dataset(train_df_split)\n",
    "val_dataset = create_dataset(val_df_split)\n",
    "\n",
    "print(f\"\\n✓ Datasets созданы!\")\n",
    "print(f\"Train dataset: {train_dataset}\")\n",
    "print(f\"Val dataset: {val_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be0f87b",
   "metadata": {},
   "source": [
    "### 2.3 Preprocessing функция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b1b34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Preprocessing для аудио\n",
    "    \"\"\"\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    \n",
    "    # Обрезаем или дополняем до MAX_DURATION\n",
    "    max_length = int(SAMPLE_RATE * MAX_DURATION)\n",
    "    processed_arrays = []\n",
    "    \n",
    "    for audio in audio_arrays:\n",
    "        if len(audio) > max_length:\n",
    "            audio = audio[:max_length]\n",
    "        elif len(audio) < max_length:\n",
    "            audio = np.pad(audio, (0, max_length - len(audio)), mode='constant')\n",
    "        processed_arrays.append(audio)\n",
    "    \n",
    "    # Feature extraction\n",
    "    inputs = feature_extractor(\n",
    "        processed_arrays,\n",
    "        sampling_rate=SAMPLE_RATE,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    inputs[\"labels\"] = examples[\"label\"]\n",
    "    return inputs\n",
    "\n",
    "# Применяем preprocessing\n",
    "print(\"Preprocessing данных...\")\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True, batch_size=8, remove_columns=[\"audio\"])\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True, batch_size=8, remove_columns=[\"audio\"])\n",
    "\n",
    "print(\"✓ Preprocessing завершен!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e732cfa3",
   "metadata": {},
   "source": [
    "### 2.4 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71a1168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Вычисление метрик\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='macro')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4214dce",
   "metadata": {},
   "source": [
    "### 2.5 Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb95acae",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./audio_finetuned_model\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=50,\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=False,\n",
    "    fp16=torch.cuda.is_available(),  # Mixed precision training\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "print(\"✓ Training arguments настроены!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f831d2",
   "metadata": {},
   "source": [
    "### 2.6 Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2064099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\nНачало fine-tuning...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Обучение\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ Fine-tuning завершен!\")\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a70ee22",
   "metadata": {},
   "source": [
    "### 2.7 Оценка на валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a62d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nРезультаты на валидации:\")\n",
    "print(\"=\"*60)\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2432adb",
   "metadata": {},
   "source": [
    "### 2.8 Детальный анализ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54889f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказания на валидации\n",
    "predictions = trainer.predict(val_dataset)\n",
    "y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "y_true = predictions.label_ids\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547a8abb",
   "metadata": {},
   "source": [
    "## 3. Предсказания на Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af16dc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка test dataset\n",
    "test_df_copy = test_df.copy()\n",
    "test_df_copy['label_encoded'] = 0  # Dummy label\n",
    "\n",
    "test_dataset = create_dataset(test_df_copy)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True, batch_size=8, remove_columns=[\"audio\"])\n",
    "\n",
    "print(f\"✓ Test dataset готов: {len(test_dataset)} samples\")\n",
    "\n",
    "# Предсказания\n",
    "print(\"\\nГенерация предсказаний...\")\n",
    "test_predictions = trainer.predict(test_dataset)\n",
    "test_pred_labels = np.argmax(test_predictions.predictions, axis=1)\n",
    "\n",
    "# Декодирование меток\n",
    "test_pred_labels_decoded = le.inverse_transform(test_pred_labels)\n",
    "\n",
    "print(f\"\\n✓ Предсказания готовы!\")\n",
    "print(f\"\\nРаспределение предсказанных классов:\")\n",
    "print(pd.Series(test_pred_labels_decoded).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f0348b",
   "metadata": {},
   "source": [
    "## 4. Сохранение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41aef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение лучшей модели\n",
    "trainer.save_model(\"./best_audio_model\")\n",
    "feature_extractor.save_pretrained(\"./best_audio_model\")\n",
    "\n",
    "print(\"✓ Модель сохранена в ./best_audio_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c63753e",
   "metadata": {},
   "source": [
    "## 5. Inference функция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b11b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_audio(audio_path, model, feature_extractor, label_encoder):\n",
    "    \"\"\"\n",
    "    Предсказание класса для одного аудио файла\n",
    "    \"\"\"\n",
    "    # Загрузка аудио\n",
    "    audio, sr = librosa.load(audio_path, sr=SAMPLE_RATE, duration=MAX_DURATION)\n",
    "    \n",
    "    # Padding\n",
    "    max_length = int(SAMPLE_RATE * MAX_DURATION)\n",
    "    if len(audio) < max_length:\n",
    "        audio = np.pad(audio, (0, max_length - len(audio)), mode='constant')\n",
    "    elif len(audio) > max_length:\n",
    "        audio = audio[:max_length]\n",
    "    \n",
    "    # Feature extraction\n",
    "    inputs = feature_extractor(\n",
    "        audio,\n",
    "        sampling_rate=SAMPLE_RATE,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Предсказание\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    \n",
    "    predicted_class_id = logits.argmax(-1).item()\n",
    "    predicted_label = label_encoder.inverse_transform([predicted_class_id])[0]\n",
    "    confidence = torch.softmax(logits, dim=-1)[0][predicted_class_id].item()\n",
    "    \n",
    "    return predicted_label, confidence\n",
    "\n",
    "# Пример использования\n",
    "sample_file = test_df.iloc[0]['file_path']\n",
    "pred_label, confidence = predict_audio(sample_file, model, feature_extractor, le)\n",
    "print(f\"\\nПример предсказания:\")\n",
    "print(f\"Файл: {sample_file}\")\n",
    "print(f\"Класс: {pred_label}\")\n",
    "print(f\"Уверенность: {confidence:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae2da48",
   "metadata": {},
   "source": [
    "## 6. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055fd89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'id': test_df.index,  # или test_df['id']\n",
    "    'prediction': test_pred_labels_decoded\n",
    "})\n",
    "\n",
    "submission.to_csv('audio_classification_submission.csv', index=False)\n",
    "print(\"\\n✓ Submission сохранен!\")\n",
    "print(submission.head(10))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
