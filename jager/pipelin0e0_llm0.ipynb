{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee3362e6",
   "metadata": {},
   "source": [
    "# LLM Pipeline - Готовый пайплайн для работы с большими языковыми моделями\n",
    "\n",
    "Этот notebook содержит готовый пайплайн для:\n",
    "- Загрузки и использования LLM моделей\n",
    "- Fine-tuning с использованием LoRA/QLoRA\n",
    "- Генерации текста и чат-ботов\n",
    "- Квантизации моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24456a5f",
   "metadata": {},
   "source": [
    "## 1. Установка зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a14c33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка необходимых библиотек\n",
    "!pip install transformers accelerate bitsandbytes peft datasets torch trl -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe50ad3",
   "metadata": {},
   "source": [
    "## 2. Импорты и настройка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c745845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Проверка доступности GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Используемое устройство: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2866dace",
   "metadata": {},
   "source": [
    "## 3. Загрузка данных\n",
    "**Подставьте свои данные здесь**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbf8e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ВАШИ ДАННЫЕ ===\n",
    "# Вариант 1: Загрузка из CSV/TSV\n",
    "# train_df = pd.read_csv('train.csv')\n",
    "# test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Вариант 2: Загрузка из Hugging Face\n",
    "# dataset = load_dataset(\"dataset_name\")\n",
    "\n",
    "# Вариант 3: Создание своего датасета\n",
    "data = {\n",
    "    'text': [\n",
    "        \"Пример текста для обучения 1\",\n",
    "        \"Пример текста для обучения 2\",\n",
    "        # Добавьте свои данные\n",
    "    ]\n",
    "}\n",
    "train_df = pd.DataFrame(data)\n",
    "\n",
    "# Для fine-tuning в формате инструкций\n",
    "instruction_data = {\n",
    "    'instruction': ['Вопрос 1', 'Вопрос 2'],\n",
    "    'response': ['Ответ 1', 'Ответ 2']\n",
    "}\n",
    "# train_df = pd.DataFrame(instruction_data)\n",
    "\n",
    "print(f\"Размер датасета: {len(train_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be61ad2",
   "metadata": {},
   "source": [
    "## 4. Загрузка модели (Inference режим)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b35541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === НАСТРОЙКИ МОДЕЛИ ===\n",
    "MODEL_NAME = \"microsoft/phi-2\"  # Замените на вашу модель\n",
    "# Примеры: \"meta-llama/Llama-2-7b-hf\", \"mistralai/Mistral-7B-v0.1\", \"google/gemma-7b\"\n",
    "\n",
    "# Квантизация для экономии памяти (опционально)\n",
    "USE_QUANTIZATION = True\n",
    "\n",
    "if USE_QUANTIZATION:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Модель загружена!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6388d111",
   "metadata": {},
   "source": [
    "## 5. Генерация текста (Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd146e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание pipeline для генерации\n",
    "text_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "# === ВАШИ ПРОМПТЫ ===\n",
    "prompts = [\n",
    "    \"Напиши краткое объяснение машинного обучения:\",\n",
    "    \"Что такое нейронные сети?\"\n",
    "]\n",
    "\n",
    "# Генерация\n",
    "for prompt in prompts:\n",
    "    outputs = text_generator(prompt)\n",
    "    print(f\"\\nПромпт: {prompt}\")\n",
    "    print(f\"Ответ: {outputs[0]['generated_text']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e6ebcb",
   "metadata": {},
   "source": [
    "## 6. Fine-tuning с LoRA/QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка модели для обучения с квантизацией\n",
    "if USE_QUANTIZATION:\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Конфигурация LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Ранг матриц LoRA\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Модули для применения LoRA\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04456259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка данных для обучения\n",
    "from datasets import Dataset\n",
    "\n",
    "# Форматирование данных\n",
    "def format_instruction(sample):\n",
    "    return f\"\"\"Инструкция: {sample.get('instruction', sample.get('text', ''))}\n",
    "Ответ: {sample.get('response', '')}\"\"\"\n",
    "\n",
    "# Преобразование в Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "\n",
    "print(f\"Размер тренировочного датасета: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be8f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ПАРАМЕТРЫ ОБУЧЕНИЯ ===\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llm_finetuned\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    warmup_ratio=0.1,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    ")\n",
    "\n",
    "# Тренер\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",  # Укажите название колонки с текстом\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=512,\n",
    ")\n",
    "\n",
    "print(\"Trainer готов!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6541b5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запуск обучения\n",
    "trainer.train()\n",
    "\n",
    "# Сохранение модели\n",
    "trainer.save_model(\"./llm_finetuned_final\")\n",
    "print(\"Модель обучена и сохранена!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d368fe7c",
   "metadata": {},
   "source": [
    "## 7. Inference с обученной моделью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfeb368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка обученной модели\n",
    "from peft import PeftModel\n",
    "\n",
    "# Базовая модель\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Загрузка LoRA адаптера\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, \"./llm_finetuned_final\")\n",
    "finetuned_model = finetuned_model.merge_and_unload()  # Объединение с базовой моделью\n",
    "\n",
    "# Pipeline для генерации\n",
    "finetuned_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=finetuned_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256\n",
    ")\n",
    "\n",
    "print(\"Обученная модель загружена!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ca29a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ТЕСТИРОВАНИЕ ===\n",
    "test_prompts = [\n",
    "    \"Ваш тестовый промпт 1\",\n",
    "    \"Ваш тестовый промпт 2\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    output = finetuned_generator(prompt)\n",
    "    print(f\"\\nПромпт: {prompt}\")\n",
    "    print(f\"Ответ: {output[0]['generated_text']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1223c4",
   "metadata": {},
   "source": [
    "## 8. Создание предсказаний для сабмита"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98604dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ГЕНЕРАЦИЯ ПРЕДСКАЗАНИЙ ===\n",
    "# test_df = pd.read_csv('test.csv')  # Загрузите тестовые данные\n",
    "\n",
    "# predictions = []\n",
    "# for text in test_df['text_column']:\n",
    "#     output = finetuned_generator(text, max_new_tokens=100)\n",
    "#     predictions.append(output[0]['generated_text'])\n",
    "\n",
    "# # Сохранение\n",
    "# submission = pd.DataFrame({\n",
    "#     'id': test_df['id'],\n",
    "#     'prediction': predictions\n",
    "# })\n",
    "# submission.to_csv('llm_submission.csv', index=False)\n",
    "# print(\"Предсказания сохранены!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
