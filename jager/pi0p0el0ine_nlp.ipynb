{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d33ac21e",
   "metadata": {},
   "source": [
    "# NLP Pipeline - Комплексный пайплайн для обработки текста\n",
    "\n",
    "Пайплайн включает:\n",
    "- Text Preprocessing\n",
    "- Feature Extraction (TF-IDF, Word2Vec, BERT)\n",
    "- Text Classification\n",
    "- NER (Named Entity Recognition)\n",
    "- Sentiment Analysis\n",
    "- Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d3782",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy scikit-learn transformers torch nltk spacy gensim textblob -q\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5c52dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from typing import List, Dict\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import spacy\n",
    "\n",
    "# Feature extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Transformers\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoModelForSequenceClassification,\n",
    "    pipeline, Trainer, TrainingArguments\n",
    ")\n",
    "\n",
    "# Sentiment\n",
    "from textblob import TextBlob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Загрузка NLTK данных\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "# Загрузка spaCy модели\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(\"✓ Библиотеки загружены!\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94a009d",
   "metadata": {},
   "source": [
    "## 1. Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a29ebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ВАШИ ДАННЫЕ ===\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "TEXT_COL = 'text'\n",
    "TARGET_COL = 'label'\n",
    "ID_COL = 'id'\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"\\nПервые строки:\")\n",
    "print(train_df.head())\n",
    "\n",
    "# Распределение классов\n",
    "if TARGET_COL in train_df.columns:\n",
    "    print(f\"\\nРаспределение классов:\")\n",
    "    print(train_df[TARGET_COL].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111517aa",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62385d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Комплексная предобработка текста\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 lowercase=True,\n",
    "                 remove_punctuation=True,\n",
    "                 remove_numbers=False,\n",
    "                 remove_stopwords=True,\n",
    "                 lemmatize=True,\n",
    "                 stem=False,\n",
    "                 language='english'):\n",
    "        \n",
    "        self.lowercase = lowercase\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.remove_numbers = remove_numbers\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatize = lemmatize\n",
    "        self.stem = stem\n",
    "        \n",
    "        self.stop_words = set(stopwords.words(language))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stemmer = PorterStemmer()\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Базовая очистка текста\"\"\"\n",
    "        # Lowercase\n",
    "        if self.lowercase:\n",
    "            text = text.lower()\n",
    "        \n",
    "        # Удаление URL\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        \n",
    "        # Удаление email\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # Удаление HTML тегов\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        \n",
    "        # Удаление mentions (@username)\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        \n",
    "        # Удаление hashtags\n",
    "        text = re.sub(r'#\\w+', '', text)\n",
    "        \n",
    "        # Удаление чисел\n",
    "        if self.remove_numbers:\n",
    "            text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Удаление пунктуации\n",
    "        if self.remove_punctuation:\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        # Удаление множественных пробелов\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def tokenize_and_process(self, text: str) -> List[str]:\n",
    "        \"\"\"Токенизация и обработка\"\"\"\n",
    "        # Токенизация\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Удаление stopwords\n",
    "        if self.remove_stopwords:\n",
    "            tokens = [t for t in tokens if t not in self.stop_words]\n",
    "        \n",
    "        # Лемматизация\n",
    "        if self.lemmatize:\n",
    "            tokens = [self.lemmatizer.lemmatize(t) for t in tokens]\n",
    "        \n",
    "        # Стемминг\n",
    "        if self.stem:\n",
    "            tokens = [self.stemmer.stem(t) for t in tokens]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def preprocess(self, text: str, return_tokens=False) -> str:\n",
    "        \"\"\"Полная предобработка\"\"\"\n",
    "        text = self.clean_text(text)\n",
    "        tokens = self.tokenize_and_process(text)\n",
    "        \n",
    "        if return_tokens:\n",
    "            return tokens\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "# Создание препроцессора\n",
    "preprocessor = TextPreprocessor(\n",
    "    lowercase=True,\n",
    "    remove_punctuation=True,\n",
    "    remove_stopwords=True,\n",
    "    lemmatize=True\n",
    ")\n",
    "\n",
    "# Применение к данным\n",
    "print(\"\\nПрименение предобработки...\")\n",
    "train_df['text_clean'] = train_df[TEXT_COL].apply(preprocessor.preprocess)\n",
    "test_df['text_clean'] = test_df[TEXT_COL].apply(preprocessor.preprocess)\n",
    "\n",
    "print(\"\\n✓ Предобработка завершена!\")\n",
    "print(f\"\\nПример:\\nОригинал: {train_df[TEXT_COL].iloc[0]}\")\n",
    "print(f\"Обработанный: {train_df['text_clean'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ead0b1e",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc94a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF векторизация\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 3),  # uni-grams, bi-grams, tri-grams\n",
    "    min_df=2,\n",
    "    max_df=0.9\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(train_df['text_clean'])\n",
    "X_test_tfidf = tfidf.transform(test_df['text_clean'])\n",
    "y_train = train_df[TARGET_COL]\n",
    "\n",
    "print(f\"TF-IDF shape: Train {X_train_tfidf.shape}, Test {X_test_tfidf.shape}\")\n",
    "print(f\"\\nТоп-20 TF-IDF признаков:\")\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "print(feature_names[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682a819c",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e08fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка данных для Word2Vec\n",
    "train_tokens = train_df['text_clean'].apply(lambda x: x.split()).tolist()\n",
    "test_tokens = test_df['text_clean'].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "# Обучение Word2Vec\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=train_tokens,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "def get_w2v_features(tokens, model, vector_size=100):\n",
    "    \"\"\"Усреднение Word2Vec векторов для документа\"\"\"\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        if token in model.wv:\n",
    "            vectors.append(model.wv[token])\n",
    "    \n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    return np.zeros(vector_size)\n",
    "\n",
    "X_train_w2v = np.array([get_w2v_features(tokens, w2v_model) for tokens in train_tokens])\n",
    "X_test_w2v = np.array([get_w2v_features(tokens, w2v_model) for tokens in test_tokens])\n",
    "\n",
    "print(f\"✓ Word2Vec готов! Shape: Train {X_train_w2v.shape}, Test {X_test_w2v.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dad244",
   "metadata": {},
   "source": [
    "## 5. Feature Extraction - BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba49929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка BERT модели\n",
    "bert_model_name = 'bert-base-uncased'  # или 'distilbert-base-uncased' для скорости\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = AutoModel.from_pretrained(bert_model_name)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bert_model = bert_model.to(device)\n",
    "bert_model.eval()\n",
    "\n",
    "def get_bert_embedding(text, tokenizer, model, max_length=128):\n",
    "    \"\"\"Получение BERT embedding для текста\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Используем [CLS] token embedding\n",
    "    embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    return embedding[0]\n",
    "\n",
    "# Извлечение embeddings (для примера - первые 100 строк)\n",
    "print(\"Извлечение BERT embeddings...\")\n",
    "sample_size = min(100, len(train_df))\n",
    "X_train_bert_sample = np.array([\n",
    "    get_bert_embedding(text, tokenizer, bert_model) \n",
    "    for text in train_df[TEXT_COL][:sample_size]\n",
    "])\n",
    "\n",
    "print(f\"✓ BERT embeddings готовы! Shape: {X_train_bert_sample.shape}\")\n",
    "print(\"Для полного датасета используйте батч-обработку\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a765c8",
   "metadata": {},
   "source": [
    "## 6. Text Classification - Traditional ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a7cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбиение на train/val\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train_tfidf, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# Модели\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nОбучение {name}...\")\n",
    "    model.fit(X_tr, y_tr)\n",
    "    \n",
    "    # Предсказания\n",
    "    val_pred = model.predict(X_val)\n",
    "    acc = accuracy_score(y_val, val_pred)\n",
    "    results[name] = acc\n",
    "    \n",
    "    print(f\"{name} Validation Accuracy: {acc:.4f}\")\n",
    "    print(f\"\\nClassification Report:\\n{classification_report(y_val, val_pred)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"СРАВНЕНИЕ МОДЕЛЕЙ\")\n",
    "print(\"=\"*60)\n",
    "for name, acc in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{name}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6185bfb",
   "metadata": {},
   "source": [
    "## 7. Text Classification - BERT Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe094c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Энкодинг меток\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['label_encoded'] = label_encoder.fit_transform(train_df[TARGET_COL])\n",
    "num_labels = len(label_encoder.classes_)\n",
    "\n",
    "class TextClassificationDataset(TorchDataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Создание datasets\n",
    "train_texts = train_df[TEXT_COL].tolist()[:1000]  # Для примера - первые 1000\n",
    "train_labels = train_df['label_encoded'].tolist()[:1000]\n",
    "\n",
    "train_dataset = TextClassificationDataset(\n",
    "    train_texts, train_labels, tokenizer\n",
    ")\n",
    "\n",
    "print(f\"✓ Dataset готов! Размер: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044896df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка модели для классификации\n",
    "bert_classifier = AutoModelForSequenceClassification.from_pretrained(\n",
    "    bert_model_name,\n",
    "    num_labels=num_labels\n",
    ").to(device)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./bert_classifier',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    logging_steps=50,\n",
    "    save_strategy='epoch',\n",
    "    evaluation_strategy='epoch'\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=bert_classifier,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset\n",
    ")\n",
    "\n",
    "print(\"\\nЗапуск обучения BERT...\")\n",
    "# trainer.train()  # Раскомментируйте для обучения\n",
    "print(\"Обучение закомментировано. Раскомментируйте для запуска.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6e7abc",
   "metadata": {},
   "source": [
    "## 8. Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a0ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER с spaCy\n",
    "def extract_entities(text):\n",
    "    \"\"\"Извлечение именованных сущностей\"\"\"\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "# Пример\n",
    "sample_text = train_df[TEXT_COL].iloc[0]\n",
    "entities = extract_entities(sample_text)\n",
    "\n",
    "print(f\"Текст: {sample_text}\\n\")\n",
    "print(\"Найденные сущности:\")\n",
    "for entity, label in entities:\n",
    "    print(f\"  {entity} -> {label}\")\n",
    "\n",
    "# NER с transformers\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"dslim/bert-base-NER\",\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "ner_results = ner_pipeline(sample_text)\n",
    "print(\"\\nNER с BERT:\")\n",
    "for entity in ner_results:\n",
    "    print(f\"  {entity['word']} -> {entity['entity_group']} (score: {entity['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86836a1e",
   "metadata": {},
   "source": [
    "## 9. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdafd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment с TextBlob\n",
    "def get_sentiment_textblob(text):\n",
    "    \"\"\"Анализ sentiment с TextBlob\"\"\"\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity  # -1 (negative) to 1 (positive)\n",
    "    subjectivity = blob.sentiment.subjectivity  # 0 (objective) to 1 (subjective)\n",
    "    \n",
    "    if polarity > 0:\n",
    "        sentiment = 'positive'\n",
    "    elif polarity < 0:\n",
    "        sentiment = 'negative'\n",
    "    else:\n",
    "        sentiment = 'neutral'\n",
    "    \n",
    "    return {\n",
    "        'sentiment': sentiment,\n",
    "        'polarity': polarity,\n",
    "        'subjectivity': subjectivity\n",
    "    }\n",
    "\n",
    "# Sentiment с transformers\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "# Примеры\n",
    "sample_texts = [\n",
    "    \"This is an amazing product! I love it!\",\n",
    "    \"Terrible experience. Would not recommend.\",\n",
    "    \"It's okay, nothing special.\"\n",
    "]\n",
    "\n",
    "print(\"Sentiment Analysis:\\n\")\n",
    "for text in sample_texts:\n",
    "    textblob_result = get_sentiment_textblob(text)\n",
    "    transformer_result = sentiment_pipeline(text)[0]\n",
    "    \n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"  TextBlob: {textblob_result['sentiment']} (polarity: {textblob_result['polarity']:.2f})\")\n",
    "    print(f\"  Transformer: {transformer_result['label']} (score: {transformer_result['score']:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6157c66d",
   "metadata": {},
   "source": [
    "## 10. Additional Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd17d80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_features(text):\n",
    "    \"\"\"\n",
    "    Извлечение статистических признаков текста\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Длина текста\n",
    "    features['char_count'] = len(text)\n",
    "    features['word_count'] = len(text.split())\n",
    "    features['sentence_count'] = len(sent_tokenize(text))\n",
    "    \n",
    "    # Средние длины\n",
    "    words = text.split()\n",
    "    features['avg_word_length'] = np.mean([len(w) for w in words]) if words else 0\n",
    "    \n",
    "    # Количество знаков препинания\n",
    "    features['punctuation_count'] = sum([1 for c in text if c in string.punctuation])\n",
    "    \n",
    "    # Количество заглавных букв\n",
    "    features['upper_count'] = sum([1 for c in text if c.isupper()])\n",
    "    \n",
    "    # Sentiment\n",
    "    blob = TextBlob(text)\n",
    "    features['polarity'] = blob.sentiment.polarity\n",
    "    features['subjectivity'] = blob.sentiment.subjectivity\n",
    "    \n",
    "    # Уникальные слова\n",
    "    features['unique_word_ratio'] = len(set(words)) / len(words) if words else 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Применение к датасету\n",
    "print(\"Извлечение дополнительных признаков...\")\n",
    "text_features_train = train_df[TEXT_COL].apply(extract_text_features)\n",
    "text_features_test = test_df[TEXT_COL].apply(extract_text_features)\n",
    "\n",
    "# Преобразование в DataFrame\n",
    "features_train_df = pd.DataFrame(text_features_train.tolist())\n",
    "features_test_df = pd.DataFrame(text_features_test.tolist())\n",
    "\n",
    "print(f\"\\n✓ Признаки извлечены! Shape: {features_train_df.shape}\")\n",
    "print(f\"\\nПризнаки: {features_train_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2739f053",
   "metadata": {},
   "source": [
    "## 11. Ensemble с разными фичами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd908cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# Комбинирование признаков\n",
    "# TF-IDF + Word2Vec + Text features\n",
    "X_train_combined = hstack([\n",
    "    X_train_tfidf,\n",
    "    X_train_w2v,\n",
    "    features_train_df.values\n",
    "])\n",
    "\n",
    "X_test_combined = hstack([\n",
    "    X_test_tfidf,\n",
    "    X_test_w2v,\n",
    "    features_test_df.values\n",
    "])\n",
    "\n",
    "print(f\"Combined features shape: Train {X_train_combined.shape}, Test {X_test_combined.shape}\")\n",
    "\n",
    "# Обучение на комбинированных признаках\n",
    "lr_combined = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_combined.fit(X_train_combined, y_train)\n",
    "\n",
    "# Предсказания\n",
    "test_predictions = lr_combined.predict(X_test_combined)\n",
    "\n",
    "print(\"\\n✓ Модель на комбинированных признаках обучена!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae42025c",
   "metadata": {},
   "source": [
    "## 12. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca257a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем лучшую модель\n",
    "best_model = models['Logistic Regression']  # Замените на лучшую\n",
    "final_predictions = best_model.predict(X_test_tfidf)\n",
    "\n",
    "# Декодирование меток если нужно\n",
    "# final_predictions = label_encoder.inverse_transform(final_predictions)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    ID_COL: test_df[ID_COL],\n",
    "    'prediction': final_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('nlp_submission.csv', index=False)\n",
    "print(\"\\n✓ Submission сохранен!\")\n",
    "print(submission.head())\n",
    "print(f\"\\nРаспределение предсказаний:\")\n",
    "print(submission['prediction'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
