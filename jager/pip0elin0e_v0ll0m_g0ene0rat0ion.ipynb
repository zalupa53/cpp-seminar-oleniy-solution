{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78204ffa",
   "metadata": {},
   "source": [
    "# Pipeline: vLLM Text Generation —Å –ë–∞—Ç—á–∞–º–∏\n",
    "\n",
    "–≠—Ç–æ—Ç notebook –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å vLLM –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –±–∞—Ç—á–µ–π.\n",
    "\n",
    "## –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:\n",
    "- **–í—ã—Å–æ–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å**: vLLM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç PagedAttention –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é\n",
    "- **–ë–∞—Ç—á–∏–Ω–≥**: –û–±—Ä–∞–±–æ—Ç–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ\n",
    "- **Continuous batching**: –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –±–∞—Ç—á\n",
    "- **–ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π**: LLaMA, Mistral, Qwen, Phi –∏ –¥—Ä—É–≥–∏–µ\n",
    "- **Sampling –ø–∞—Ä–∞–º–µ—Ç—Ä—ã**: Temperature, top-p, top-k, beam search\n",
    "\n",
    "## –£—Å—Ç–∞–Ω–æ–≤–∫–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5162b041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ vLLM\n",
    "!pip install vllm -q\n",
    "!pip install transformers accelerate -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8b5255",
   "metadata": {},
   "source": [
    "## 1. –ò–º–ø–æ—Ä—Ç—ã –∏ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1003ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Union\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import json\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.outputs import RequestOutput\n",
    "\n",
    "# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è\n",
    "MODEL_NAME = \"microsoft/phi-2\"  # –ú–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞: \"mistralai/Mistral-7B-v0.1\", \"meta-llama/Llama-2-7b-hf\", \"Qwen/Qwen-7B\"\n",
    "MAX_MODEL_LEN = 2048  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n",
    "TENSOR_PARALLEL_SIZE = 1  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU –¥–ª—è —Ç–µ–Ω–∑–æ—Ä–Ω–æ–≥–æ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞\n",
    "GPU_MEMORY_UTILIZATION = 0.9  # –ü—Ä–æ—Ü–µ–Ω—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU –ø–∞–º—è—Ç–∏\n",
    "\n",
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\n",
    "DEFAULT_TEMPERATURE = 0.7\n",
    "DEFAULT_TOP_P = 0.9\n",
    "DEFAULT_TOP_K = 50\n",
    "DEFAULT_MAX_TOKENS = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e707282e",
   "metadata": {},
   "source": [
    "## 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è vLLM –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382daa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_vllm_model(\n",
    "    model_name: str = MODEL_NAME,\n",
    "    max_model_len: int = MAX_MODEL_LEN,\n",
    "    tensor_parallel_size: int = TENSOR_PARALLEL_SIZE,\n",
    "    gpu_memory_utilization: float = GPU_MEMORY_UTILIZATION,\n",
    "    trust_remote_code: bool = True\n",
    ") -> LLM:\n",
    "    \"\"\"\n",
    "    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç vLLM –º–æ–¥–µ–ª—å\n",
    "    \n",
    "    Args:\n",
    "        model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏–∑ HuggingFace\n",
    "        max_model_len: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n",
    "        tensor_parallel_size: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU\n",
    "        gpu_memory_utilization: –ü—Ä–æ—Ü–µ–Ω—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU –ø–∞–º—è—Ç–∏ (0.0-1.0)\n",
    "        trust_remote_code: –î–æ–≤–µ—Ä—è—Ç—å –ª–∏ remote code\n",
    "    \n",
    "    Returns:\n",
    "        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è LLM –º–æ–¥–µ–ª—å\n",
    "    \"\"\"\n",
    "    print(f\"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}...\")\n",
    "    \n",
    "    llm = LLM(\n",
    "        model=model_name,\n",
    "        max_model_len=max_model_len,\n",
    "        tensor_parallel_size=tensor_parallel_size,\n",
    "        gpu_memory_utilization=gpu_memory_utilization,\n",
    "        trust_remote_code=trust_remote_code,\n",
    "    )\n",
    "    \n",
    "    print(f\"–ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!\")\n",
    "    return llm\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏\n",
    "llm = initialize_vllm_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcefc5aa",
   "metadata": {},
   "source": [
    "## 3. –°–æ–∑–¥–∞–Ω–∏–µ Sampling Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707f0c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sampling_params(\n",
    "    temperature: float = DEFAULT_TEMPERATURE,\n",
    "    top_p: float = DEFAULT_TOP_P,\n",
    "    top_k: int = DEFAULT_TOP_K,\n",
    "    max_tokens: int = DEFAULT_MAX_TOKENS,\n",
    "    n: int = 1,\n",
    "    stop: Optional[List[str]] = None,\n",
    "    presence_penalty: float = 0.0,\n",
    "    frequency_penalty: float = 0.0,\n",
    "    repetition_penalty: float = 1.0,\n",
    "    use_beam_search: bool = False,\n",
    "    best_of: Optional[int] = None,\n",
    ") -> SamplingParams:\n",
    "    \"\"\"\n",
    "    –°–æ–∑–¥–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "    \n",
    "    Args:\n",
    "        temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è (–≤—ã—à–µ = –±–æ–ª–µ–µ —Å–ª—É—á–∞–π–Ω—ã–π –≤—ã–≤–æ–¥)\n",
    "        top_p: Nucleus sampling (—Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è —Ç–æ–∫–µ–Ω—ã —Å —Å—É–º–º–∞—Ä–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é top_p)\n",
    "        top_k: Top-k sampling (—Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è top_k –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤)\n",
    "        max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
    "        n: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–π –Ω–∞ –æ–¥–∏–Ω –ø—Ä–æ–º–ø—Ç\n",
    "        stop: –°–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π\n",
    "        presence_penalty: –®—Ç—Ä–∞—Ñ –∑–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ (-2.0 to 2.0)\n",
    "        frequency_penalty: –®—Ç—Ä–∞—Ñ –∑–∞ —á–∞—Å—Ç–æ—Ç—É —Ç–æ–∫–µ–Ω–æ–≤ (-2.0 to 2.0)\n",
    "        repetition_penalty: –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ (1.0 = –Ω–µ—Ç —à—Ç—Ä–∞—Ñ–∞)\n",
    "        use_beam_search: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ beam search\n",
    "        best_of: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –≤—ã–±–æ—Ä–∞ –ª—É—á—à–µ–≥–æ (–¥–ª—è beam search)\n",
    "    \n",
    "    Returns:\n",
    "        SamplingParams –æ–±—ä–µ–∫—Ç\n",
    "    \"\"\"\n",
    "    return SamplingParams(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        max_tokens=max_tokens,\n",
    "        n=n,\n",
    "        stop=stop,\n",
    "        presence_penalty=presence_penalty,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        use_beam_search=use_beam_search,\n",
    "        best_of=best_of,\n",
    "    )\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä—ã —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π\n",
    "sampling_params_creative = create_sampling_params(\n",
    "    temperature=1.0,\n",
    "    top_p=0.95,\n",
    "    max_tokens=512\n",
    ")\n",
    "\n",
    "sampling_params_precise = create_sampling_params(\n",
    "    temperature=0.1,\n",
    "    top_p=0.9,\n",
    "    max_tokens=256\n",
    ")\n",
    "\n",
    "sampling_params_beam = create_sampling_params(\n",
    "    temperature=0.0,\n",
    "    use_beam_search=True,\n",
    "    best_of=5,\n",
    "    n=1,\n",
    "    max_tokens=256\n",
    ")\n",
    "\n",
    "print(\"Sampling parameters —Å–æ–∑–¥–∞–Ω—ã!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19446a37",
   "metadata": {},
   "source": [
    "## 4. –ü—Ä–æ—Å—Ç–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è (Single Prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a590ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_single(\n",
    "    llm: LLM,\n",
    "    prompt: str,\n",
    "    sampling_params: Optional[SamplingParams] = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ–¥–∏–Ω –ø—Ä–æ–º–ø—Ç\n",
    "    \n",
    "    Args:\n",
    "        llm: vLLM –º–æ–¥–µ–ª—å\n",
    "        prompt: –¢–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞\n",
    "        sampling_params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    "    \n",
    "    Returns:\n",
    "        –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
    "    \"\"\"\n",
    "    if sampling_params is None:\n",
    "        sampling_params = create_sampling_params()\n",
    "    \n",
    "    outputs = llm.generate([prompt], sampling_params)\n",
    "    return outputs[0].outputs[0].text\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "test_prompt = \"Explain quantum computing in simple terms:\"\n",
    "response = generate_single(llm, test_prompt, sampling_params_precise)\n",
    "print(f\"Prompt: {test_prompt}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc8243e",
   "metadata": {},
   "source": [
    "## 5. –ë–∞—Ç—á-–≥–µ–Ω–µ—Ä–∞—Ü–∏—è (Multiple Prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6e479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(\n",
    "    llm: LLM,\n",
    "    prompts: List[str],\n",
    "    sampling_params: Optional[SamplingParams] = None,\n",
    "    show_progress: bool = True\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã –Ω–∞ –±–∞—Ç—á –ø—Ä–æ–º–ø—Ç–æ–≤\n",
    "    \n",
    "    Args:\n",
    "        llm: vLLM –º–æ–¥–µ–ª—å\n",
    "        prompts: –°–ø–∏—Å–æ–∫ –ø—Ä–æ–º–ø—Ç–æ–≤\n",
    "        sampling_params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    "        show_progress: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –ª–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å\n",
    "    \n",
    "    Returns:\n",
    "        –°–ø–∏—Å–æ–∫ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤\n",
    "    \"\"\"\n",
    "    if sampling_params is None:\n",
    "        sampling_params = create_sampling_params()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    if show_progress:\n",
    "        print(f\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª—è {len(prompts)} –ø—Ä–æ–º–ø—Ç–æ–≤...\")\n",
    "    \n",
    "    # vLLM –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞—Ç—á–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    \n",
    "    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤\n",
    "    generated_texts = [output.outputs[0].text for output in outputs]\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    if show_progress:\n",
    "        print(f\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {elapsed_time:.2f}s\")\n",
    "        print(f\"–°–∫–æ—Ä–æ—Å—Ç—å: {len(prompts)/elapsed_time:.2f} –ø—Ä–æ–º–ø—Ç–æ–≤/—Å–µ–∫\")\n",
    "    \n",
    "    return generated_texts\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "test_prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Explain neural networks.\",\n",
    "    \"What is deep learning?\",\n",
    "    \"Define artificial intelligence.\",\n",
    "    \"What are transformers in NLP?\"\n",
    "]\n",
    "\n",
    "batch_responses = generate_batch(llm, test_prompts, sampling_params_precise)\n",
    "\n",
    "for prompt, response in zip(test_prompts, batch_responses):\n",
    "    print(f\"\\nQ: {prompt}\")\n",
    "    print(f\"A: {response[:100]}...\")  # –ü–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af6ac22",
   "metadata": {},
   "source": [
    "## 6. –ë–∞—Ç—á-–≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b8adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_with_params(\n",
    "    llm: LLM,\n",
    "    prompts: List[str],\n",
    "    sampling_params_list: Optional[List[SamplingParams]] = None,\n",
    "    show_progress: bool = True\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã —Å –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞\n",
    "    \n",
    "    Args:\n",
    "        llm: vLLM –º–æ–¥–µ–ª—å\n",
    "        prompts: –°–ø–∏—Å–æ–∫ –ø—Ä–æ–º–ø—Ç–æ–≤\n",
    "        sampling_params_list: –°–ø–∏—Å–æ–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞\n",
    "        show_progress: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –ª–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å\n",
    "    \n",
    "    Returns:\n",
    "        –°–ø–∏—Å–æ–∫ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤\n",
    "    \"\"\"\n",
    "    if sampling_params_list is None:\n",
    "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–¥–∏–Ω –Ω–∞–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –≤—Å–µ—Ö\n",
    "        return generate_batch(llm, prompts, show_progress=show_progress)\n",
    "    \n",
    "    assert len(prompts) == len(sampling_params_list), \"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–æ–ª–∂–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    if show_progress:\n",
    "        print(f\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª—è {len(prompts)} –ø—Ä–æ–º–ø—Ç–æ–≤ —Å –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏...\")\n",
    "    \n",
    "    # vLLM –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤\n",
    "    generated_texts = []\n",
    "    \n",
    "    for i, (prompt, params) in enumerate(zip(prompts, sampling_params_list)):\n",
    "        outputs = llm.generate([prompt], params)\n",
    "        generated_texts.append(outputs[0].outputs[0].text)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    if show_progress:\n",
    "        print(f\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {elapsed_time:.2f}s\")\n",
    "    \n",
    "    return generated_texts\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä: —Ä–∞–∑–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∑–∞–¥–∞—á\n",
    "mixed_prompts = [\n",
    "    \"Write a creative story about a robot:\",  # –¢–≤–æ—Ä—á–µ—Å–∫–∞—è –∑–∞–¥–∞—á–∞\n",
    "    \"What is 2+2?\",  # –¢–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç\n",
    "    \"Explain the theory of relativity:\",  # –ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–∞—è –∑–∞–¥–∞—á–∞\n",
    "]\n",
    "\n",
    "mixed_params = [\n",
    "    create_sampling_params(temperature=1.0, max_tokens=200),  # –í—ã—Å–æ–∫–∞—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å\n",
    "    create_sampling_params(temperature=0.1, max_tokens=50),   # –ù–∏–∑–∫–∞—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å\n",
    "    create_sampling_params(temperature=0.5, max_tokens=300),  # –°—Ä–µ–¥–Ω—è—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å\n",
    "]\n",
    "\n",
    "mixed_responses = generate_batch_with_params(llm, mixed_prompts, mixed_params)\n",
    "\n",
    "for prompt, response in zip(mixed_prompts, mixed_responses):\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Response: {response[:150]}...\")  # –ü–µ—Ä–≤—ã–µ 150 —Å–∏–º–≤–æ–ª–æ–≤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940e99cc",
   "metadata": {},
   "source": [
    "## 7. –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ (Competition Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4171d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_in_batches(\n",
    "    llm: LLM,\n",
    "    df: pd.DataFrame,\n",
    "    prompt_column: str,\n",
    "    batch_size: int = 32,\n",
    "    sampling_params: Optional[SamplingParams] = None,\n",
    "    output_column: str = \"generated_text\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –±–∞—Ç—á–∞–º–∏ –¥–ª—è —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–π\n",
    "    \n",
    "    Args:\n",
    "        llm: vLLM –º–æ–¥–µ–ª—å\n",
    "        df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏\n",
    "        prompt_column: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –ø—Ä–æ–º–ø—Ç–∞–º–∏\n",
    "        batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞\n",
    "        sampling_params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    "        output_column: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω–æ–π –∫–æ–ª–æ–Ω–∫–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "    \"\"\"\n",
    "    if sampling_params is None:\n",
    "        sampling_params = create_sampling_params()\n",
    "    \n",
    "    df = df.copy()\n",
    "    prompts = df[prompt_column].tolist()\n",
    "    all_generated = []\n",
    "    \n",
    "    print(f\"–û–±—Ä–∞–±–æ—Ç–∫–∞ {len(prompts)} –ø—Ä–æ–º–ø—Ç–æ–≤ –±–∞—Ç—á–∞–º–∏ –ø–æ {batch_size}...\")\n",
    "    \n",
    "    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞–º–∏\n",
    "    for i in tqdm(range(0, len(prompts), batch_size)):\n",
    "        batch_prompts = prompts[i:i+batch_size]\n",
    "        batch_outputs = llm.generate(batch_prompts, sampling_params)\n",
    "        batch_texts = [output.outputs[0].text for output in batch_outputs]\n",
    "        all_generated.extend(batch_texts)\n",
    "    \n",
    "    df[output_column] = all_generated\n",
    "    return df\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä: –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "sample_data = {\n",
    "    'id': range(10),\n",
    "    'prompt': [\n",
    "        \"Summarize: Machine learning is a subset of AI\",\n",
    "        \"Translate to French: Hello world\",\n",
    "        \"Question: What is Python? Answer:\",\n",
    "        \"Complete: The capital of France is\",\n",
    "        \"Classify sentiment: This movie is great!\",\n",
    "        \"Extract entities: John works at Google in California\",\n",
    "        \"Paraphrase: AI is changing the world\",\n",
    "        \"Generate title: Article about climate change\",\n",
    "        \"Answer: What is the speed of light?\",\n",
    "        \"Simplify: Quantum entanglement is a phenomenon\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_test = pd.DataFrame(sample_data)\n",
    "print(\"–ò—Å—Ö–æ–¥–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç:\")\n",
    "print(df_test.head())\n",
    "\n",
    "# –û–±—Ä–∞–±–æ—Ç–∫–∞\n",
    "df_results = process_dataset_in_batches(\n",
    "    llm, \n",
    "    df_test, \n",
    "    prompt_column='prompt',\n",
    "    batch_size=4,\n",
    "    sampling_params=sampling_params_precise\n",
    ")\n",
    "\n",
    "print(\"\\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã:\")\n",
    "print(df_results[['id', 'prompt', 'generated_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbd31c0",
   "metadata": {},
   "source": [
    "## 8. –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤ —Å —à–∞–±–ª–æ–Ω–∞–º–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e5530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompts_from_template(\n",
    "    df: pd.DataFrame,\n",
    "    template: str,\n",
    "    column_mapping: Dict[str, str]\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    –°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º–ø—Ç—ã –∏–∑ —à–∞–±–ª–æ–Ω–∞ –∏ –¥–∞–Ω–Ω—ã—Ö\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏\n",
    "        template: –®–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç–∞ —Å –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–∞–º–∏ {column_name}\n",
    "        column_mapping: –ú–∞–ø–ø–∏–Ω–≥ –∫–æ–ª–æ–Ω–æ–∫ DataFrame –Ω–∞ –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã\n",
    "    \n",
    "    Returns:\n",
    "        –°–ø–∏—Å–æ–∫ –ø—Ä–æ–º–ø—Ç–æ–≤\n",
    "    \"\"\"\n",
    "    prompts = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        prompt = template\n",
    "        for placeholder, column in column_mapping.items():\n",
    "            prompt = prompt.replace(f\"{{{placeholder}}}\", str(row[column]))\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    return prompts\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä —à–∞–±–ª–æ–Ω–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á\n",
    "TEMPLATES = {\n",
    "    'classification': \"\"\"Classify the following text into one of these categories: {categories}\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Category:\"\"\",\n",
    "    \n",
    "    'summarization': \"\"\"Summarize the following text in {num_sentences} sentences:\n",
    "\n",
    "{text}\n",
    "\n",
    "Summary:\"\"\",\n",
    "    \n",
    "    'question_answering': \"\"\"Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\",\n",
    "    \n",
    "    'translation': \"\"\"Translate the following text from {source_lang} to {target_lang}:\n",
    "\n",
    "{text}\n",
    "\n",
    "Translation:\"\"\",\n",
    "    \n",
    "    'text_generation': \"\"\"Complete the following text:\n",
    "\n",
    "{text}\n",
    "\n",
    "Continuation:\"\"\"\n",
    "}\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "classification_data = pd.DataFrame({\n",
    "    'text': [\n",
    "        \"This product is amazing! I love it.\",\n",
    "        \"Terrible service, would not recommend.\",\n",
    "        \"It's okay, nothing special.\"\n",
    "    ],\n",
    "    'categories': ['positive, negative, neutral'] * 3\n",
    "})\n",
    "\n",
    "classification_prompts = create_prompts_from_template(\n",
    "    classification_data,\n",
    "    TEMPLATES['classification'],\n",
    "    {'text': 'text', 'categories': 'categories'}\n",
    ")\n",
    "\n",
    "print(\"–ü—Ä–∏–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:\")\n",
    "print(classification_prompts[0])\n",
    "\n",
    "# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è\n",
    "classification_results = generate_batch(\n",
    "    llm, \n",
    "    classification_prompts, \n",
    "    sampling_params_precise,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "classification_data['predicted_category'] = classification_results\n",
    "print(\"\\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:\")\n",
    "print(classification_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafa4962",
   "metadata": {},
   "source": [
    "## 9. Multiple Generations (N>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadb1592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiple_candidates(\n",
    "    llm: LLM,\n",
    "    prompts: List[str],\n",
    "    n: int = 3,\n",
    "    sampling_params: Optional[SamplingParams] = None\n",
    ") -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞\n",
    "    \n",
    "    Args:\n",
    "        llm: vLLM –º–æ–¥–µ–ª—å\n",
    "        prompts: –°–ø–∏—Å–æ–∫ –ø—Ä–æ–º–ø—Ç–æ–≤\n",
    "        n: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ –ø—Ä–æ–º–ø—Ç\n",
    "        sampling_params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    "    \n",
    "    Returns:\n",
    "        –°–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤\n",
    "    \"\"\"\n",
    "    if sampling_params is None:\n",
    "        sampling_params = create_sampling_params(n=n, temperature=0.8)\n",
    "    else:\n",
    "        # –û–±–Ω–æ–≤–ª—è–µ–º n –≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö\n",
    "        sampling_params.n = n\n",
    "    \n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    \n",
    "    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø—Ä–æ–º–ø—Ç–∞–º\n",
    "    all_candidates = []\n",
    "    for output in outputs:\n",
    "        candidates = [gen.text for gen in output.outputs]\n",
    "        all_candidates.append(candidates)\n",
    "    \n",
    "    return all_candidates\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞\n",
    "test_prompts = [\n",
    "    \"Write a tagline for an AI company:\",\n",
    "    \"Create a catchy name for a coffee shop:\"\n",
    "]\n",
    "\n",
    "candidates = generate_multiple_candidates(\n",
    "    llm, \n",
    "    test_prompts, \n",
    "    n=5,\n",
    "    sampling_params=create_sampling_params(temperature=0.9, max_tokens=50)\n",
    ")\n",
    "\n",
    "for i, (prompt, prompt_candidates) in enumerate(zip(test_prompts, candidates)):\n",
    "    print(f\"\\nPrompt {i+1}: {prompt}\")\n",
    "    for j, candidate in enumerate(prompt_candidates):\n",
    "        print(f\"  Candidate {j+1}: {candidate.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44644ad",
   "metadata": {},
   "source": [
    "## 10. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01f52cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(\n",
    "    df: pd.DataFrame,\n",
    "    output_path: str,\n",
    "    format: str = 'csv'\n",
    "):\n",
    "    \"\"\"\n",
    "    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏\n",
    "        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\n",
    "        format: –§–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞ ('csv', 'json', 'parquet')\n",
    "    \"\"\"\n",
    "    if format == 'csv':\n",
    "        df.to_csv(output_path, index=False)\n",
    "    elif format == 'json':\n",
    "        df.to_json(output_path, orient='records', lines=True)\n",
    "    elif format == 'parquet':\n",
    "        df.to_parquet(output_path, index=False)\n",
    "    else:\n",
    "        raise ValueError(f\"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {format}\")\n",
    "    \n",
    "    print(f\"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_path}\")\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "save_results(df_results, 'vllm_generation_results.csv', format='csv')\n",
    "\n",
    "# –¢–∞–∫–∂–µ –º–æ–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    "metadata = {\n",
    "    'model': MODEL_NAME,\n",
    "    'timestamp': pd.Timestamp.now().isoformat(),\n",
    "    'num_prompts': len(df_results),\n",
    "    'sampling_params': {\n",
    "        'temperature': DEFAULT_TEMPERATURE,\n",
    "        'top_p': DEFAULT_TOP_P,\n",
    "        'max_tokens': DEFAULT_MAX_TOKENS\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('vllm_generation_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc1b74d",
   "metadata": {},
   "source": [
    "## 11. –ü–æ–ª–Ω—ã–π Pipeline –¥–ª—è –°–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baca2d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLLMCompetitionPipeline:\n",
    "    \"\"\"\n",
    "    –ü–æ–ª–Ω—ã–π pipeline –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è—Ö\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = MODEL_NAME,\n",
    "        max_model_len: int = MAX_MODEL_LEN,\n",
    "        gpu_memory_utilization: float = GPU_MEMORY_UTILIZATION\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.llm = initialize_vllm_model(\n",
    "            model_name=model_name,\n",
    "            max_model_len=max_model_len,\n",
    "            gpu_memory_utilization=gpu_memory_utilization\n",
    "        )\n",
    "        \n",
    "    def prepare_prompts(\n",
    "        self, \n",
    "        df: pd.DataFrame, \n",
    "        template: str,\n",
    "        column_mapping: Dict[str, str]\n",
    "    ) -> List[str]:\n",
    "        \"\"\"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞\"\"\"\n",
    "        return create_prompts_from_template(df, template, column_mapping)\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        batch_size: int = 32,\n",
    "        sampling_params: Optional[SamplingParams] = None\n",
    "    ) -> List[str]:\n",
    "        \"\"\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±–∞—Ç—á–∞–º–∏\"\"\"\n",
    "        if sampling_params is None:\n",
    "            sampling_params = create_sampling_params()\n",
    "        \n",
    "        all_outputs = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(prompts), batch_size), desc=\"Generating\"):\n",
    "            batch = prompts[i:i+batch_size]\n",
    "            outputs = self.llm.generate(batch, sampling_params)\n",
    "            texts = [out.outputs[0].text for out in outputs]\n",
    "            all_outputs.extend(texts)\n",
    "        \n",
    "        return all_outputs\n",
    "    \n",
    "    def process_and_save(\n",
    "        self,\n",
    "        train_path: str,\n",
    "        test_path: str,\n",
    "        template: str,\n",
    "        column_mapping: Dict[str, str],\n",
    "        output_path: str = 'submission.csv',\n",
    "        batch_size: int = 32,\n",
    "        sampling_params: Optional[SamplingParams] = None\n",
    "    ):\n",
    "        \"\"\"–ü–æ–ª–Ω—ã–π pipeline: –∑–∞–≥—Ä—É–∑–∫–∞ -> –æ–±—Ä–∞–±–æ—Ç–∫–∞ -> —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\"\"\"\n",
    "        \n",
    "        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "        print(\"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "        test_df = pd.read_csv(test_path)\n",
    "        \n",
    "        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤\n",
    "        print(\"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤...\")\n",
    "        prompts = self.prepare_prompts(test_df, template, column_mapping)\n",
    "        \n",
    "        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è\n",
    "        print(f\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª—è {len(prompts)} –ø—Ä–∏–º–µ—Ä–æ–≤...\")\n",
    "        predictions = self.generate(prompts, batch_size, sampling_params)\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–Ω–∏–µ submission\n",
    "        print(\"–°–æ–∑–¥–∞–Ω–∏–µ submission —Ñ–∞–π–ª–∞...\")\n",
    "        submission_df = pd.DataFrame({\n",
    "            'id': test_df['id'] if 'id' in test_df.columns else range(len(test_df)),\n",
    "            'prediction': predictions\n",
    "        })\n",
    "        \n",
    "        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\n",
    "        submission_df.to_csv(output_path, index=False)\n",
    "        print(f\"Submission —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {output_path}\")\n",
    "        \n",
    "        return submission_df\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è pipeline\n",
    "print(\"\\n=== –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Competition Pipeline ===\")\n",
    "pipeline = VLLMCompetitionPipeline(model_name=MODEL_NAME)\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "# pipeline.process_and_save(\n",
    "#     train_path='train.csv',\n",
    "#     test_path='test.csv',\n",
    "#     template=TEMPLATES['classification'],\n",
    "#     column_mapping={'text': 'text', 'categories': 'categories'},\n",
    "#     output_path='submission.csv',\n",
    "#     batch_size=32,\n",
    "#     sampling_params=create_sampling_params(temperature=0.3)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e430df",
   "metadata": {},
   "source": [
    "## 12. Benchmark –∏ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d724e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_generation(\n",
    "    llm: LLM,\n",
    "    num_prompts: int = 100,\n",
    "    batch_sizes: List[int] = [1, 4, 8, 16, 32],\n",
    "    prompt_length: int = 50,\n",
    "    max_tokens: int = 100\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    –ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –±–∞—Ç—á–µ–π\n",
    "    \n",
    "    Args:\n",
    "        llm: vLLM –º–æ–¥–µ–ª—å\n",
    "        num_prompts: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∞\n",
    "        batch_sizes: –°–ø–∏—Å–æ–∫ —Ä–∞–∑–º–µ—Ä–æ–≤ –±–∞—Ç—á–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "        prompt_length: –î–ª–∏–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞ –≤ —Å–ª–æ–≤–∞—Ö\n",
    "        max_tokens: –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞\n",
    "    \"\"\"\n",
    "    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤\n",
    "    test_prompts = [\n",
    "        f\"Generate a summary of: {' '.join(['word'] * prompt_length)}\"\n",
    "        for _ in range(num_prompts)\n",
    "    ]\n",
    "    \n",
    "    sampling_params = create_sampling_params(max_tokens=max_tokens)\n",
    "    results = []\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"\\n–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ batch_size={batch_size}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±–∞—Ç—á–∞–º–∏\n",
    "        for i in range(0, len(test_prompts), batch_size):\n",
    "            batch = test_prompts[i:i+batch_size]\n",
    "            _ = llm.generate(batch, sampling_params)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        throughput = num_prompts / elapsed\n",
    "        \n",
    "        results.append({\n",
    "            'batch_size': batch_size,\n",
    "            'total_time': elapsed,\n",
    "            'prompts_per_sec': throughput,\n",
    "            'time_per_prompt': elapsed / num_prompts\n",
    "        })\n",
    "        \n",
    "        print(f\"  –í—Ä–µ–º—è: {elapsed:.2f}s, Throughput: {throughput:.2f} prompts/s\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞\n",
    "print(\"=== Benchmark Production Performance ===\")\n",
    "benchmark_results = benchmark_generation(\n",
    "    llm, \n",
    "    num_prompts=20,  # –ú–µ–Ω—å—à–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞\n",
    "    batch_sizes=[1, 4, 8],\n",
    "    max_tokens=50\n",
    ")\n",
    "\n",
    "print(\"\\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞:\")\n",
    "print(benchmark_results)\n",
    "print(f\"\\n–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π batch_size: {benchmark_results.loc[benchmark_results['prompts_per_sec'].idxmax(), 'batch_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a6e196",
   "metadata": {},
   "source": [
    "## 13. –°–æ–≤–µ—Ç—ã –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏\n",
    "\n",
    "### –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:\n",
    "\n",
    "1. **GPU Memory Utilization**: –£–≤–µ–ª–∏—á—å—Ç–µ `gpu_memory_utilization` –¥–æ 0.95 –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU\n",
    "\n",
    "2. **Batch Size**: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –≤–æ–∑–º–æ–∂–Ω—ã–π batch size (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç –¥–ª–∏–Ω—ã –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏)\n",
    "\n",
    "3. **Tensor Parallelism**: –î–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ GPU:\n",
    "   ```python\n",
    "   llm = LLM(model=\"meta-llama/Llama-2-70b-hf\", tensor_parallel_size=4)\n",
    "   ```\n",
    "\n",
    "4. **Quantization**: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏:\n",
    "   ```python\n",
    "   llm = LLM(model=\"TheBloke/Llama-2-7B-GPTQ\", quantization=\"gptq\")\n",
    "   ```\n",
    "\n",
    "5. **Max Model Length**: –£–º–µ–Ω—å—à–∏—Ç–µ –µ—Å–ª–∏ –Ω–µ –Ω—É–∂–µ–Ω –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç\n",
    "\n",
    "### –ö–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:\n",
    "\n",
    "1. **Temperature**: \n",
    "   - 0.1-0.3: –î–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ)\n",
    "   - 0.7-0.9: –î–ª—è —Ç–≤–æ—Ä—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞, –¥–∏–∞–ª–æ–≥–∏)\n",
    "   \n",
    "2. **Top-P vs Top-K**: \n",
    "   - Top-P (nucleus sampling): –ë–æ–ª–µ–µ –¥–∏–Ω–∞–º–∏—á–Ω—ã–π –≤—ã–±–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤\n",
    "   - Top-K: –ë–æ–ª–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "   \n",
    "3. **Beam Search**: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–ª—è –∑–∞–¥–∞—á –≥–¥–µ –≤–∞–∂–Ω–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç—å (—Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è, –ø–µ—Ä–µ–≤–æ–¥)\n",
    "\n",
    "4. **Repetition Penalty**: –£–≤–µ–ª–∏—á—å—Ç–µ –¥–æ 1.1-1.3 –¥–ª—è –±–æ—Ä—å–±—ã —Å –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è–º–∏\n",
    "\n",
    "### –≠–∫–æ–Ω–æ–º–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤:\n",
    "\n",
    "1. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ–Ω—å—à–∏–µ –º–æ–¥–µ–ª–∏ –∫–æ–≥–¥–∞ –≤–æ–∑–º–æ–∂–Ω–æ (Phi-2, Mistral-7B)\n",
    "2. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —Ä–∞–∑—É–º–Ω—ã–π `max_tokens`\n",
    "3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `stop` sequences –¥–ª—è —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed7667d",
   "metadata": {},
   "source": [
    "## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ\n",
    "\n",
    "–≠—Ç–æ—Ç notebook –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ–ª–Ω—ã–π pipeline –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å vLLM.\n",
    "\n",
    "### –û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:\n",
    "- ‚úÖ –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∞\n",
    "- ‚úÖ –ì–∏–±–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    "- ‚úÖ –®–∞–±–ª–æ–Ω—ã –ø—Ä–æ–º–ø—Ç–æ–≤\n",
    "- ‚úÖ Pipeline –¥–ª—è —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–π\n",
    "- ‚úÖ –ë–µ–Ω—á–º–∞—Ä–∫–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è\n",
    "\n",
    "### –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:\n",
    "1. –ó–∞–º–µ–Ω–∏—Ç–µ `MODEL_NAME` –Ω–∞ –≤–∞—à—É –º–æ–¥–µ–ª—å\n",
    "2. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ `TEMPLATES` –¥–ª—è –≤–∞—à–µ–π –∑–∞–¥–∞—á–∏\n",
    "3. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –≤–∞—à–∏ –¥–∞–Ω–Ω—ã–µ\n",
    "4. –ó–∞–ø—É—Å—Ç–∏—Ç–µ `VLLMCompetitionPipeline.process_and_save()`\n",
    "5. –°–æ–∑–¥–∞–π—Ç–µ submission!\n",
    "\n",
    "–£–¥–∞—á–∏ –≤ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è—Ö! üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
