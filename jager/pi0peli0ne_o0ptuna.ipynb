{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e770cb9e",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization with Optuna\n",
    "\n",
    "Пайплайн для автоматической оптимизации гиперпараметров:\n",
    "- Bayesian optimization с Optuna\n",
    "- Pruning неперспективных trials\n",
    "- Multi-objective optimization\n",
    "- Интеграция с CatBoost, XGBoost, LightGBM, Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4719cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna pandas numpy scikit-learn catboost xgboost lightgbm torch plotly -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5274c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, mean_squared_error, f1_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Отключение логов Optuna (по желанию)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "print(\"✓ Библиотеки загружены!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956e72fb",
   "metadata": {},
   "source": [
    "## 1. Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b67b529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ВАШИ ДАННЫЕ ===\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "TARGET_COL = 'target'\n",
    "ID_COL = 'id'\n",
    "TASK_TYPE = 'classification'  # 'classification' или 'regression'\n",
    "\n",
    "# Подготовка данных\n",
    "feature_cols = [col for col in train_df.columns if col not in [TARGET_COL, ID_COL]]\n",
    "X = train_df[feature_cols]\n",
    "y = train_df[TARGET_COL]\n",
    "X_test = test_df[feature_cols]\n",
    "\n",
    "# Train/Val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, \n",
    "    stratify=y if TASK_TYPE == 'classification' else None\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eafc41",
   "metadata": {},
   "source": [
    "## 2. Optuna для XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af5449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgboost(trial):\n",
    "    \"\"\"\n",
    "    Objective function для оптимизации XGBoost\n",
    "    \"\"\"\n",
    "    # Определяем пространство гиперпараметров\n",
    "    param = {\n",
    "        'objective': 'binary:logistic' if TASK_TYPE == 'classification' else 'reg:squarederror',\n",
    "        'eval_metric': 'auc' if TASK_TYPE == 'classification' else 'rmse',\n",
    "        'booster': trial.suggest_categorical('booster', ['gbtree', 'gblinear', 'dart']),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),\n",
    "    }\n",
    "    \n",
    "    if param['booster'] in ['gbtree', 'dart']:\n",
    "        param['max_depth'] = trial.suggest_int('max_depth', 3, 10)\n",
    "        param['eta'] = trial.suggest_float('eta', 0.01, 0.3, log=True)\n",
    "        param['gamma'] = trial.suggest_float('gamma', 1e-8, 1.0, log=True)\n",
    "        param['grow_policy'] = trial.suggest_categorical('grow_policy', ['depthwise', 'lossguide'])\n",
    "        param['min_child_weight'] = trial.suggest_int('min_child_weight', 1, 10)\n",
    "        param['subsample'] = trial.suggest_float('subsample', 0.5, 1.0)\n",
    "        param['colsample_bytree'] = trial.suggest_float('colsample_bytree', 0.5, 1.0)\n",
    "    \n",
    "    if param['booster'] == 'dart':\n",
    "        param['sample_type'] = trial.suggest_categorical('sample_type', ['uniform', 'weighted'])\n",
    "        param['normalize_type'] = trial.suggest_categorical('normalize_type', ['tree', 'forest'])\n",
    "        param['rate_drop'] = trial.suggest_float('rate_drop', 1e-8, 1.0, log=True)\n",
    "        param['skip_drop'] = trial.suggest_float('skip_drop', 1e-8, 1.0, log=True)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) if TASK_TYPE == 'classification' else None\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    \n",
    "    # Pruning callback\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, 'test-auc' if TASK_TYPE == 'classification' else 'test-rmse')\n",
    "    \n",
    "    cv_results = xgb.cv(\n",
    "        param,\n",
    "        dtrain,\n",
    "        num_boost_round=1000,\n",
    "        early_stopping_rounds=50,\n",
    "        folds=cv,\n",
    "        callbacks=[pruning_callback],\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    # Возвращаем лучший скор\n",
    "    if TASK_TYPE == 'classification':\n",
    "        return cv_results['test-auc-mean'].max()\n",
    "    else:\n",
    "        return cv_results['test-rmse-mean'].min()\n",
    "\n",
    "# Создание study и оптимизация\n",
    "study_xgb = optuna.create_study(\n",
    "    direction='maximize' if TASK_TYPE == 'classification' else 'minimize',\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=10),\n",
    "    sampler=optuna.samplers.TPESampler(seed=42)\n",
    ")\n",
    "\n",
    "print(\"\\nЗапуск оптимизации XGBoost...\")\n",
    "study_xgb.optimize(objective_xgboost, n_trials=50, timeout=600)  # 50 trials или 10 минут\n",
    "\n",
    "print(\"\\n✓ Оптимизация XGBoost завершена!\")\n",
    "print(f\"Лучший скор: {study_xgb.best_value:.6f}\")\n",
    "print(f\"Лучшие параметры: {study_xgb.best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d34293d",
   "metadata": {},
   "source": [
    "## 3. Optuna для LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cc9c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lightgbm(trial):\n",
    "    \"\"\"\n",
    "    Objective function для оптимизации LightGBM\n",
    "    \"\"\"\n",
    "    param = {\n",
    "        'objective': 'binary' if TASK_TYPE == 'classification' else 'regression',\n",
    "        'metric': 'auc' if TASK_TYPE == 'classification' else 'rmse',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart', 'goss']),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'min_gain_to_split': trial.suggest_float('min_gain_to_split', 0, 15),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "    }\n",
    "    \n",
    "    # Cross-validation с LightGBM\n",
    "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "    \n",
    "    # Pruning callback\n",
    "    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, 'auc' if TASK_TYPE == 'classification' else 'rmse')\n",
    "    \n",
    "    cv_results = lgb.cv(\n",
    "        param,\n",
    "        dtrain,\n",
    "        num_boost_round=1000,\n",
    "        nfold=5,\n",
    "        stratified=TASK_TYPE == 'classification',\n",
    "        callbacks=[pruning_callback, lgb.early_stopping(50)],\n",
    "        return_cvbooster=False\n",
    "    )\n",
    "    \n",
    "    if TASK_TYPE == 'classification':\n",
    "        return max(cv_results['valid auc-mean'])\n",
    "    else:\n",
    "        return min(cv_results['valid rmse-mean'])\n",
    "\n",
    "study_lgb = optuna.create_study(\n",
    "    direction='maximize' if TASK_TYPE == 'classification' else 'minimize',\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=10),\n",
    "    sampler=optuna.samplers.TPESampler(seed=42)\n",
    ")\n",
    "\n",
    "print(\"\\nЗапуск оптимизации LightGBM...\")\n",
    "study_lgb.optimize(objective_lightgbm, n_trials=50, timeout=600)\n",
    "\n",
    "print(\"\\n✓ Оптимизация LightGBM завершена!\")\n",
    "print(f\"Лучший скор: {study_lgb.best_value:.6f}\")\n",
    "print(f\"Лучшие параметры: {study_lgb.best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da430929",
   "metadata": {},
   "source": [
    "## 4. Optuna для CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dabd55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_catboost(trial):\n",
    "    \"\"\"\n",
    "    Objective function для оптимизации CatBoost\n",
    "    \"\"\"\n",
    "    param = {\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),\n",
    "        'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']),\n",
    "        'random_strength': trial.suggest_float('random_strength', 1e-8, 10.0, log=True),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "        'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n",
    "        'od_wait': trial.suggest_int('od_wait', 10, 50),\n",
    "        'random_state': 42,\n",
    "        'verbose': 0\n",
    "    }\n",
    "    \n",
    "    if param['bootstrap_type'] == 'Bayesian':\n",
    "        param['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0, 10)\n",
    "    elif param['bootstrap_type'] == 'Bernoulli':\n",
    "        param['subsample'] = trial.suggest_float('subsample', 0.5, 1.0)\n",
    "    \n",
    "    # Определяем loss function\n",
    "    if TASK_TYPE == 'classification':\n",
    "        param['loss_function'] = 'Logloss'\n",
    "        param['eval_metric'] = 'AUC'\n",
    "    else:\n",
    "        param['loss_function'] = 'RMSE'\n",
    "        param['eval_metric'] = 'RMSE'\n",
    "    \n",
    "    # Обучение с валидацией\n",
    "    model = cb.CatBoostClassifier(**param) if TASK_TYPE == 'classification' else cb.CatBoostRegressor(**param)\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=(X_val, y_val),\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Предсказания\n",
    "    if TASK_TYPE == 'classification':\n",
    "        preds = model.predict_proba(X_val)[:, 1]\n",
    "        score = roc_auc_score(y_val, preds)\n",
    "    else:\n",
    "        preds = model.predict(X_val)\n",
    "        score = mean_squared_error(y_val, preds, squared=False)\n",
    "    \n",
    "    return score\n",
    "\n",
    "study_cb = optuna.create_study(\n",
    "    direction='maximize' if TASK_TYPE == 'classification' else 'minimize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=42)\n",
    ")\n",
    "\n",
    "print(\"\\nЗапуск оптимизации CatBoost...\")\n",
    "study_cb.optimize(objective_catboost, n_trials=30, timeout=600)\n",
    "\n",
    "print(\"\\n✓ Оптимизация CatBoost завершена!\")\n",
    "print(f\"Лучший скор: {study_cb.best_value:.6f}\")\n",
    "print(f\"Лучшие параметры: {study_cb.best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0c417e",
   "metadata": {},
   "source": [
    "## 5. Optuna для Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ad8a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X.values)\n",
    "        self.y = torch.FloatTensor(y.values) if TASK_TYPE == 'regression' else torch.LongTensor(y.values)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def create_model(trial, input_dim, output_dim):\n",
    "    \"\"\"\n",
    "    Создание нейронной сети с оптимизируемой архитектурой\n",
    "    \"\"\"\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 5)\n",
    "    layers = []\n",
    "    \n",
    "    in_features = input_dim\n",
    "    for i in range(n_layers):\n",
    "        out_features = trial.suggest_int(f'n_units_l{i}', 32, 512, log=True)\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        # Dropout\n",
    "        dropout_rate = trial.suggest_float(f'dropout_l{i}', 0.0, 0.5)\n",
    "        if dropout_rate > 0:\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        # BatchNorm\n",
    "        if trial.suggest_categorical(f'batch_norm_l{i}', [True, False]):\n",
    "            layers.append(nn.BatchNorm1d(out_features))\n",
    "        \n",
    "        in_features = out_features\n",
    "    \n",
    "    # Output layer\n",
    "    layers.append(nn.Linear(in_features, output_dim))\n",
    "    \n",
    "    if TASK_TYPE == 'classification' and output_dim == 1:\n",
    "        layers.append(nn.Sigmoid())\n",
    "    elif TASK_TYPE == 'classification':\n",
    "        layers.append(nn.Softmax(dim=1))\n",
    "    \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def objective_neural_network(trial):\n",
    "    \"\"\"\n",
    "    Objective function для оптимизации Neural Network\n",
    "    \"\"\"\n",
    "    # Гиперпараметры\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 256, log=True)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'AdamW', 'SGD'])\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-8, 1e-2, log=True)\n",
    "    \n",
    "    # Создание модели\n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = 1 if TASK_TYPE == 'regression' or len(y.unique()) == 2 else len(y.unique())\n",
    "    \n",
    "    model = create_model(trial, input_dim, output_dim)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'AdamW':\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=0.9)\n",
    "    \n",
    "    # Loss function\n",
    "    if TASK_TYPE == 'classification':\n",
    "        criterion = nn.BCELoss() if output_dim == 1 else nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        criterion = nn.MSELoss()\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_dataset = TabularDataset(X_train, y_train)\n",
    "    val_dataset = TabularDataset(X_val, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Training loop\n",
    "    n_epochs = 50\n",
    "    best_val_score = float('inf') if TASK_TYPE == 'regression' else 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch).squeeze()\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                outputs = model(X_batch).squeeze()\n",
    "                val_preds.extend(outputs.cpu().numpy())\n",
    "                val_targets.extend(y_batch.numpy())\n",
    "        \n",
    "        # Score\n",
    "        if TASK_TYPE == 'classification':\n",
    "            val_score = roc_auc_score(val_targets, val_preds)\n",
    "            if val_score > best_val_score:\n",
    "                best_val_score = val_score\n",
    "        else:\n",
    "            val_score = mean_squared_error(val_targets, val_preds, squared=False)\n",
    "            if val_score < best_val_score:\n",
    "                best_val_score = val_score\n",
    "        \n",
    "        # Pruning\n",
    "        trial.report(best_val_score, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    \n",
    "    return best_val_score\n",
    "\n",
    "study_nn = optuna.create_study(\n",
    "    direction='maximize' if TASK_TYPE == 'classification' else 'minimize',\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
    "    sampler=optuna.samplers.TPESampler(seed=42)\n",
    ")\n",
    "\n",
    "print(\"\\nЗапуск оптимизации Neural Network...\")\n",
    "study_nn.optimize(objective_neural_network, n_trials=30, timeout=600)\n",
    "\n",
    "print(\"\\n✓ Оптимизация Neural Network завершена!\")\n",
    "print(f\"Лучший скор: {study_nn.best_value:.6f}\")\n",
    "print(f\"Лучшие параметры: {study_nn.best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299e8be5",
   "metadata": {},
   "source": [
    "## 6. Визуализация результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71c89a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "\n",
    "# Сравнение всех моделей\n",
    "results = pd.DataFrame([\n",
    "    ('XGBoost', study_xgb.best_value),\n",
    "    ('LightGBM', study_lgb.best_value),\n",
    "    ('CatBoost', study_cb.best_value),\n",
    "    ('Neural Network', study_nn.best_value)\n",
    "], columns=['Model', 'Best Score'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"РЕЗУЛЬТАТЫ ОПТИМИЗАЦИИ ВСЕХ МОДЕЛЕЙ\")\n",
    "print(\"=\"*60)\n",
    "print(results.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Optimization history для лучшей модели\n",
    "best_study = study_cb  # Замените на лучшую\n",
    "fig = optuna.visualization.plot_optimization_history(best_study)\n",
    "fig.show()\n",
    "\n",
    "# Importance параметров\n",
    "fig = optuna.visualization.plot_param_importances(best_study)\n",
    "fig.show()\n",
    "\n",
    "# Parallel coordinate plot\n",
    "fig = optuna.visualization.plot_parallel_coordinate(best_study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b98caa",
   "metadata": {},
   "source": [
    "## 7. Multi-objective optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a65cbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_multi(trial):\n",
    "    \"\"\"\n",
    "    Multi-objective: оптимизируем accuracy И скорость предсказания\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    # Параметры модели\n",
    "    param = {\n",
    "        'iterations': trial.suggest_int('iterations', 50, 500),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'depth': trial.suggest_int('depth', 3, 10),\n",
    "        'random_state': 42,\n",
    "        'verbose': 0\n",
    "    }\n",
    "    \n",
    "    model = cb.CatBoostClassifier(**param) if TASK_TYPE == 'classification' else cb.CatBoostRegressor(**param)\n",
    "    \n",
    "    # Обучение\n",
    "    model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50, verbose=False)\n",
    "    \n",
    "    # Метрика качества\n",
    "    if TASK_TYPE == 'classification':\n",
    "        preds = model.predict_proba(X_val)[:, 1]\n",
    "        accuracy = roc_auc_score(y_val, preds)\n",
    "    else:\n",
    "        preds = model.predict(X_val)\n",
    "        accuracy = -mean_squared_error(y_val, preds, squared=False)\n",
    "    \n",
    "    # Скорость инференса\n",
    "    start_time = time.time()\n",
    "    _ = model.predict(X_val)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    return accuracy, -inference_time  # Максимизируем оба (поэтому -time)\n",
    "\n",
    "# Multi-objective study\n",
    "study_multi = optuna.create_study(\n",
    "    directions=['maximize', 'maximize'],  # Оба объектива на максимум\n",
    "    sampler=optuna.samplers.NSGAIISampler(seed=42)\n",
    ")\n",
    "\n",
    "print(\"\\nЗапуск multi-objective optimization...\")\n",
    "study_multi.optimize(objective_multi, n_trials=50)\n",
    "\n",
    "print(\"\\n✓ Multi-objective optimization завершена!\")\n",
    "print(f\"Количество Pareto-оптимальных решений: {len(study_multi.best_trials)}\")\n",
    "\n",
    "# Визуализация Pareto front\n",
    "fig = optuna.visualization.plot_pareto_front(study_multi, target_names=['Accuracy', 'Speed'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400a387a",
   "metadata": {},
   "source": [
    "## 8. Обучение финальной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f35139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем лучшие параметры для обучения на всех данных\n",
    "best_params = study_cb.best_params  # Замените на лучшую модель\n",
    "\n",
    "if TASK_TYPE == 'classification':\n",
    "    best_params['loss_function'] = 'Logloss'\n",
    "    best_params['eval_metric'] = 'AUC'\n",
    "    final_model = cb.CatBoostClassifier(**best_params, random_state=42, verbose=0)\n",
    "else:\n",
    "    best_params['loss_function'] = 'RMSE'\n",
    "    final_model = cb.CatBoostRegressor(**best_params, random_state=42, verbose=0)\n",
    "\n",
    "# Обучение на всех данных\n",
    "final_model.fit(X, y)\n",
    "\n",
    "# Предсказания\n",
    "if TASK_TYPE == 'classification':\n",
    "    predictions = final_model.predict_proba(X_test)[:, 1]\n",
    "else:\n",
    "    predictions = final_model.predict(X_test)\n",
    "\n",
    "print(\"\\n✓ Финальная модель обучена!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e11a4",
   "metadata": {},
   "source": [
    "## 9. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5b8cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    ID_COL: test_df[ID_COL],\n",
    "    'prediction': predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('optuna_submission.csv', index=False)\n",
    "print(\"\\n✓ Submission сохранен!\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
