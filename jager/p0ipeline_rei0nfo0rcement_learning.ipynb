{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f7ef5b4",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Pipeline\n",
    "\n",
    "Пайплайны для:\n",
    "- DQN (Deep Q-Network)\n",
    "- PPO (Proximal Policy Optimization)\n",
    "- A2C/A3C\n",
    "- Gym environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56965b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym stable-baselines3 torch numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1725b1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from stable_baselines3 import DQN, PPO, A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "\n",
    "print(\"✓ Библиотеки загружены!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2396f069",
   "metadata": {},
   "source": [
    "## 1. Создание окружения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ВАШЕ ОКРУЖЕНИЕ ===\n",
    "# Используйте готовое окружение из Gym или создайте свое\n",
    "\n",
    "ENV_NAME = 'CartPole-v1'  # Примеры: 'LunarLander-v2', 'Acrobot-v1', 'MountainCar-v0'\n",
    "\n",
    "# Создание окружения\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "print(f\"Окружение: {ENV_NAME}\")\n",
    "print(f\"Пространство наблюдений: {env.observation_space}\")\n",
    "print(f\"Пространство действий: {env.action_space}\")\n",
    "\n",
    "# Тестирование окружения\n",
    "obs = env.reset()\n",
    "print(f\"\\nНачальное наблюдение: {obs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1800d93",
   "metadata": {},
   "source": [
    "## 2. DQN - Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73da356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание DQN модели\n",
    "dqn_model = DQN(\n",
    "    'MlpPolicy',\n",
    "    env,\n",
    "    learning_rate=1e-3,\n",
    "    buffer_size=50000,\n",
    "    learning_starts=1000,\n",
    "    batch_size=32,\n",
    "    tau=1.0,\n",
    "    gamma=0.99,\n",
    "    train_freq=4,\n",
    "    gradient_steps=1,\n",
    "    target_update_interval=1000,\n",
    "    exploration_fraction=0.1,\n",
    "    exploration_initial_eps=1.0,\n",
    "    exploration_final_eps=0.05,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./dqn_tensorboard/\"\n",
    ")\n",
    "\n",
    "print(\"✓ DQN модель создана!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237426fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение DQN\n",
    "TIMESTEPS = 100000\n",
    "\n",
    "# Callback для остановки при достижении целевой награды\n",
    "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=195, verbose=1)\n",
    "eval_callback = EvalCallback(\n",
    "    env,\n",
    "    callback_on_new_best=callback_on_best,\n",
    "    eval_freq=10000,\n",
    "    best_model_save_path='./logs/',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "dqn_model.learn(total_timesteps=TIMESTEPS, callback=eval_callback)\n",
    "\n",
    "# Сохранение\n",
    "dqn_model.save(\"dqn_model\")\n",
    "print(\"✓ DQN модель обучена!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c5b58c",
   "metadata": {},
   "source": [
    "## 3. PPO - Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fe4276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Векторизованное окружение для PPO\n",
    "vec_env = make_vec_env(ENV_NAME, n_envs=4)\n",
    "\n",
    "# PPO модель\n",
    "ppo_model = PPO(\n",
    "    'MlpPolicy',\n",
    "    vec_env,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./ppo_tensorboard/\"\n",
    ")\n",
    "\n",
    "print(\"✓ PPO модель создана!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e4681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение PPO\n",
    "ppo_model.learn(total_timesteps=TIMESTEPS)\n",
    "\n",
    "# Сохранение\n",
    "ppo_model.save(\"ppo_model\")\n",
    "print(\"✓ PPO модель обучена!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bf6635",
   "metadata": {},
   "source": [
    "## 4. Оценка моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d31072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка обученной модели\n",
    "trained_model = DQN.load(\"dqn_model\", env=env)\n",
    "# trained_model = PPO.load(\"ppo_model\", env=vec_env)\n",
    "\n",
    "# Оценка\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    trained_model, \n",
    "    env, \n",
    "    n_eval_episodes=10,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0114b4b",
   "metadata": {},
   "source": [
    "## 5. Визуализация агента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610e7a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запуск обученного агента\n",
    "def visualize_agent(model, env, n_episodes=5):\n",
    "    for episode in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            env.render()  # Визуализация (если поддерживается)\n",
    "        \n",
    "        print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "# visualize_agent(trained_model, env, n_episodes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cd9637",
   "metadata": {},
   "source": [
    "## 6. Кастомное окружение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8239afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если нужно создать свое окружение\n",
    "# from gym import Env, spaces\n",
    "\n",
    "# class CustomEnv(Env):\n",
    "#     def __init__(self):\n",
    "#         super(CustomEnv, self).__init__()\n",
    "#         self.action_space = spaces.Discrete(2)  # Пример: 2 действия\n",
    "#         self.observation_space = spaces.Box(low=0, high=1, shape=(4,))  # 4 признака\n",
    "    \n",
    "#     def reset(self):\n",
    "#         # Инициализация начального состояния\n",
    "#         return np.array([0.0, 0.0, 0.0, 0.0])\n",
    "    \n",
    "#     def step(self, action):\n",
    "#         # Логика выполнения действия\n",
    "#         observation = np.random.rand(4)\n",
    "#         reward = 1.0\n",
    "#         done = False\n",
    "#         info = {}\n",
    "#         return observation, reward, done, info\n",
    "    \n",
    "#     def render(self, mode='human'):\n",
    "#         pass"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
