{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11b492e3",
   "metadata": {},
   "source": [
    "# BERT Pipeline - Fine-tuning для NLP задач\n",
    "\n",
    "Этот notebook содержит готовые пайплайны для:\n",
    "- Классификации текста (sentiment, topic classification)\n",
    "- Named Entity Recognition (NER)\n",
    "- Question Answering\n",
    "- Token Classification\n",
    "- Text Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d144c9",
   "metadata": {},
   "source": [
    "## 1. Установка зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85acaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets accelerate torch scikit-learn -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ec27c3",
   "metadata": {},
   "source": [
    "## 2. Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb34d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2918006c",
   "metadata": {},
   "source": [
    "## 3. Загрузка данных\n",
    "**Подставьте свои данные здесь**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470bd7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ВАШИ ДАННЫЕ ===\n",
    "# Формат для классификации текста:\n",
    "# df должен содержать колонки 'text' и 'label'\n",
    "\n",
    "# Вариант 1: Из CSV\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Вариант 2: Пример данных\n",
    "# data = {\n",
    "#     'text': ['Отличный фильм!', 'Ужасно скучно', 'Неплохо'],\n",
    "#     'label': [1, 0, 1]  # 0 - негативный, 1 - позитивный\n",
    "# }\n",
    "# train_df = pd.DataFrame(data)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"\\nПример данных:\\n{train_df.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f4309",
   "metadata": {},
   "source": [
    "## 4. Настройка задачи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515b479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === НАСТРОЙКИ ===\n",
    "MODEL_NAME = \"bert-base-multilingual-cased\"  # Модель для русского и английского\n",
    "# Альтернативы:\n",
    "# \"bert-base-uncased\" - английский\n",
    "# \"DeepPavlov/rubert-base-cased\" - русский\n",
    "# \"distilbert-base-uncased\" - быстрая версия\n",
    "# \"xlm-roberta-base\" - многоязычная\n",
    "\n",
    "TEXT_COLUMN = 'text'       # Название колонки с текстом\n",
    "LABEL_COLUMN = 'label'     # Название колонки с метками\n",
    "NUM_LABELS = 2             # Количество классов\n",
    "MAX_LENGTH = 128           # Максимальная длина текста\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "print(f\"Модель: {MODEL_NAME}\")\n",
    "print(f\"Количество классов: {NUM_LABELS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb5e546",
   "metadata": {},
   "source": [
    "## 5. Загрузка модели и токенизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509537e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Токенизатор\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Модель для классификации\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS\n",
    ").to(device)\n",
    "\n",
    "print(\"✓ Модель и токенизатор загружены!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d161be72",
   "metadata": {},
   "source": [
    "## 6. Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b0813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение на train/validation\n",
    "if LABEL_COLUMN in train_df.columns:\n",
    "    train_data, val_data = train_test_split(\n",
    "        train_df, \n",
    "        test_size=0.1, \n",
    "        random_state=42,\n",
    "        stratify=train_df[LABEL_COLUMN]\n",
    "    )\n",
    "else:\n",
    "    train_data = train_df\n",
    "    val_data = None\n",
    "\n",
    "# Преобразование в Dataset\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "if val_data is not None:\n",
    "    val_dataset = Dataset.from_pandas(val_data)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "if val_data is not None:\n",
    "    print(f\"Val size: {len(val_dataset)}\")\n",
    "print(f\"Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520c3de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция токенизации\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[TEXT_COLUMN],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "# Токенизация датасетов\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "if val_data is not None:\n",
    "    val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Удаление ненужных колонок и переименование labels\n",
    "if LABEL_COLUMN in train_dataset.column_names:\n",
    "    train_dataset = train_dataset.rename_column(LABEL_COLUMN, \"labels\")\n",
    "    if val_data is not None:\n",
    "        val_dataset = val_dataset.rename_column(LABEL_COLUMN, \"labels\")\n",
    "\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "if val_data is not None:\n",
    "    val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "print(\"✓ Данные токенизированы!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780c56b4",
   "metadata": {},
   "source": [
    "## 7. Метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0785187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd30e7ea",
   "metadata": {},
   "source": [
    "## 8. Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa097a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройки обучения\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_results\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\" if val_data is not None else \"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True if val_data is not None else False,\n",
    "    metric_for_best_model=\"f1\" if val_data is not None else None,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# Тренер\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset if val_data is not None else None,\n",
    "    compute_metrics=compute_metrics if val_data is not None else None,\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer готов!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8dd602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение\n",
    "trainer.train()\n",
    "\n",
    "# Сохранение\n",
    "trainer.save_model(\"./bert_finetuned\")\n",
    "tokenizer.save_pretrained(\"./bert_finetuned\")\n",
    "\n",
    "print(\"✓ Модель обучена и сохранена!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f258532",
   "metadata": {},
   "source": [
    "## 9. Оценка на валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba40b92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if val_data is not None:\n",
    "    # Оценка\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(\"\\nРезультаты на валидации:\")\n",
    "    for key, value in eval_results.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    # Детальный отчет\n",
    "    predictions = trainer.predict(val_dataset)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(val_data[LABEL_COLUMN].values, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e1e802",
   "metadata": {},
   "source": [
    "## 10. Предсказания на тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7866e9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка обученной модели\n",
    "model_finetuned = AutoModelForSequenceClassification.from_pretrained(\"./bert_finetuned\")\n",
    "tokenizer_finetuned = AutoTokenizer.from_pretrained(\"./bert_finetuned\")\n",
    "\n",
    "# Pipeline для предсказаний\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model_finetuned,\n",
    "    tokenizer=tokenizer_finetuned,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "print(\"✓ Модель загружена для inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eef3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказания\n",
    "test_texts = test_df[TEXT_COLUMN].tolist()\n",
    "\n",
    "# Предсказание батчами для ускорения\n",
    "predictions = []\n",
    "batch_size = 32\n",
    "\n",
    "for i in range(0, len(test_texts), batch_size):\n",
    "    batch = test_texts[i:i+batch_size]\n",
    "    batch_preds = classifier(batch, truncation=True, max_length=MAX_LENGTH)\n",
    "    predictions.extend([int(pred['label'].split('_')[-1]) for pred in batch_preds])\n",
    "\n",
    "print(f\"✓ Сделано {len(predictions)} предсказаний\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23903c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === СОЗДАНИЕ SUBMISSION ===\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df.index if 'id' not in test_df.columns else test_df['id'],\n",
    "    'prediction': predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('bert_submission.csv', index=False)\n",
    "print(\"✓ Submission сохранен!\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7e638f",
   "metadata": {},
   "source": [
    "## 11. Text Embeddings (извлечение признаков)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7989a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Использование BERT для получения embeddings\n",
    "from transformers import AutoModel\n",
    "\n",
    "# Модель для embeddings (без classification head)\n",
    "embedding_model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def get_embeddings(texts, batch_size=32):\n",
    "    \"\"\"Получить embeddings для текстов\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        # Токенизация\n",
    "        encoded = embedding_tokenizer(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        \n",
    "        # Получение embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = embedding_model(**encoded)\n",
    "            # Используем [CLS] токен (первый токен)\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(batch_embeddings)\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Пример использования\n",
    "# sample_texts = [\"Пример текста 1\", \"Пример текста 2\"]\n",
    "# embeddings = get_embeddings(sample_texts)\n",
    "# print(f\"Shape embeddings: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d56cb4",
   "metadata": {},
   "source": [
    "## 12. Inference с разными моделями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c75e5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Быстрый inference с разными pre-trained моделями\n",
    "\n",
    "# Sentiment Analysis (английский)\n",
    "# sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "# result = sentiment_pipeline(\"I love this movie!\")\n",
    "# print(result)\n",
    "\n",
    "# Named Entity Recognition\n",
    "# ner_pipeline = pipeline(\"ner\", grouped_entities=True)\n",
    "# result = ner_pipeline(\"Apple was founded by Steve Jobs in California.\")\n",
    "# print(result)\n",
    "\n",
    "# Question Answering\n",
    "# qa_pipeline = pipeline(\"question-answering\")\n",
    "# context = \"Python is a programming language.\"\n",
    "# question = \"What is Python?\"\n",
    "# result = qa_pipeline(question=question, context=context)\n",
    "# print(result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
