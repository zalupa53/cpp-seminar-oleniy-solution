{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2a7b891",
   "metadata": {},
   "source": [
    "# Audio Processing Pipeline\n",
    "\n",
    "Пайплайн для работы с аудио:\n",
    "- Классификация аудио (музыкальные жанры, речь, эмоции)\n",
    "- MFCC и спектрограммы\n",
    "- Аугментация аудио\n",
    "- CNN/RNN для аудио\n",
    "- Transfer Learning (YAMNet, wav2vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dfefa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install librosa soundfile torch torchaudio transformers pandas numpy scikit-learn matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cf35db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Библиотеки загружены!\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecbe5d8",
   "metadata": {},
   "source": [
    "## 1. Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac899fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ВАШИ ДАННЫЕ ===\n",
    "# Формат: CSV с колонками 'file_path', 'label'\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Параметры\n",
    "SAMPLE_RATE = 22050  # частота дискретизации\n",
    "DURATION = 3  # длительность аудио в секундах\n",
    "N_MFCC = 40  # количество MFCC коэффициентов\n",
    "N_MELS = 128  # количество мел-фильтров\n",
    "\n",
    "# Энкодинг меток\n",
    "le = LabelEncoder()\n",
    "train_df['label_encoded'] = le.fit_transform(train_df['label'])\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "print(f\"Количество классов: {num_classes}\")\n",
    "print(f\"Классы: {le.classes_}\")\n",
    "print(f\"\\nTrain samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8405e20e",
   "metadata": {},
   "source": [
    "## 2. Feature extraction - MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f1aba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(file_path, sr=SAMPLE_RATE, duration=DURATION, n_mfcc=N_MFCC):\n",
    "    \"\"\"\n",
    "    Извлечение MFCC признаков из аудио файла\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Загрузка аудио\n",
    "        audio, sr = librosa.load(file_path, sr=sr, duration=duration)\n",
    "        \n",
    "        # Padding если аудио короче duration\n",
    "        if len(audio) < sr * duration:\n",
    "            audio = np.pad(audio, (0, sr * duration - len(audio)), mode='constant')\n",
    "        \n",
    "        # Извлечение MFCC\n",
    "        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
    "        \n",
    "        # Усреднение по времени\n",
    "        mfcc_mean = np.mean(mfcc.T, axis=0)\n",
    "        \n",
    "        return mfcc_mean\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при обработке {file_path}: {e}\")\n",
    "        return np.zeros(n_mfcc)\n",
    "\n",
    "# Пример извлечения признаков\n",
    "sample_file = train_df.iloc[0]['file_path']\n",
    "mfcc_features = extract_mfcc(sample_file)\n",
    "print(f\"MFCC shape: {mfcc_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b3cbc1",
   "metadata": {},
   "source": [
    "## 3. Feature extraction - Mel Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9413401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mel_spectrogram(file_path, sr=SAMPLE_RATE, duration=DURATION, n_mels=N_MELS):\n",
    "    \"\"\"\n",
    "    Извлечение мел-спектрограммы\n",
    "    \"\"\"\n",
    "    try:\n",
    "        audio, sr = librosa.load(file_path, sr=sr, duration=duration)\n",
    "        \n",
    "        if len(audio) < sr * duration:\n",
    "            audio = np.pad(audio, (0, sr * duration - len(audio)), mode='constant')\n",
    "        \n",
    "        # Мел-спектрограмма\n",
    "        mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=n_mels)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        return mel_spec_db\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка: {e}\")\n",
    "        return np.zeros((n_mels, int(sr * duration / 512) + 1))\n",
    "\n",
    "# Визуализация спектрограммы\n",
    "mel_spec = extract_mel_spectrogram(sample_file)\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(mel_spec, sr=SAMPLE_RATE, x_axis='time', y_axis='mel')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel Spectrogram')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mel Spectrogram shape: {mel_spec.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54697558",
   "metadata": {},
   "source": [
    "## 4. Audio Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d60530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_audio(audio, sr):\n",
    "    \"\"\"\n",
    "    Аугментация аудио: изменение высоты тона, скорости, добавление шума\n",
    "    \"\"\"\n",
    "    augmentation_type = np.random.choice(['pitch', 'speed', 'noise', 'none'])\n",
    "    \n",
    "    if augmentation_type == 'pitch':\n",
    "        # Изменение высоты тона\n",
    "        n_steps = np.random.randint(-3, 4)\n",
    "        audio = librosa.effects.pitch_shift(audio, sr=sr, n_steps=n_steps)\n",
    "    \n",
    "    elif augmentation_type == 'speed':\n",
    "        # Изменение скорости\n",
    "        rate = np.random.uniform(0.8, 1.2)\n",
    "        audio = librosa.effects.time_stretch(audio, rate=rate)\n",
    "    \n",
    "    elif augmentation_type == 'noise':\n",
    "        # Добавление шума\n",
    "        noise = np.random.randn(len(audio))\n",
    "        audio = audio + 0.005 * noise\n",
    "    \n",
    "    return audio\n",
    "\n",
    "# Пример аугментации\n",
    "audio, sr = librosa.load(sample_file, sr=SAMPLE_RATE)\n",
    "audio_aug = augment_audio(audio.copy(), sr)\n",
    "print(f\"Original audio shape: {audio.shape}\")\n",
    "print(f\"Augmented audio shape: {audio_aug.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503c768d",
   "metadata": {},
   "source": [
    "## 5. PyTorch Dataset для аудио"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109a0167",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, df, sr=SAMPLE_RATE, duration=DURATION, n_mels=N_MELS, augment=False):\n",
    "        self.df = df\n",
    "        self.sr = sr\n",
    "        self.duration = duration\n",
    "        self.n_mels = n_mels\n",
    "        self.augment = augment\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        file_path = row['file_path']\n",
    "        \n",
    "        # Загрузка аудио\n",
    "        audio, sr = librosa.load(file_path, sr=self.sr, duration=self.duration)\n",
    "        \n",
    "        # Аугментация\n",
    "        if self.augment:\n",
    "            audio = augment_audio(audio, sr)\n",
    "        \n",
    "        # Padding\n",
    "        if len(audio) < sr * self.duration:\n",
    "            audio = np.pad(audio, (0, sr * self.duration - len(audio)), mode='constant')\n",
    "        \n",
    "        # Мел-спектрограмма\n",
    "        mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=self.n_mels)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        # Нормализация\n",
    "        mel_spec_db = (mel_spec_db - mel_spec_db.mean()) / (mel_spec_db.std() + 1e-8)\n",
    "        \n",
    "        # Преобразование в тензор (добавляем channel dimension)\n",
    "        mel_spec_tensor = torch.FloatTensor(mel_spec_db).unsqueeze(0)\n",
    "        \n",
    "        # Label\n",
    "        label = torch.LongTensor([row['label_encoded']])[0] if 'label_encoded' in row else 0\n",
    "        \n",
    "        return mel_spec_tensor, label\n",
    "\n",
    "# Создание DataLoaders\n",
    "train_dataset = AudioDataset(train_df, augment=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "print(f\"✓ Dataset создан! Размер: {len(train_dataset)}\")\n",
    "\n",
    "# Проверка батча\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"Batch shape: {sample_batch[0].shape}\")\n",
    "print(f\"Labels shape: {sample_batch[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebf7354",
   "metadata": {},
   "source": [
    "## 6. CNN для классификации аудио"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d48c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(AudioCNN, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Conv block 1\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            # Conv block 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            # Conv block 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "        \n",
    "        # Адаптивный pooling для фиксированного размера\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))\n",
    "        \n",
    "        # FC layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(128 * 4 * 4, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# Создание модели\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AudioCNN(num_classes=num_classes).to(device)\n",
    "\n",
    "print(f\"✓ Модель создана!\")\n",
    "print(f\"Параметры модели: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa04839",
   "metadata": {},
   "source": [
    "## 7. Обучение CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa72d578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбиение на train/val\n",
    "train_df_split, val_df_split = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['label'])\n",
    "\n",
    "train_dataset = AudioDataset(train_df_split, augment=True)\n",
    "val_dataset = AudioDataset(val_df_split, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# Loss & Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5)\n",
    "\n",
    "# Training loop\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "# Обучение\n",
    "n_epochs = 20\n",
    "best_val_acc = 0\n",
    "\n",
    "print(\"\\nНачало обучения...\\n\")\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    scheduler.step(val_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Сохранение лучшей модели\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_audio_model.pth')\n",
    "        print(f\"✓ Модель сохранена! Val Acc: {val_acc:.2f}%\\n\")\n",
    "\n",
    "print(f\"\\n✓ Обучение завершено! Лучшая Val Accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08b59b8",
   "metadata": {},
   "source": [
    "## 8. Transfer Learning - wav2vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6ed152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка предобученной модели wav2vec2\n",
    "model_name = \"facebook/wav2vec2-base\"\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "pretrained_model = AutoModelForAudioClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=num_classes,\n",
    "    ignore_mismatched_sizes=True\n",
    ").to(device)\n",
    "\n",
    "print(\"✓ Pretrained модель загружена!\")\n",
    "\n",
    "# Dataset для wav2vec2\n",
    "class Wav2VecDataset(Dataset):\n",
    "    def __init__(self, df, feature_extractor, sr=16000, duration=3):\n",
    "        self.df = df\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.sr = sr\n",
    "        self.duration = duration\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Загрузка (wav2vec2 работает на 16kHz)\n",
    "        audio, sr = librosa.load(row['file_path'], sr=self.sr, duration=self.duration)\n",
    "        \n",
    "        # Feature extraction\n",
    "        inputs = self.feature_extractor(\n",
    "            audio, \n",
    "            sampling_rate=self.sr, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        label = row['label_encoded'] if 'label_encoded' in row else 0\n",
    "        \n",
    "        return inputs.input_values.squeeze(), torch.LongTensor([label])[0]\n",
    "\n",
    "# Fine-tuning можно провести аналогично CNN обучению\n",
    "print(\"\\nДля fine-tuning используйте тот же training loop с pretrained_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97cfaa4",
   "metadata": {},
   "source": [
    "## 9. Предсказания на тестовом наборе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a63c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка лучшей модели\n",
    "model.load_state_dict(torch.load('best_audio_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Создание test dataset (без меток)\n",
    "test_df_copy = test_df.copy()\n",
    "test_df_copy['label_encoded'] = 0  # Dummy label\n",
    "test_dataset = AudioDataset(test_df_copy, augment=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# Предсказания\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Декодирование меток\n",
    "predicted_labels = le.inverse_transform(predictions)\n",
    "\n",
    "print(f\"\\n✓ Предсказания готовы! Всего: {len(predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df15932d",
   "metadata": {},
   "source": [
    "## 10. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6479a22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'id': test_df.index,  # или test_df['id'] если есть\n",
    "    'prediction': predicted_labels\n",
    "})\n",
    "\n",
    "submission.to_csv('audio_submission.csv', index=False)\n",
    "print(\"\\n✓ Submission сохранен!\")\n",
    "print(submission.head())\n",
    "print(f\"\\nРаспределение предсказаний:\")\n",
    "print(submission['prediction'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
