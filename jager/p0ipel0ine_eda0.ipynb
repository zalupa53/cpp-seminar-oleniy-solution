{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2bfff4e",
   "metadata": {},
   "source": [
    "# Advanced EDA Pipeline - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è:\n",
    "- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö\n",
    "- –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "- Feature importance –∏ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏\n",
    "- –í—ã—è–≤–ª–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π\n",
    "- Distribution analysis\n",
    "- Interactive plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811146c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy matplotlib seaborn plotly scikit-learn scipy pandas-profiling sweetviz autoviz phik missingno -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154b354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ ML\n",
    "from scipy import stats\n",
    "from scipy.stats import normaltest, shapiro, chi2_contingency\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "\n",
    "# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è EDA\n",
    "import missingno as msno\n",
    "from phik import phik_matrix\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "print(\"‚úì –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc462f8",
   "metadata": {},
   "source": [
    "## 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63338b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === –í–ê–®–ò –î–ê–ù–ù–´–ï ===\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "TARGET_COL = 'target'  # –£–∫–∞–∂–∏—Ç–µ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é\n",
    "ID_COL = 'id'  # –£–∫–∞–∂–∏—Ç–µ ID –∫–æ–ª–æ–Ω–∫—É\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"\\nMemory usage: {train_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd3519c",
   "metadata": {},
   "source": [
    "## 2. –ë–∞–∑–æ–≤—ã–π –æ–±–∑–æ—Ä –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20645a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_overview(df, name='Dataset'):\n",
    "    \"\"\"\n",
    "    –ü–æ–ª–Ω—ã–π –æ–±–∑–æ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üìä {name} OVERVIEW\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # –†–∞–∑–º–µ—Ä\n",
    "    print(f\"\\nüìê Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "    print(f\"üíæ Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö\n",
    "    print(f\"\\nüìã Data Types:\")\n",
    "    dtype_counts = df.dtypes.value_counts()\n",
    "    for dtype, count in dtype_counts.items():\n",
    "        print(f\"  {dtype}: {count} columns\")\n",
    "    \n",
    "    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –∫–æ–ª–æ–Ω–æ–∫\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "    \n",
    "    print(f\"\\nüî¢ Numeric columns: {len(numeric_cols)}\")\n",
    "    print(f\"üìù Categorical columns: {len(categorical_cols)}\")\n",
    "    print(f\"üìÖ Datetime columns: {len(datetime_cols)}\")\n",
    "    \n",
    "    # –ü—Ä–æ–ø—É—Å–∫–∏\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing': missing,\n",
    "        'Percent': missing_pct\n",
    "    })\n",
    "    missing_df = missing_df[missing_df['Missing'] > 0].sort_values('Percent', ascending=False)\n",
    "    \n",
    "    if len(missing_df) > 0:\n",
    "        print(f\"\\n‚ùå Missing Values: {len(missing_df)} columns\")\n",
    "        print(missing_df.head(10))\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No missing values!\")\n",
    "    \n",
    "    # –î—É–±–ª–∏–∫–∞—Ç—ã\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\nüîÑ Duplicate rows: {duplicates:,} ({duplicates/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "    print(f\"\\nüéØ Unique values per column:\")\n",
    "    unique_counts = df.nunique().sort_values()\n",
    "    print(unique_counts.head(10))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ\n",
    "dataset_overview(train_df, 'TRAIN')\n",
    "dataset_overview(test_df, 'TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430b1257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–µ—Ä–≤—ã–µ –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏\n",
    "print(\"\\nüìå First 5 rows:\")\n",
    "display(train_df.head())\n",
    "\n",
    "print(\"\\nüìå Last 5 rows:\")\n",
    "display(train_df.tail())\n",
    "\n",
    "# –°–ª—É—á–∞–π–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏\n",
    "print(\"\\nüìå Random 5 rows:\")\n",
    "display(train_df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed8b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "print(\"\\nüìä Descriptive Statistics:\")\n",
    "display(train_df.describe(include='all').T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6455fb83",
   "metadata": {},
   "source": [
    "## 3. –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—Å–∫–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897c31cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤ —Å missingno\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
    "\n",
    "# Matrix\n",
    "plt.subplot(2, 2, 1)\n",
    "msno.matrix(train_df, ax=plt.gca(), fontsize=10)\n",
    "plt.title('Missing Data Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bar chart\n",
    "plt.subplot(2, 2, 2)\n",
    "msno.bar(train_df, ax=plt.gca(), fontsize=10)\n",
    "plt.title('Missing Data Bar Chart', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Heatmap\n",
    "plt.subplot(2, 2, 3)\n",
    "msno.heatmap(train_df, ax=plt.gca(), fontsize=10)\n",
    "plt.title('Missing Data Correlation Heatmap', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Dendrogram\n",
    "plt.subplot(2, 2, 4)\n",
    "msno.dendrogram(train_df, ax=plt.gca(), fontsize=10)\n",
    "plt.title('Missing Data Dendrogram', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791c2787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—Å–∫–æ–≤\n",
    "def analyze_missing_values(df):\n",
    "    \"\"\"\n",
    "    –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
    "    \"\"\"\n",
    "    missing_stats = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        missing_count = df[col].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            missing_pct = (missing_count / len(df)) * 100\n",
    "            dtype = df[col].dtype\n",
    "            unique_count = df[col].nunique()\n",
    "            \n",
    "            missing_stats.append({\n",
    "                'Column': col,\n",
    "                'Missing_Count': missing_count,\n",
    "                'Missing_Percent': missing_pct,\n",
    "                'Dtype': dtype,\n",
    "                'Unique_Values': unique_count\n",
    "            })\n",
    "    \n",
    "    if missing_stats:\n",
    "        missing_df = pd.DataFrame(missing_stats)\n",
    "        missing_df = missing_df.sort_values('Missing_Percent', ascending=False)\n",
    "        \n",
    "        print(\"\\nüìä MISSING VALUES ANALYSIS:\")\n",
    "        print(\"=\"*80)\n",
    "        display(missing_df)\n",
    "        \n",
    "        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "        fig = px.bar(\n",
    "            missing_df,\n",
    "            x='Column',\n",
    "            y='Missing_Percent',\n",
    "            title='Missing Values Percentage by Column',\n",
    "            labels={'Missing_Percent': 'Missing %'},\n",
    "            color='Missing_Percent',\n",
    "            color_continuous_scale='Reds'\n",
    "        )\n",
    "        fig.update_layout(height=500)\n",
    "        fig.show()\n",
    "        \n",
    "        return missing_df\n",
    "    else:\n",
    "        print(\"‚úÖ No missing values found!\")\n",
    "        return None\n",
    "\n",
    "missing_analysis = analyze_missing_values(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60e7442",
   "metadata": {},
   "source": [
    "## 4. –ê–Ω–∞–ª–∏–∑ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d9d963",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TARGET_COL in train_df.columns:\n",
    "    print(f\"\\nüéØ TARGET VARIABLE ANALYSIS: '{TARGET_COL}'\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏\n",
    "    n_unique = train_df[TARGET_COL].nunique()\n",
    "    is_classification = n_unique < 20\n",
    "    \n",
    "    if is_classification:\n",
    "        print(f\"\\nüìä Task Type: CLASSIFICATION ({n_unique} classes)\")\n",
    "        \n",
    "        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤\n",
    "        target_counts = train_df[TARGET_COL].value_counts()\n",
    "        target_pcts = train_df[TARGET_COL].value_counts(normalize=True) * 100\n",
    "        \n",
    "        target_stats = pd.DataFrame({\n",
    "            'Count': target_counts,\n",
    "            'Percentage': target_pcts\n",
    "        })\n",
    "        \n",
    "        print(\"\\nClass Distribution:\")\n",
    "        display(target_stats)\n",
    "        \n",
    "        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "        fig = make_subplots(\n",
    "            rows=1, cols=2,\n",
    "            subplot_titles=('Class Distribution', 'Class Percentages'),\n",
    "            specs=[[{'type': 'bar'}, {'type': 'pie'}]]\n",
    "        )\n",
    "        \n",
    "        # Bar chart\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=target_counts.index, y=target_counts.values, name='Count'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Pie chart\n",
    "        fig.add_trace(\n",
    "            go.Pie(labels=target_counts.index, values=target_counts.values),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(height=400, showlegend=False)\n",
    "        fig.show()\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞\n",
    "        imbalance_ratio = target_counts.max() / target_counts.min()\n",
    "        print(f\"\\n‚öñÔ∏è Imbalance Ratio: {imbalance_ratio:.2f}\")\n",
    "        if imbalance_ratio > 5:\n",
    "            print(\"‚ö†Ô∏è WARNING: Significant class imbalance detected!\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"\\nüìä Task Type: REGRESSION\")\n",
    "        \n",
    "        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "        print(\"\\nTarget Statistics:\")\n",
    "        print(train_df[TARGET_COL].describe())\n",
    "        \n",
    "        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Histogram\n",
    "        axes[0, 0].hist(train_df[TARGET_COL].dropna(), bins=50, edgecolor='black')\n",
    "        axes[0, 0].set_title('Distribution', fontweight='bold')\n",
    "        axes[0, 0].set_xlabel(TARGET_COL)\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        \n",
    "        # Box plot\n",
    "        axes[0, 1].boxplot(train_df[TARGET_COL].dropna())\n",
    "        axes[0, 1].set_title('Box Plot', fontweight='bold')\n",
    "        axes[0, 1].set_ylabel(TARGET_COL)\n",
    "        \n",
    "        # Q-Q plot\n",
    "        stats.probplot(train_df[TARGET_COL].dropna(), dist=\"norm\", plot=axes[1, 0])\n",
    "        axes[1, 0].set_title('Q-Q Plot', fontweight='bold')\n",
    "        \n",
    "        # KDE plot\n",
    "        train_df[TARGET_COL].dropna().plot(kind='kde', ax=axes[1, 1])\n",
    "        axes[1, 1].set_title('Kernel Density Estimate', fontweight='bold')\n",
    "        axes[1, 1].set_xlabel(TARGET_COL)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç–∏\n",
    "        statistic, p_value = normaltest(train_df[TARGET_COL].dropna())\n",
    "        print(f\"\\nüìä Normality Test (D'Agostino-Pearson):\")\n",
    "        print(f\"  Statistic: {statistic:.4f}\")\n",
    "        print(f\"  P-value: {p_value:.4f}\")\n",
    "        if p_value < 0.05:\n",
    "            print(\"  ‚ùå Target is NOT normally distributed\")\n",
    "        else:\n",
    "            print(\"  ‚úÖ Target appears normally distributed\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ffd25f",
   "metadata": {},
   "source": [
    "## 5. –ê–Ω–∞–ª–∏–∑ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542c7174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–ª—É—á–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫\n",
    "numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if ID_COL in numeric_cols:\n",
    "    numeric_cols.remove(ID_COL)\n",
    "if TARGET_COL in numeric_cols:\n",
    "    numeric_cols.remove(TARGET_COL)\n",
    "\n",
    "print(f\"\\nüî¢ NUMERIC FEATURES ANALYSIS ({len(numeric_cols)} features)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(numeric_cols) > 0:\n",
    "    # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "    numeric_stats = train_df[numeric_cols].describe().T\n",
    "    numeric_stats['missing'] = train_df[numeric_cols].isnull().sum()\n",
    "    numeric_stats['missing_pct'] = (numeric_stats['missing'] / len(train_df)) * 100\n",
    "    numeric_stats['unique'] = train_df[numeric_cols].nunique()\n",
    "    numeric_stats['skew'] = train_df[numeric_cols].skew()\n",
    "    numeric_stats['kurtosis'] = train_df[numeric_cols].kurtosis()\n",
    "    \n",
    "    print(\"\\nNumeric Features Statistics:\")\n",
    "    display(numeric_stats)\n",
    "    \n",
    "    # –í—ã—è–≤–ª–µ–Ω–∏–µ —Å–∏–ª—å–Ω–æ —Å–∫–æ—à–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "    high_skew = numeric_stats[abs(numeric_stats['skew']) > 1].sort_values('skew', key=abs, ascending=False)\n",
    "    if len(high_skew) > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è Highly Skewed Features (|skew| > 1): {len(high_skew)}\")\n",
    "        print(high_skew[['mean', 'std', 'skew']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d192b1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "if len(numeric_cols) > 0:\n",
    "    n_cols = min(4, len(numeric_cols))\n",
    "    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
    "    \n",
    "    for idx, col in enumerate(numeric_cols[:20]):  # –ü–µ—Ä–≤—ã–µ 20 –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞\n",
    "        ax = axes[idx]\n",
    "        train_df[col].hist(bins=50, ax=ax, edgecolor='black', alpha=0.7)\n",
    "        ax.set_title(f'{col}\\nSkew: {train_df[col].skew():.2f}', fontweight='bold')\n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel('Frequency')\n",
    "    \n",
    "    # –°–∫—Ä—ã—Ç—å –ª–∏—à–Ω–∏–µ subplots\n",
    "    for idx in range(len(numeric_cols), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.suptitle('Numeric Features Distributions', fontsize=16, fontweight='bold', y=1.0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd97999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤\n",
    "if len(numeric_cols) > 0:\n",
    "    fig = make_subplots(\n",
    "        rows=(len(numeric_cols[:12]) + 3) // 4,\n",
    "        cols=4,\n",
    "        subplot_titles=numeric_cols[:12]\n",
    "    )\n",
    "    \n",
    "    for idx, col in enumerate(numeric_cols[:12]):\n",
    "        row = idx // 4 + 1\n",
    "        col_num = idx % 4 + 1\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Box(y=train_df[col].dropna(), name=col, showlegend=False),\n",
    "            row=row, col=col_num\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(height=300 * ((len(numeric_cols[:12]) + 3) // 4), \n",
    "                      title_text=\"Box Plots - Outlier Detection\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c959af47",
   "metadata": {},
   "source": [
    "## 6. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb0a76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(numeric_cols) > 0:\n",
    "    print(\"\\nüîó CORRELATION ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Pearson correlation\n",
    "    corr_matrix = train_df[numeric_cols].corr()\n",
    "    \n",
    "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Heatmap\n",
    "    sns.heatmap(corr_matrix, annot=True if len(numeric_cols) < 15 else False, \n",
    "                cmap='coolwarm', center=0, square=True, ax=axes[0],\n",
    "                fmt='.2f', linewidths=0.5)\n",
    "    axes[0].set_title('Pearson Correlation Heatmap', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Clustermap alternative\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True if len(numeric_cols) < 15 else False,\n",
    "                cmap='coolwarm', center=0, square=True, ax=axes[1],\n",
    "                fmt='.2f', linewidths=0.5)\n",
    "    axes[1].set_title('Correlation Heatmap (Lower Triangle)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # –¢–æ–ø –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π\n",
    "    corr_pairs = corr_matrix.unstack()\n",
    "    corr_pairs = corr_pairs[corr_pairs != 1]\n",
    "    high_corr = corr_pairs[abs(corr_pairs) > 0.7].sort_values(key=abs, ascending=False)\n",
    "    \n",
    "    if len(high_corr) > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è High Correlations (|r| > 0.7):\")\n",
    "        print(high_corr.head(20))\n",
    "    \n",
    "    # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å —Ç–∞—Ä–≥–µ—Ç–æ–º\n",
    "    if TARGET_COL in train_df.columns and TARGET_COL not in numeric_cols:\n",
    "        # –î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ - –∏—Å–ø–æ–ª—å–∑—É–µ–º point-biserial –∏–ª–∏ mutual information\n",
    "        if train_df[TARGET_COL].nunique() < 20:\n",
    "            target_corr = []\n",
    "            for col in numeric_cols:\n",
    "                corr, _ = stats.pointbiserialr(train_df[TARGET_COL], train_df[col])\n",
    "                target_corr.append((col, corr))\n",
    "            target_corr = pd.Series(dict(target_corr)).sort_values(key=abs, ascending=False)\n",
    "        else:\n",
    "            target_corr = train_df[numeric_cols + [TARGET_COL]].corr()[TARGET_COL].drop(TARGET_COL)\n",
    "            target_corr = target_corr.sort_values(key=abs, ascending=False)\n",
    "        \n",
    "        print(f\"\\nüéØ Top 15 Features Correlated with Target:\")\n",
    "        print(target_corr.head(15))\n",
    "        \n",
    "        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "        fig = px.bar(\n",
    "            x=target_corr.head(15).values,\n",
    "            y=target_corr.head(15).index,\n",
    "            orientation='h',\n",
    "            title='Top 15 Features by Correlation with Target',\n",
    "            labels={'x': 'Correlation', 'y': 'Feature'},\n",
    "            color=target_corr.head(15).values,\n",
    "            color_continuous_scale='RdBu'\n",
    "        )\n",
    "        fig.update_layout(height=500)\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c242abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phi-K correlation –¥–ª—è mixed data types\n",
    "try:\n",
    "    print(\"\\nüìä Phi-K Correlation Matrix (works with categorical data):\")\n",
    "    phik_corr = train_df[numeric_cols[:15]].phik_matrix()  # –ü–µ—Ä–≤—ã–µ 15 –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(phik_corr, annot=True, cmap='viridis', center=0.5, square=True, fmt='.2f')\n",
    "    plt.title('Phi-K Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Phi-K calculation skipped: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d121f9e4",
   "metadata": {},
   "source": [
    "## 7. –ê–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85e8f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = train_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"\\nüìù CATEGORICAL FEATURES ANALYSIS ({len(categorical_cols)} features)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(categorical_cols) > 0:\n",
    "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º\n",
    "    cat_stats = []\n",
    "    for col in categorical_cols:\n",
    "        cat_stats.append({\n",
    "            'Feature': col,\n",
    "            'Unique_Values': train_df[col].nunique(),\n",
    "            'Missing': train_df[col].isnull().sum(),\n",
    "            'Missing_Pct': (train_df[col].isnull().sum() / len(train_df)) * 100,\n",
    "            'Most_Common': train_df[col].mode()[0] if len(train_df[col].mode()) > 0 else None,\n",
    "            'Most_Common_Freq': train_df[col].value_counts().iloc[0] if len(train_df[col].value_counts()) > 0 else 0\n",
    "        })\n",
    "    \n",
    "    cat_stats_df = pd.DataFrame(cat_stats).sort_values('Unique_Values', ascending=False)\n",
    "    print(\"\\nCategorical Features Statistics:\")\n",
    "    display(cat_stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e5f1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–ø –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "if len(categorical_cols) > 0:\n",
    "    for col in categorical_cols[:6]:  # –ü–µ—Ä–≤—ã–µ 6\n",
    "        value_counts = train_df[col].value_counts().head(15)\n",
    "        \n",
    "        fig = px.bar(\n",
    "            x=value_counts.index,\n",
    "            y=value_counts.values,\n",
    "            title=f\"Distribution of '{col}' (Top 15)\",\n",
    "            labels={'x': col, 'y': 'Count'},\n",
    "            color=value_counts.values,\n",
    "            color_continuous_scale='Blues'\n",
    "        )\n",
    "        fig.update_layout(height=400, showlegend=False)\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fd879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ vs Target\n",
    "if TARGET_COL in train_df.columns and len(categorical_cols) > 0:\n",
    "    print(f\"\\nüìä Categorical Features vs Target:\")\n",
    "    \n",
    "    for col in categorical_cols[:5]:  # –ü–µ—Ä–≤—ã–µ 5\n",
    "        if train_df[TARGET_COL].nunique() < 20:  # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è\n",
    "            ct = pd.crosstab(train_df[col], train_df[TARGET_COL], normalize='index') * 100\n",
    "            \n",
    "            fig = px.bar(\n",
    "                ct.head(15),\n",
    "                title=f\"'{col}' vs Target (Top 15 categories)\",\n",
    "                labels={'value': 'Percentage', 'variable': 'Target'},\n",
    "                barmode='group'\n",
    "            )\n",
    "            fig.update_layout(height=500)\n",
    "            fig.show()\n",
    "            \n",
    "            # Chi-square test\n",
    "            ct_counts = pd.crosstab(train_df[col], train_df[TARGET_COL])\n",
    "            chi2, p_value, dof, expected = chi2_contingency(ct_counts)\n",
    "            print(f\"\\n  {col} - Chi-square test:\")\n",
    "            print(f\"    Chi2 statistic: {chi2:.4f}\")\n",
    "            print(f\"    P-value: {p_value:.4f}\")\n",
    "            if p_value < 0.05:\n",
    "                print(f\"    ‚úÖ Significant relationship with target\")\n",
    "            else:\n",
    "                print(f\"    ‚ùå No significant relationship with target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5868e49",
   "metadata": {},
   "source": [
    "## 8. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b03cdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TARGET_COL in train_df.columns and len(numeric_cols) > 0:\n",
    "    print(\"\\nüèÜ FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "    X = train_df[numeric_cols].fillna(train_df[numeric_cols].median())\n",
    "    y = train_df[TARGET_COL]\n",
    "    \n",
    "    # Random Forest Feature Importance\n",
    "    if y.nunique() < 20:  # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    else:  # –†–µ–≥—Ä–µ—Å—Å–∏—è\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    \n",
    "    rf.fit(X, y)\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': numeric_cols,\n",
    "        'Importance': rf.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nüìä Top 20 Features by Random Forest Importance:\")\n",
    "    print(feature_importance.head(20))\n",
    "    \n",
    "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "    fig = px.bar(\n",
    "        feature_importance.head(20),\n",
    "        x='Importance',\n",
    "        y='Feature',\n",
    "        orientation='h',\n",
    "        title='Top 20 Features - Random Forest Importance',\n",
    "        color='Importance',\n",
    "        color_continuous_scale='Viridis'\n",
    "    )\n",
    "    fig.update_layout(height=600, yaxis={'categoryorder': 'total ascending'})\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b69d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutual Information\n",
    "if TARGET_COL in train_df.columns and len(numeric_cols) > 0:\n",
    "    print(\"\\nüîó Mutual Information Scores:\")\n",
    "    \n",
    "    X = train_df[numeric_cols].fillna(train_df[numeric_cols].median())\n",
    "    y = train_df[TARGET_COL]\n",
    "    \n",
    "    if y.nunique() < 20:\n",
    "        mi_scores = mutual_info_classif(X, y, random_state=42)\n",
    "    else:\n",
    "        mi_scores = mutual_info_regression(X, y, random_state=42)\n",
    "    \n",
    "    mi_df = pd.DataFrame({\n",
    "        'Feature': numeric_cols,\n",
    "        'MI_Score': mi_scores\n",
    "    }).sort_values('MI_Score', ascending=False)\n",
    "    \n",
    "    print(mi_df.head(20))\n",
    "    \n",
    "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "    fig = px.bar(\n",
    "        mi_df.head(20),\n",
    "        x='MI_Score',\n",
    "        y='Feature',\n",
    "        orientation='h',\n",
    "        title='Top 20 Features - Mutual Information',\n",
    "        color='MI_Score',\n",
    "        color_continuous_scale='Plasma'\n",
    "    )\n",
    "    fig.update_layout(height=600, yaxis={'categoryorder': 'total ascending'})\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3368387",
   "metadata": {},
   "source": [
    "## 9. Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada74fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(numeric_cols) > 0:\n",
    "    print(\"\\nüîç OUTLIER DETECTION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # IQR method\n",
    "    outlier_stats = []\n",
    "    for col in numeric_cols:\n",
    "        Q1 = train_df[col].quantile(0.25)\n",
    "        Q3 = train_df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = train_df[(train_df[col] < lower_bound) | (train_df[col] > upper_bound)][col]\n",
    "        outlier_count = len(outliers)\n",
    "        outlier_pct = (outlier_count / len(train_df)) * 100\n",
    "        \n",
    "        outlier_stats.append({\n",
    "            'Feature': col,\n",
    "            'Outliers': outlier_count,\n",
    "            'Outlier_Pct': outlier_pct,\n",
    "            'Lower_Bound': lower_bound,\n",
    "            'Upper_Bound': upper_bound\n",
    "        })\n",
    "    \n",
    "    outlier_df = pd.DataFrame(outlier_stats).sort_values('Outlier_Pct', ascending=False)\n",
    "    outlier_df = outlier_df[outlier_df['Outliers'] > 0]\n",
    "    \n",
    "    if len(outlier_df) > 0:\n",
    "        print(\"\\nFeatures with Outliers (IQR method):\")\n",
    "        display(outlier_df)\n",
    "        \n",
    "        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "        fig = px.bar(\n",
    "            outlier_df.head(15),\n",
    "            x='Feature',\n",
    "            y='Outlier_Pct',\n",
    "            title='Top 15 Features by Outlier Percentage',\n",
    "            labels={'Outlier_Pct': 'Outlier %'},\n",
    "            color='Outlier_Pct',\n",
    "            color_continuous_scale='Reds'\n",
    "        )\n",
    "        fig.update_layout(height=500)\n",
    "        fig.show()\n",
    "    else:\n",
    "        print(\"‚úÖ No outliers detected using IQR method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea546c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forest –¥–ª—è –∞–Ω–æ–º–∞–ª–∏–π\n",
    "if len(numeric_cols) > 0:\n",
    "    print(\"\\nüå≤ Isolation Forest Anomaly Detection:\")\n",
    "    \n",
    "    X = train_df[numeric_cols].fillna(train_df[numeric_cols].median())\n",
    "    \n",
    "    iso_forest = IsolationForest(contamination=0.05, random_state=42, n_jobs=-1)\n",
    "    anomaly_labels = iso_forest.fit_predict(X)\n",
    "    \n",
    "    n_anomalies = (anomaly_labels == -1).sum()\n",
    "    anomaly_pct = (n_anomalies / len(train_df)) * 100\n",
    "    \n",
    "    print(f\"  Detected anomalies: {n_anomalies} ({anomaly_pct:.2f}%)\")\n",
    "    \n",
    "    train_df['is_anomaly'] = anomaly_labels\n",
    "    \n",
    "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å PCA\n",
    "    if len(numeric_cols) >= 2:\n",
    "        pca = PCA(n_components=2)\n",
    "        X_pca = pca.fit_transform(X)\n",
    "        \n",
    "        fig = px.scatter(\n",
    "            x=X_pca[:, 0],\n",
    "            y=X_pca[:, 1],\n",
    "            color=anomaly_labels,\n",
    "            title='Anomaly Detection (PCA projection)',\n",
    "            labels={'x': 'PC1', 'y': 'PC2', 'color': 'Anomaly'},\n",
    "            color_discrete_map={1: 'blue', -1: 'red'}\n",
    "        )\n",
    "        fig.update_layout(height=600)\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aae64c",
   "metadata": {},
   "source": [
    "## 10. Dimensionality Reduction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677f5f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(numeric_cols) > 2 and TARGET_COL in train_df.columns:\n",
    "    print(\"\\nüé® DIMENSIONALITY REDUCTION VISUALIZATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    X = train_df[numeric_cols].fillna(train_df[numeric_cols].median())\n",
    "    y = train_df[TARGET_COL]\n",
    "    \n",
    "    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # PCA\n",
    "    print(\"\\nComputing PCA...\")\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    print(f\"PCA Explained Variance: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "    \n",
    "    # t-SNE\n",
    "    print(\"\\nComputing t-SNE (–º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è)...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, n_jobs=-1)\n",
    "    X_tsne = tsne.fit_transform(X_scaled[:1000])  # –ü–µ—Ä–≤—ã–µ 1000 –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏\n",
    "    \n",
    "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=('PCA', 't-SNE'),\n",
    "        specs=[[{'type': 'scatter'}, {'type': 'scatter'}]]\n",
    "    )\n",
    "    \n",
    "    # PCA plot\n",
    "    for target_val in y.unique()[:10]:  # –ú–∞–∫—Å–∏–º—É–º 10 –∫–ª–∞—Å—Å–æ–≤\n",
    "        mask = y == target_val\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X_pca[mask, 0],\n",
    "                y=X_pca[mask, 1],\n",
    "                mode='markers',\n",
    "                name=str(target_val),\n",
    "                marker=dict(size=5, opacity=0.6)\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # t-SNE plot\n",
    "    for target_val in y[:1000].unique()[:10]:\n",
    "        mask = y[:1000] == target_val\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X_tsne[mask, 0],\n",
    "                y=X_tsne[mask, 1],\n",
    "                mode='markers',\n",
    "                name=str(target_val),\n",
    "                marker=dict(size=5, opacity=0.6),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"PC1\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"PC2\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Dimension 1\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Dimension 2\", row=1, col=2)\n",
    "    \n",
    "    fig.update_layout(height=500, title_text=\"Dimensionality Reduction Visualization\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb8685c",
   "metadata": {},
   "source": [
    "## 11. Train vs Test Distribution Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a20fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîÑ TRAIN vs TEST DISTRIBUTION COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "common_numeric = [col for col in numeric_cols if col in test_df.columns]\n",
    "\n",
    "if len(common_numeric) > 0:\n",
    "    for col in common_numeric[:6]:  # –ü–µ—Ä–≤—ã–µ 6\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        fig.add_trace(go.Histogram(\n",
    "            x=train_df[col].dropna(),\n",
    "            name='Train',\n",
    "            opacity=0.7,\n",
    "            nbinsx=50\n",
    "        ))\n",
    "        \n",
    "        fig.add_trace(go.Histogram(\n",
    "            x=test_df[col].dropna(),\n",
    "            name='Test',\n",
    "            opacity=0.7,\n",
    "            nbinsx=50\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f\"Distribution Comparison: '{col}'\",\n",
    "            xaxis_title=col,\n",
    "            yaxis_title='Frequency',\n",
    "            barmode='overlay',\n",
    "            height=400\n",
    "        )\n",
    "        fig.show()\n",
    "        \n",
    "        # KS test\n",
    "        ks_stat, ks_pval = stats.ks_2samp(\n",
    "            train_df[col].dropna(),\n",
    "            test_df[col].dropna()\n",
    "        )\n",
    "        print(f\"\\n{col} - Kolmogorov-Smirnov Test:\")\n",
    "        print(f\"  Statistic: {ks_stat:.4f}\")\n",
    "        print(f\"  P-value: {ks_pval:.4f}\")\n",
    "        if ks_pval < 0.05:\n",
    "            print(f\"  ‚ö†Ô∏è WARNING: Distributions are significantly different!\")\n",
    "        else:\n",
    "            print(f\"  ‚úÖ Distributions are similar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cf0bd3",
   "metadata": {},
   "source": [
    "## 12. Automated Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d9ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas Profiling –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ—Ç—á–µ—Ç–∞\n",
    "try:\n",
    "    from ydata_profiling import ProfileReport\n",
    "    \n",
    "    print(\"\\nüìã Generating Automated EDA Report...\")\n",
    "    print(\"‚è≥ This may take a few minutes...\")\n",
    "    \n",
    "    profile = ProfileReport(\n",
    "        train_df,\n",
    "        title='Automated EDA Report',\n",
    "        explorative=True,\n",
    "        minimal=False\n",
    "    )\n",
    "    \n",
    "    profile.to_file('eda_report.html')\n",
    "    print(\"\\n‚úÖ Report saved as 'eda_report.html'\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è ydata-profiling not installed. Run: pip install ydata-profiling\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Error generating report: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78e0d16",
   "metadata": {},
   "source": [
    "## 13. Key Insights Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaf1ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä KEY INSIGHTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£ Dataset Overview:\")\n",
    "print(f\"   ‚Ä¢ Train size: {train_df.shape[0]:,} rows √ó {train_df.shape[1]} columns\")\n",
    "print(f\"   ‚Ä¢ Test size: {test_df.shape[0]:,} rows √ó {test_df.shape[1]} columns\")\n",
    "print(f\"   ‚Ä¢ Numeric features: {len(numeric_cols)}\")\n",
    "print(f\"   ‚Ä¢ Categorical features: {len(categorical_cols)}\")\n",
    "\n",
    "if TARGET_COL in train_df.columns:\n",
    "    print(f\"\\n2Ô∏è‚É£ Target Variable:\")\n",
    "    if train_df[TARGET_COL].nunique() < 20:\n",
    "        print(f\"   ‚Ä¢ Type: Classification ({train_df[TARGET_COL].nunique()} classes)\")\n",
    "        imbalance = train_df[TARGET_COL].value_counts().max() / train_df[TARGET_COL].value_counts().min()\n",
    "        print(f\"   ‚Ä¢ Imbalance ratio: {imbalance:.2f}\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ Type: Regression\")\n",
    "        print(f\"   ‚Ä¢ Range: [{train_df[TARGET_COL].min():.2f}, {train_df[TARGET_COL].max():.2f}]\")\n",
    "\n",
    "if missing_analysis is not None and len(missing_analysis) > 0:\n",
    "    print(f\"\\n3Ô∏è‚É£ Data Quality:\")\n",
    "    print(f\"   ‚Ä¢ Features with missing values: {len(missing_analysis)}\")\n",
    "    print(f\"   ‚Ä¢ Highest missing rate: {missing_analysis['Missing_Percent'].max():.1f}%\")\n",
    "else:\n",
    "    print(f\"\\n3Ô∏è‚É£ Data Quality:\")\n",
    "    print(f\"   ‚Ä¢ ‚úÖ No missing values!\")\n",
    "\n",
    "print(f\"   ‚Ä¢ Duplicate rows: {train_df.duplicated().sum():,}\")\n",
    "\n",
    "if len(numeric_cols) > 0:\n",
    "    print(f\"\\n4Ô∏è‚É£ Feature Characteristics:\")\n",
    "    high_skew = abs(train_df[numeric_cols].skew()) > 1\n",
    "    print(f\"   ‚Ä¢ Highly skewed features: {high_skew.sum()}\")\n",
    "    \n",
    "    if 'outlier_df' in locals() and len(outlier_df) > 0:\n",
    "        print(f\"   ‚Ä¢ Features with outliers: {len(outlier_df)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ EDA Complete!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
