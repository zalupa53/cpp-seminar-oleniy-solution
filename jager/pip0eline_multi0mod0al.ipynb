{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3737591",
   "metadata": {},
   "source": [
    "# Multimodal Pipeline - Текст + Изображения\n",
    "\n",
    "Этот notebook содержит пайплайны для:\n",
    "- CLIP (сопоставление текста и изображений)\n",
    "- Image Captioning\n",
    "- Visual Question Answering\n",
    "- Text-to-Image поиск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96863631",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers pillow torch torchvision clip-by-openai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597124ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForConditionalGeneration\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d050d835",
   "metadata": {},
   "source": [
    "## 1. CLIP - сопоставление текста и изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5806cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ВАШИ НАСТРОЙКИ ===\n",
    "IMAGE_DIR = './images'  # Папка с изображениями\n",
    "# Создайте папку и поместите туда изображения\n",
    "\n",
    "# Загрузка CLIP\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "print(\"✓ CLIP модель загружена!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106d48a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для сопоставления изображения с текстами\n",
    "def match_image_with_texts(image_path, texts):\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=texts,\n",
    "        images=image,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        probs = logits_per_image.softmax(dim=1)\n",
    "    \n",
    "    return probs.cpu().numpy()[0]\n",
    "\n",
    "# === ПРИМЕР ИСПОЛЬЗОВАНИЯ ===\n",
    "# texts = [\"кошка\", \"собака\", \"автомобиль\", \"дерево\"]\n",
    "# image_path = Path(IMAGE_DIR) / \"example.jpg\"\n",
    "# scores = match_image_with_texts(image_path, texts)\n",
    "# for text, score in zip(texts, scores):\n",
    "#     print(f\"{text}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef4fb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение эмбеддингов для изображений и текстов\n",
    "def get_image_embeddings(image_paths, batch_size=32):\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        images = [Image.open(p) for p in batch_paths]\n",
    "        \n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_features = model.get_image_features(**inputs)\n",
    "            embeddings.append(image_features.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "def get_text_embeddings(texts, batch_size=32):\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        inputs = processor(text=batch_texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            text_features = model.get_text_features(**inputs)\n",
    "            embeddings.append(text_features.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fcc2b1",
   "metadata": {},
   "source": [
    "## 2. Image Captioning - описание изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e72c02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка BLIP для генерации описаний\n",
    "caption_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "caption_model = BlipForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip-image-captioning-base\"\n",
    ").to(device)\n",
    "\n",
    "print(\"✓ BLIP модель загружена!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445a6053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    inputs = caption_processor(image, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = caption_model.generate(**inputs, max_length=50)\n",
    "    \n",
    "    caption = caption_processor.decode(output[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "# === ПРИМЕР ===\n",
    "# caption = generate_caption(Path(IMAGE_DIR) / \"example.jpg\")\n",
    "# print(f\"Описание: {caption}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0623d0",
   "metadata": {},
   "source": [
    "## 3. Предсказания для соревнования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badc9161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ЗАГРУЗИТЕ СВОИ ДАННЫЕ ===\n",
    "# Пример: CSV с путями к изображениям и текстовыми метками\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Если задача - классификация изображений по текстовым меткам\n",
    "# class_labels = [\"класс1\", \"класс2\", \"класс3\"]  # Ваши классы\n",
    "\n",
    "# predictions = []\n",
    "# for img_path in test_df['image_path']:\n",
    "#     scores = match_image_with_texts(img_path, class_labels)\n",
    "#     pred_class = class_labels[np.argmax(scores)]\n",
    "#     predictions.append(pred_class)\n",
    "\n",
    "# submission = pd.DataFrame({\n",
    "#     'id': test_df['id'],\n",
    "#     'prediction': predictions\n",
    "# })\n",
    "# submission.to_csv('multimodal_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d427f552",
   "metadata": {},
   "source": [
    "## 4. Поиск изображений по текстовому запросу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b06ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Получение эмбеддингов для всех изображений\n",
    "# image_paths = list(Path(IMAGE_DIR).glob('*.jpg'))\n",
    "# image_embeddings = get_image_embeddings(image_paths)\n",
    "\n",
    "# Поиск по текстовому запросу\n",
    "# query = \"красная машина\"\n",
    "# query_embedding = get_text_embeddings([query])\n",
    "\n",
    "# similarities = cosine_similarity(query_embedding, image_embeddings)[0]\n",
    "# top_indices = np.argsort(similarities)[::-1][:5]  # Топ-5\n",
    "\n",
    "# print(f\"Топ-5 изображений для запроса '{query}':\")\n",
    "# for idx in top_indices:\n",
    "#     print(f\"{image_paths[idx].name}: {similarities[idx]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
