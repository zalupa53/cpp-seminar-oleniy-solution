{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb97e484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.12.0.tar.gz (69 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting sentence-transformers\n",
      "  Using cached sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.4.1)\n",
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (5.0.0.dev0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from faiss-cpu) (2.2.6)\n",
      "Requirement already satisfied: packaging in /Users/quezaltendes/Library/Python/3.12/lib/python/site-packages (from faiss-cpu) (24.2)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sentence-transformers) (1.6.0)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sentence-transformers) (1.1.2)\n",
      "Requirement already satisfied: Pillow in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.11.13)\n",
      "Requirement already satisfied: anyio in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (75.5.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/quezaltendes/Library/Python/3.12/lib/python/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/quezaltendes/Library/Python/3.12/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Using cached sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
      "Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Building wheels for collected packages: faiss-cpu\n",
      "  Building wheel for faiss-cpu (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for faiss-cpu \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[8 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'faiss._swigfaiss' extension\n",
      "  \u001b[31m   \u001b[0m swigging third-party/faiss/faiss/python/swigfaiss.i to third-party/faiss/faiss/python/swigfaiss_wrap.cpp\n",
      "  \u001b[31m   \u001b[0m swig -python -c++ -Doverride= -doxygen -Ithird-party/faiss -I/private/var/folders/jh/f9tnvk0j15j7zq3v2fcc0n8m0000gn/T/pip-build-env-t61h9bzt/overlay/lib/python3.12/site-packages/numpy/_core/include -Ithird-party/faiss -I/usr/local/include -o third-party/faiss/faiss/python/swigfaiss_wrap.cpp third-party/faiss/faiss/python/swigfaiss.i\n",
      "  \u001b[31m   \u001b[0m error: command 'swig' failed: No such file or directory\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[31m  ERROR: Failed building wheel for faiss-cpu\u001b[0m\u001b[31m\n",
      "\u001b[0mFailed to build faiss-cpu\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Failed to build installable wheels for some pyproject.toml based projects (faiss-cpu)\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu sentence-transformers datasets transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef553b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datasets import Dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Загружаем твой датасет\n",
    "with open(\"data.pickle\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "texts = [\n",
    "    f\"Question: {ex['question']}\\nOptions: {ex['options']}\\nAnswer: {ex['answer']}\"\n",
    "    for ex in data\n",
    "]\n",
    "\n",
    "encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "embs = encoder.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "d = embs.shape[1]     \n",
    "faiss_index = faiss.IndexFlatL2(d)\n",
    "faiss_index.add(embs)\n",
    "\n",
    "print(\"FAISS index built:\", faiss_index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cc1f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Загружаем Qwen\n",
    "model_name = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\").eval()\n",
    "\n",
    "def rag_answer(question):\n",
    "    query = f\"Question: {question}\"\n",
    "    \n",
    "    # 1. Получить эмбеддинг вопроса\n",
    "    q_emb = encoder.encode([query], convert_to_numpy=True)\n",
    "\n",
    "    # 2. Найти похожие куски (top-3)\n",
    "    D, I = faiss_index.search(q_emb, 3)\n",
    "\n",
    "    retrieved = \"\\n\\n\".join([texts[i] for i in I[0]])\n",
    "\n",
    "    # 3. Сформировать промпт с контекстом\n",
    "    prompt = f\"\"\"\n",
    "You are an expert at solving math and physics multiple-choice questions.\n",
    "\n",
    "Use ONLY the context below to answer.\n",
    "\n",
    "### Context:\n",
    "{retrieved}\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Your answer (just letters without spaces):\n",
    "\"\"\"\n",
    "    \n",
    "    # 4. Генерация\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "    answer = tokenizer.decode(out[0][inputs['input_ids'].size(1):], skip_special_tokens=True)\n",
    "    \n",
    "    return answer.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rag_answer(\"The perimeter of a square with side 3 is:\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# подготавливаем токены\n",
    "tokenized_docs = [doc.split() for doc in docs]\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "def retrieve_bm25(query, top_k=3):\n",
    "    tokenized_query = query.split()\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    ranked = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
    "    return [docs[i] for i in ranked[:top_k]]\n",
    "\n",
    "# пример\n",
    "question = \"What is the formula for the perimeter of a square?\"\n",
    "retrieved = retrieve_bm25(question)\n",
    "answer = generate_answer(question, retrieved)\n",
    "\n",
    "print(\"=== BM25 Retrieved ===\")\n",
    "print(retrieved)\n",
    "print(\"=== Answer ===\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b816e22",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c632dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# 1. Загрузка моделей\n",
    "print(\"Загружаем модели...\")\n",
    "\n",
    "# Модель для эмбеддингов (русский язык)\n",
    "embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Модель для генерации\n",
    "model_name = \"IlyaGusev/saiga_yandexgpt_8b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 2. Подготовка базы знаний (пример с русскими текстами)\n",
    "def create_knowledge_base():\n",
    "    \"\"\"Создаем расширенную базу знаний на русском\"\"\"\n",
    "    documents = [\n",
    "        # География и города\n",
    "        \"Москва - столица России, крупнейший город страны с населением более 12 миллионов человек. Основана в 1147 году Юрием Долгоруким.\",\n",
    "        \"Санкт-Петербург был основан Петром I в 1703 году и является культурной столицей России. Население около 5 миллионов человек.\",\n",
    "        \"Новосибирск - третий по численности населения город России, крупнейший научный и образовательный центр Сибири.\",\n",
    "        \"Сочи - курортный город на черноморском побережье России, известный своими пляжами и горнолыжными курортами.\",\n",
    "        \"Калининград - самый западный город России, расположенный между Польшей и Литвой на берегу Балтийского моря.\",\n",
    "        \n",
    "        # Природа и география\n",
    "        \"Байкал - самое глубокое озеро в мире, расположенное в Сибири. Глубина достигает 1642 метров, содержит 20% мировых запасов пресной воды.\",\n",
    "        \"Волга - самая длинная река в Европе, протяженностью 3530 км. Протекает через 15 субъектов Российской Федерации.\",\n",
    "        \"Эльбрус - высочайшая горная вершина России и Европы, высота 5642 метра. Расположен на Кавказе.\",\n",
    "        \"Камчатка - полуостров на Дальнем Востоке России, известный своими вулканами и гейзерами.\",\n",
    "        \"Тайга - крупнейший в мире лесной массив, занимающий большую часть Сибири и Дальнего Востока.\",\n",
    "        \n",
    "        # История\n",
    "        \"Великая Отечественная война длилась с 1941 по 1945 год и завершилась победой Советского Союза над нацистской Германией.\",\n",
    "        \"Октябрьская революция 1917 года привела к установлению советской власти в России и созданию СССР.\",\n",
    "        \"Петр I Великий - русский царь, провел масштабные реформы и основал Санкт-Петербург. Правил с 1682 по 1725 год.\",\n",
    "        \"Екатерина II Великая - императрица России с 1762 по 1796 год, значительно расширила территорию Российской империи.\",\n",
    "        \n",
    "        # Культура и искусство\n",
    "        \"Эрмитаж в Санкт-Петербурге - один из крупнейших художественных музеев мира, основанный Екатериной II в 1764 году.\",\n",
    "        \"Третьяковская галерея в Москве - главный музей русского изобразительного искусства, основанный Павлом Третьяковым.\",\n",
    "        \"Большой театр в Москве - один из ведущих театров оперы и балета в мире, основанный в 1776 году.\",\n",
    "        \"Александр Пушкин - великий русский поэт, основоположник современного русского литературного языка. Родился в 1799 году.\",\n",
    "        \"Лев Толстой - классик русской литературы, автор романов 'Война и мир' и 'Анна Каренина'.\",\n",
    "        \n",
    "        # Наука и технологии\n",
    "        \"Дмитрий Менделеев - русский ученый, создатель периодической системы химических элементов в 1869 году.\",\n",
    "        \"Юрий Гагарин - первый человек в космосе, совершил полет 12 апреля 1961 года на корабле 'Восток-1'.\",\n",
    "        \"Михаил Ломоносов - русский ученый-энциклопедист, основатель Московского университета в 1755 году.\",\n",
    "        \"Сергей Королев - советский конструктор ракетно-космической техники, руководитель программы первого полета человека в космос.\",\n",
    "        \n",
    "        # Экономика и промышленность\n",
    "        \"Газпром - крупнейшая газовая компания мира, основана в 1989 году. Штаб-квартира расположена в Москве.\",\n",
    "        \"Роснефть - одна из крупнейших нефтяных компаний мира, основана в 1993 году.\",\n",
    "        \"Транссибирская магистраль - самая длинная железная дорога в мире, протяженностью 9288 км от Москвы до Владивостока.\",\n",
    "        \"Рубль - национальная валюта России, введена в обращение в 14 веке. Современный код валюты - RUB.\",\n",
    "        \n",
    "        # Политика\n",
    "        \"Владимир Путин является президентом Российской Федерации с 2012 года. Родился в 1952 году в Ленинграде.\",\n",
    "        \"Государственная Дума - нижняя палата парламента России, состоит из 450 депутатов.\",\n",
    "        \"Конституция России была принята 12 декабря 1993 года и является основным законом страны.\",\n",
    "        \"Кремль - исторический комплекс в Москве, резиденция президента России. Построен в 15 веке.\",\n",
    "        \n",
    "        # Образование\n",
    "        \"МГУ имени Ломоносова - старейший и крупнейший университет России, основан в 1755 году.\",\n",
    "        \"ЕГЭ - единый государственный экзамен, обязательный для всех выпускников школ России с 2009 года.\",\n",
    "        \"Российская академия наук - главная научная организация страны, основана Петром I в 1724 году.\",\n",
    "        \n",
    "        # Спорт\n",
    "        \"Хоккей с шайбой - один из самых популярных видов спорта в России. Сборная России многократный чемпион мира.\",\n",
    "        \"Фигурное катание - традиционно сильный вид спорта для России, множество олимпийских чемпионов.\",\n",
    "        \"Футбольный клуб Зенит из Санкт-Петербурга - один из ведущих футбольных клубов России.\",\n",
    "        \"Олимпиада 1980 года проводилась в Москве, была первой Олимпиадой в социалистической стране.\",\n",
    "        \n",
    "        # Традиции и праздники\n",
    "        \"День Победы отмечается 9 мая в честь победы в Великой Отечественной войне. Главный парад проходит на Красной площади.\",\n",
    "        \"Масленица - традиционный русский праздник проводов зимы, отмечается за неделю до Великого поста.\",\n",
    "        \"Новый год - главный праздник в России, отмечается в ночь с 31 декабря на 1 января.\",\n",
    "        \"День России - государственный праздник 12 июня, отмечается с 1992 года.\"\n",
    "    ]\n",
    "    return documents\n",
    "\n",
    "# Примеры вопросов для тестирования\n",
    "test_questions = [\n",
    "    \"Какая самая длинная река в Европе?\",\n",
    "    \"Когда был основан Санкт-Петербург?\",\n",
    "    \"Кто был первым человеком в космосе?\",\n",
    "    \"Сколько человек живет в Москве?\",\n",
    "    \"Какое озеро самое глубокое в мире?\",\n",
    "    \"Кто создал периодическую таблицу элементов?\",\n",
    "    \"Как называется валюта России?\",\n",
    "    \"Когда была Октябрьская революция?\",\n",
    "    \"Какой университет самый старый в России?\",\n",
    "    \"Что такое Эрмитаж?\",\n",
    "    \"Кто является президентом России?\",\n",
    "    \"Какой праздник отмечается 9 мая?\",\n",
    "    \"Сколько километров составляет Транссибирская магистраль?\",\n",
    "    \"Кто автор романа 'Война и мир'?\",\n",
    "    \"В каком году Юрий Гагарин полетел в космос?\"\n",
    "]\n",
    "\n",
    "def run_test_questions():\n",
    "    \"\"\"Функция для тестирования системы на подготовленных вопросах\"\"\"\n",
    "    documents = create_knowledge_base()\n",
    "    index, documents = create_vector_db(documents)\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"ТЕСТИРОВАНИЕ RAG СИСТЕМЫ\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\n{i}. Вопрос: {question}\")\n",
    "        try:\n",
    "            answer, context_docs = rag_pipeline(question, index, documents)\n",
    "            print(f\"Ответ: {answer}\")\n",
    "            print(f\"Использовано документов: {len(context_docs)}\")\n",
    "            print(\"-\" * 30)\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка: {e}\")\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# 3. Создание векторной базы данных\n",
    "def create_vector_db(documents):\n",
    "    \"\"\"Создаем векторную базу данных\"\"\"\n",
    "    print(\"Создаем векторную базу...\")\n",
    "    \n",
    "    # Получаем эмбеддинги для документов\n",
    "    embeddings = embedding_model.encode(documents)\n",
    "    \n",
    "    # Создаем FAISS индекс\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)  # Inner Product (косинусное сходство)\n",
    "    \n",
    "    # Нормализуем векторы для косинусного сходства\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    return index, documents\n",
    "\n",
    "# 4. Поиск релевантных документов\n",
    "def search_similar_documents(query, index, documents, top_k=3):\n",
    "    \"\"\"Ищем наиболее релевантные документы для запроса\"\"\"\n",
    "    # Получаем эмбеддинг запроса\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    # Ищем похожие документы\n",
    "    scores, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    # Возвращаем найденные документы и их скоринги\n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        if idx < len(documents):\n",
    "            results.append({\n",
    "                'text': documents[idx],\n",
    "                'score': scores[0][i]\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 5. Создание промпта с контекстом\n",
    "def create_prompt(question, context_docs):\n",
    "    \"\"\"Создаем промпт для модели с контекстом\"\"\"\n",
    "    context = \"\\n\".join([f\"- {doc['text']}\" for doc in context_docs])\n",
    "    \n",
    "    prompt = f\"\"\"Контекст:\n",
    "{context}\n",
    "\n",
    "Вопрос: {question}\n",
    "\n",
    "Ответ: \"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# 6. Генерация ответа\n",
    "def generate_answer(prompt):\n",
    "    \"\"\"Генерируем ответ с помощью модели\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Убираем промпт из ответа\n",
    "    answer = response[len(prompt):].strip()\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# 7. Основная функция RAG\n",
    "def rag_pipeline(question, index, documents):\n",
    "    \"\"\"Полный пайплайн RAG\"\"\"\n",
    "    print(f\"Вопрос: {question}\")\n",
    "    \n",
    "    # Шаг 1: Поиск релевантных документов\n",
    "    print(\"Ищем релевантные документы...\")\n",
    "    relevant_docs = search_similar_documents(question, index, documents)\n",
    "    \n",
    "    print(\"Найденные документы:\")\n",
    "    for i, doc in enumerate(relevant_docs):\n",
    "        print(f\"{i+1}. {doc['text']} (сходство: {doc['score']:.3f})\")\n",
    "    \n",
    "    # Шаг 2: Создание промпта\n",
    "    prompt = create_prompt(question, relevant_docs)\n",
    "    \n",
    "    # Шаг 3: Генерация ответа\n",
    "    print(\"\\nГенерируем ответ...\")\n",
    "    answer = generate_answer(prompt)\n",
    "    \n",
    "    return answer, relevant_docs\n",
    "\n",
    "# 8. Запуск системы\n",
    "def main():\n",
    "    # Инициализация\n",
    "    documents = create_knowledge_base()\n",
    "    index, documents = create_vector_db(documents)\n",
    "    \n",
    "    print(\"RAG система готова! Введите ваш вопрос (или 'выход' для завершения):\")\n",
    "    \n",
    "    # Интерактивный цикл\n",
    "    while True:\n",
    "        question = input(\"\\nВаш вопрос: \").strip()\n",
    "        \n",
    "        if question.lower() in ['выход', 'exit', 'quit']:\n",
    "            break\n",
    "            \n",
    "        if not question:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            answer, context_docs = rag_pipeline(question, index, documents)\n",
    "            print(f\"\\nОтвет: {answer}\")\n",
    "            print(f\"\\nИсточники: {len(context_docs)} документов\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_test_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b33d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# 1. Загрузка моделей\n",
    "print(\"Загружаем модели...\")\n",
    "\n",
    "# Модель для эмбеддингов (русский язык)\n",
    "embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Модель для генерации\n",
    "model_name = \"IlyaGusev/saiga_yandexgpt_8b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 2. Подготовка базы знаний (пример с русскими текстами)\n",
    "def create_knowledge_base():\n",
    "    \"\"\"Создаем расширенную базу знаний на русском\"\"\"\n",
    "    documents = [\n",
    "        # География и города\n",
    "        \"Москва - столица России, крупнейший город страны с населением более 12 миллионов человек. Основана в 1147 году Юрием Долгоруким.\",\n",
    "        \"Санкт-Петербург был основан Петром I в 1703 году и является культурной столицей России. Население около 5 миллионов человек.\",\n",
    "        \"Новосибирск - третий по численности населения город России, крупнейший научный и образовательный центр Сибири.\",\n",
    "        \"Сочи - курортный город на черноморском побережье России, известный своими пляжами и горнолыжными курортами.\",\n",
    "        \"Калининград - самый западный город России, расположенный между Польшей и Литвой на берегу Балтийского моря.\",\n",
    "        \n",
    "        # Природа и география\n",
    "        \"Байкал - самое глубокое озеро в мире, расположенное в Сибири. Глубина достигает 1642 метров, содержит 20% мировых запасов пресной воды.\",\n",
    "        \"Волга - самая длинная река в Европе, протяженностью 3530 км. Протекает через 15 субъектов Российской Федерации.\",\n",
    "        \"Эльбрус - высочайшая горная вершина России и Европы, высота 5642 метра. Расположен на Кавказе.\",\n",
    "        \"Камчатка - полуостров на Дальнем Востоке России, известный своими вулканами и гейзерами.\",\n",
    "        \"Тайга - крупнейший в мире лесной массив, занимающий большую часть Сибири и Дальнего Востока.\",\n",
    "        \n",
    "        # История\n",
    "        \"Великая Отечественная война длилась с 1941 по 1945 год и завершилась победой Советского Союза над нацистской Германией.\",\n",
    "        \"Октябрьская революция 1917 года привела к установлению советской власти в России и созданию СССР.\",\n",
    "        \"Петр I Великий - русский царь, провел масштабные реформы и основал Санкт-Петербург. Правил с 1682 по 1725 год.\",\n",
    "        \"Екатерина II Великая - императрица России с 1762 по 1796 год, значительно расширила территорию Российской империи.\",\n",
    "        \n",
    "        # Культура и искусство\n",
    "        \"Эрмитаж в Санкт-Петербурге - один из крупнейших художественных музеев мира, основанный Екатериной II в 1764 году.\",\n",
    "        \"Третьяковская галерея в Москве - главный музей русского изобразительного искусства, основанный Павлом Третьяковым.\",\n",
    "        \"Большой театр в Москве - один из ведущих театров оперы и балета в мире, основанный в 1776 году.\",\n",
    "        \"Александр Пушкин - великий русский поэт, основоположник современного русского литературного языка. Родился в 1799 году.\",\n",
    "        \"Лев Толстой - классик русской литературы, автор романов 'Война и мир' и 'Анна Каренина'.\",\n",
    "        \n",
    "        # Наука и технологии\n",
    "        \"Дмитрий Менделеев - русский ученый, создатель периодической системы химических элементов в 1869 году.\",\n",
    "        \"Юрий Гагарин - первый человек в космосе, совершил полет 12 апреля 1961 года на корабле 'Восток-1'.\",\n",
    "        \"Михаил Ломоносов - русский ученый-энциклопедист, основатель Московского университета в 1755 году.\",\n",
    "        \"Сергей Королев - советский конструктор ракетно-космической техники, руководитель программы первого полета человека в космос.\",\n",
    "        \n",
    "        # Экономика и промышленность\n",
    "        \"Газпром - крупнейшая газовая компания мира, основана в 1989 году. Штаб-квартира расположена в Москве.\",\n",
    "        \"Роснефть - одна из крупнейших нефтяных компаний мира, основана в 1993 году.\",\n",
    "        \"Транссибирская магистраль - самая длинная железная дорога в мире, протяженностью 9288 км от Москвы до Владивостока.\",\n",
    "        \"Рубль - национальная валюта России, введена в обращение в 14 веке. Современный код валюты - RUB.\",\n",
    "        \n",
    "        # Политика\n",
    "        \"Владимир Путин является президентом Российской Федерации с 2012 года. Родился в 1952 году в Ленинграде.\",\n",
    "        \"Государственная Дума - нижняя палата парламента России, состоит из 450 депутатов.\",\n",
    "        \"Конституция России была принята 12 декабря 1993 года и является основным законом страны.\",\n",
    "        \"Кремль - исторический комплекс в Москве, резиденция президента России. Построен в 15 веке.\",\n",
    "        \n",
    "        # Образование\n",
    "        \"МГУ имени Ломоносова - старейший и крупнейший университет России, основан в 1755 году.\",\n",
    "        \"ЕГЭ - единый государственный экзамен, обязательный для всех выпускников школ России с 2009 года.\",\n",
    "        \"Российская академия наук - главная научная организация страны, основана Петром I в 1724 году.\",\n",
    "        \n",
    "        # Спорт\n",
    "        \"Хоккей с шайбой - один из самых популярных видов спорта в России. Сборная России многократный чемпион мира.\",\n",
    "        \"Фигурное катание - традиционно сильный вид спорта для России, множество олимпийских чемпионов.\",\n",
    "        \"Футбольный клуб Зенит из Санкт-Петербурга - один из ведущих футбольных клубов России.\",\n",
    "        \"Олимпиада 1980 года проводилась в Москве, была первой Олимпиадой в социалистической стране.\",\n",
    "        \n",
    "        # Традиции и праздники\n",
    "        \"День Победы отмечается 9 мая в честь победы в Великой Отечественной войне. Главный парад проходит на Красной площади.\",\n",
    "        \"Масленица - традиционный русский праздник проводов зимы, отмечается за неделю до Великого поста.\",\n",
    "        \"Новый год - главный праздник в России, отмечается в ночь с 31 декабря на 1 января.\",\n",
    "        \"День России - государственный праздник 12 июня, отмечается с 1992 года.\"\n",
    "    ]\n",
    "    return documents\n",
    "\n",
    "# Примеры вопросов для тестирования\n",
    "test_questions = [\n",
    "    \"Какая самая длинная река в Европе?\",\n",
    "    \"Когда был основан Санкт-Петербург?\",\n",
    "    \"Кто был первым человеком в космосе?\",\n",
    "    \"Сколько человек живет в Москве?\",\n",
    "    \"Какое озеро самое глубокое в мире?\",\n",
    "    \"Кто создал периодическую таблицу элементов?\",\n",
    "    \"Как называется валюта России?\",\n",
    "    \"Когда была Октябрьская революция?\",\n",
    "    \"Какой университет самый старый в России?\",\n",
    "    \"Что такое Эрмитаж?\",\n",
    "    \"Кто является президентом России?\",\n",
    "    \"Какой праздник отмечается 9 мая?\",\n",
    "    \"Сколько километров составляет Транссибирская магистраль?\",\n",
    "    \"Кто автор романа 'Война и мир'?\",\n",
    "    \"В каком году Юрий Гагарин полетел в космос?\"\n",
    "]\n",
    "\n",
    "def run_test_questions():\n",
    "    \"\"\"Функция для тестирования системы на подготовленных вопросах\"\"\"\n",
    "    documents = create_knowledge_base()\n",
    "    index, documents = create_vector_db(documents)\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"ТЕСТИРОВАНИЕ RAG СИСТЕМЫ\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\n{i}. Вопрос: {question}\")\n",
    "        try:\n",
    "            answer, context_docs = rag_pipeline(question, index, documents)\n",
    "            print(f\"Ответ: {answer}\")\n",
    "            print(f\"Использовано документов: {len(context_docs)}\")\n",
    "            print(\"-\" * 30)\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка: {e}\")\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# 3. Создание векторной базы данных\n",
    "def create_vector_db(documents):\n",
    "    \"\"\"Создаем векторную базу данных\"\"\"\n",
    "    print(\"Создаем векторную базу...\")\n",
    "    \n",
    "    # Получаем эмбеддинги для документов\n",
    "    embeddings = embedding_model.encode(documents)\n",
    "    \n",
    "    # Создаем FAISS индекс\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)  # Inner Product (косинусное сходство)\n",
    "    \n",
    "    # Нормализуем векторы для косинусного сходства\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    return index, documents\n",
    "\n",
    "# 4. Поиск релевантных документов\n",
    "def search_similar_documents(query, index, documents, top_k=3):\n",
    "    \"\"\"Ищем наиболее релевантные документы для запроса\"\"\"\n",
    "    # Получаем эмбеддинг запроса\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    # Ищем похожие документы\n",
    "    scores, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    # Возвращаем найденные документы и их скоринги\n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        if idx < len(documents):\n",
    "            results.append({\n",
    "                'text': documents[idx],\n",
    "                'score': scores[0][i]\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 5. Создание промпта с контекстом\n",
    "def create_prompt(question, context_docs):\n",
    "    \"\"\"Создаем промпт для модели с контекстом\"\"\"\n",
    "    context = \"\\n\".join([f\"- {doc['text']}\" for doc in context_docs])\n",
    "    \n",
    "    prompt = f\"\"\"Контекст:\n",
    "{context}\n",
    "\n",
    "Вопрос: {question}\n",
    "\n",
    "Ответ: \"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# 6. Генерация ответа\n",
    "def generate_answer(prompt):\n",
    "    \"\"\"Генерируем ответ с помощью модели\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Убираем промпт из ответа\n",
    "    answer = response[len(prompt):].strip()\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# 7. Основная функция RAG\n",
    "def rag_pipeline(question, index, documents):\n",
    "    \"\"\"Полный пайплайн RAG\"\"\"\n",
    "    print(f\"Вопрос: {question}\")\n",
    "    \n",
    "    # Шаг 1: Поиск релевантных документов\n",
    "    print(\"Ищем релевантные документы...\")\n",
    "    relevant_docs = search_similar_documents(question, index, documents)\n",
    "    \n",
    "    print(\"Найденные документы:\")\n",
    "    for i, doc in enumerate(relevant_docs):\n",
    "        print(f\"{i+1}. {doc['text']} (сходство: {doc['score']:.3f})\")\n",
    "    \n",
    "    # Шаг 2: Создание промпта\n",
    "    prompt = create_prompt(question, relevant_docs)\n",
    "    \n",
    "    # Шаг 3: Генерация ответа\n",
    "    print(\"\\nГенерируем ответ...\")\n",
    "    answer = generate_answer(prompt)\n",
    "    \n",
    "    return answer, relevant_docs\n",
    "\n",
    "# 8. Запуск системы\n",
    "def main():\n",
    "    # Инициализация\n",
    "    documents = create_knowledge_base()\n",
    "    index, documents = create_vector_db(documents)\n",
    "    \n",
    "    print(\"RAG система готова! Введите ваш вопрос (или 'выход' для завершения):\")\n",
    "    \n",
    "    # Интерактивный цикл\n",
    "    while True:\n",
    "        question = input(\"\\nВаш вопрос: \").strip()\n",
    "        \n",
    "        if question.lower() in ['выход', 'exit', 'quit']:\n",
    "            break\n",
    "            \n",
    "        if not question:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            answer, context_docs = rag_pipeline(question, index, documents)\n",
    "            print(f\"\\nОтвет: {answer}\")\n",
    "            print(f\"\\nИсточники: {len(context_docs)} документов\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_test_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick_test.py - быстрый тест системы\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "def quick_rag_test():\n",
    "    \"\"\"Быстрый тест RAG системы с реальными новостями\"\"\"\n",
    "    \n",
    "    print(\"🚀 Быстрая загрузка RAG системы...\")\n",
    "    \n",
    "    # Загрузка моделей\n",
    "    embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"IlyaGusev/saiga_yandexgpt_8b\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"IlyaGusev/saiga_yandexgpt_8b\",\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # Загрузка датасета\n",
    "    print(\"📰 Загружаем новости...\")\n",
    "    dataset = load_dataset(\"IlyaGusev/ru_news\", split=\"train[:500]\")\n",
    "    documents = []\n",
    "    \n",
    "    for item in dataset:\n",
    "        text = item.get('text', '')\n",
    "        title = item.get('title', '')\n",
    "        if title and text:\n",
    "            documents.append(f\"{title}. {text[:200]}...\")\n",
    "    \n",
    "    print(f\"✅ Загружено {len(documents)} новостей\")\n",
    "    \n",
    "    # Создание векторной базы\n",
    "    embeddings = embedding_model.encode(documents)\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    # Простые тестовые вопросы\n",
    "    questions = [\n",
    "        \"Какие экономические новости?\",\n",
    "        \"Что нового в политике?\",\n",
    "        \"Какие культурные события?\",\n",
    "        \"Какие спортивные новости?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n🧪 Тестируем систему...\")\n",
    "    \n",
    "    for question in questions:\n",
    "        print(f\"\\n❓ Вопрос: {question}\")\n",
    "        \n",
    "        # Поиск релевантных документов\n",
    "        query_embedding = embedding_model.encode([question])\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        scores, indices = index.search(query_embedding, 2)\n",
    "        \n",
    "        # Создание контекста\n",
    "        context = \"\\n\".join([f\"- {documents[idx]}\" for idx in indices[0] if idx < len(documents)])\n",
    "        \n",
    "        # Генерация ответа\n",
    "        prompt = f\"\"\"<bos><start_of_turn>user\n",
    "На основе новостей ответь на вопрос:\n",
    "\n",
    "{context}\n",
    "\n",
    "Вопрос: {question}\n",
    "\n",
    "Краткий ответ:<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                do_sample=True,\n",
    "                temperature=0.7\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "        if \"<start_of_turn>model\" in response:\n",
    "            answer = response.split(\"<start_of_turn>model\")[-1].replace(\"<end_of_turn>\", \"\").strip()\n",
    "            print(f\"🤖 Ответ: {answer}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    quick_rag_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc500f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Загрузка моделей\n",
    "print(\"Загружаем модели...\")\n",
    "\n",
    "# Модель для эмбеддингов (русский язык)\n",
    "embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Модель для генерации\n",
    "model_name = \"IlyaGusev/saiga_yandexgpt_8b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 2. Загрузка реального датасета новостей\n",
    "def create_knowledge_base():\n",
    "    \"\"\"Загружаем реальный датасет новостей на русском языке\"\"\"\n",
    "    print(\"Загружаем датасет IlyaGusev/ru_news...\")\n",
    "    \n",
    "    try:\n",
    "        # Загружаем датасет\n",
    "        dataset = load_dataset(\"IlyaGusev/ru_news\", split=\"train[:1000]\")  # Берем первые 1000 новостей\n",
    "        \n",
    "        documents = []\n",
    "        for item in dataset:\n",
    "            # Извлекаем текст новости\n",
    "            text = item.get('text', '')\n",
    "            title = item.get('title', '')\n",
    "            \n",
    "            # Создаем документ из заголовка и начала текста\n",
    "            if title and text:\n",
    "                document = f\"{title}. {text[:300]}...\"  # Ограничиваем длину\n",
    "                documents.append(document)\n",
    "            elif text:\n",
    "                documents.append(text[:400] + \"...\")\n",
    "            elif title:\n",
    "                documents.append(title)\n",
    "        \n",
    "        print(f\"Загружено {len(documents)} новостей из датасета\")\n",
    "        return documents\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка загрузки датасета: {e}\")\n",
    "        # Резервный вариант - возвращаем несколько примеров\n",
    "        return [\n",
    "            \"Россия и Китай усиливают экономическое сотрудничество в условиях санкций.\",\n",
    "            \"Центробанк России сохранил ключевую ставку на прежнем уровне.\",\n",
    "            \"В Москве открылась новая ветка метро, соединяющая центр с спальными районами.\",\n",
    "            \"Ученые разработали новую вакцину от гриппа с повышенной эффективностью.\",\n",
    "            \"Цены на нефть выросли на мировых рынках из-за сокращения добычи.\",\n",
    "        ]\n",
    "\n",
    "# Примеры вопросов для тестирования (адаптированные под новостную тематику)\n",
    "test_questions = [\n",
    "    \"Какие последние новости о экономике России?\",\n",
    "    \"Что происходит с курсом рубля?\",\n",
    "    \"Какие новые проекты в метро Москвы?\",\n",
    "    \"Какие научные разработки последнего времени?\",\n",
    "    \"Как изменились цены на нефть?\",\n",
    "    \"Какие международные отношения у России?\",\n",
    "    \"Что нового в банковской системе?\",\n",
    "    \"Какие события в культурной жизни?\",\n",
    "    \"Какие спортивные новости?\",\n",
    "    \"Что происходит в сфере технологий?\",\n",
    "    \"Какие изменения в образовании?\",\n",
    "    \"Какие медицинские новости?\",\n",
    "    \"Какие политические события?\",\n",
    "    \"Что нового в строительстве?\",\n",
    "    \"Какие экологические проблемы обсуждаются?\"\n",
    "]\n",
    "\n",
    "def run_test_questions():\n",
    "    \"\"\"Функция для тестирования системы на подготовленных вопросах\"\"\"\n",
    "    documents = create_knowledge_base()\n",
    "    index, documents = create_vector_db(documents)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"ТЕСТИРОВАНИЕ RAG СИСТЕМЫ С РЕАЛЬНЫМИ НОВОСТЯМИ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\n{i}. Вопрос: {question}\")\n",
    "        try:\n",
    "            answer, context_docs = rag_pipeline(question, index, documents)\n",
    "            print(f\"🤖 Ответ: {answer}\")\n",
    "            print(f\"📊 Использовано документов: {len(context_docs)}\")\n",
    "            print(\"📰 Источники:\")\n",
    "            for j, doc in enumerate(context_docs, 1):\n",
    "                print(f\"   {j}. {doc['text'][:100]}... (сходство: {doc['score']:.3f})\")\n",
    "            print(\"-\" * 80)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Ошибка: {e}\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "# 3. Создание векторной базы данных\n",
    "def create_vector_db(documents):\n",
    "    \"\"\"Создаем векторную базу данных\"\"\"\n",
    "    print(\"Создаем векторную базу из новостей...\")\n",
    "    \n",
    "    # Получаем эмбеддинги для документов\n",
    "    embeddings = embedding_model.encode(documents)\n",
    "    \n",
    "    # Создаем FAISS индекс\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)  # Inner Product (косинусное сходство)\n",
    "    \n",
    "    # Нормализуем векторы для косинусного сходства\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    print(f\"Векторная база создана: {len(documents)} документов\")\n",
    "    return index, documents\n",
    "\n",
    "# 4. Поиск релевантных документов\n",
    "def search_similar_documents(query, index, documents, top_k=3):\n",
    "    \"\"\"Ищем наиболее релевантные документы для запроса\"\"\"\n",
    "    # Получаем эмбеддинг запроса\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    # Ищем похожие документы\n",
    "    scores, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    # Возвращаем найденные документы и их скоринги\n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        if idx < len(documents):\n",
    "            results.append({\n",
    "                'text': documents[idx],\n",
    "                'score': scores[0][i]\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 5. Создание промпта с контекстом\n",
    "def create_prompt(question, context_docs):\n",
    "    \"\"\"Создаем промпт для модели с контекстом\"\"\"\n",
    "    context = \"\\n\".join([f\"- {doc['text']}\" for doc in context_docs])\n",
    "    \n",
    "    prompt = f\"\"\"<bos><start_of_turn>user\n",
    "На основе следующих новостных материалов ответь на вопрос:\n",
    "\n",
    "{context}\n",
    "\n",
    "Вопрос: {question}\n",
    "\n",
    "Ответь информативно и точно, используя только предоставленную информацию.\n",
    "Если в материалах нет точного ответа, скажи об этом.<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# 6. Генерация ответа\n",
    "def generate_answer(prompt):\n",
    "    \"\"\"Генерируем ответ с помощью модели\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=2048, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    \n",
    "    # Извлекаем только ответ модели\n",
    "    if \"<start_of_turn>model\" in response:\n",
    "        response = response.split(\"<start_of_turn>model\")[-1].strip()\n",
    "    \n",
    "    # Очищаем от специальных токенов\n",
    "    response = response.replace(\"<end_of_turn>\", \"\").replace(\"<eos>\", \"\").strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# 7. Основная функция RAG\n",
    "def rag_pipeline(question, index, documents):\n",
    "    \"\"\"Полный пайплайн RAG\"\"\"\n",
    "    # Шаг 1: Поиск релевантных документов\n",
    "    relevant_docs = search_similar_documents(question, index, documents, top_k=3)\n",
    "    \n",
    "    # Шаг 2: Создание промпта\n",
    "    prompt = create_prompt(question, relevant_docs)\n",
    "    \n",
    "    # Шаг 3: Генерация ответа\n",
    "    answer = generate_answer(prompt)\n",
    "    \n",
    "    return answer, relevant_docs\n",
    "\n",
    "# 8. Запуск системы\n",
    "def main():\n",
    "    # Инициализация\n",
    "    documents = create_knowledge_base()\n",
    "    index, documents = create_vector_db(documents)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RAG СИСТЕМА НА ОСНОВЕ РЕАЛЬНЫХ НОВОСТЕЙ\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Загружено {len(documents)} новостных документов\")\n",
    "    print(\"Система готова! Введите ваш вопрос (или 'выход' для завершения):\")\n",
    "    \n",
    "    # Сначала запускаем тестовые вопросы\n",
    "    run_test_questions()\n",
    "    \n",
    "    # Интерактивный цикл\n",
    "    while True:\n",
    "        question = input(\"\\n🎯 Ваш вопрос: \").strip()\n",
    "        \n",
    "        if question.lower() in ['выход', 'exit', 'quit']:\n",
    "            print(\"До свидания!\")\n",
    "            break\n",
    "            \n",
    "        if not question:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            print(\"🔍 Ищем релевантные новости...\")\n",
    "            answer, context_docs = rag_pipeline(question, index, documents)\n",
    "            print(f\"\\n🤖 Ответ: {answer}\")\n",
    "            print(f\"\\n📊 Использовано источников: {len(context_docs)}\")\n",
    "            print(\"📰 Релевантные новости:\")\n",
    "            for i, doc in enumerate(context_docs, 1):\n",
    "                print(f\"   {i}. {doc['text'][:120]}... (релевантность: {doc['score']:.3f})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Ошибка: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19ce671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Загрузка моделей\n",
    "print(\"Загружаем модели...\")\n",
    "\n",
    "# Модель для эмбеддингов (русский язык)\n",
    "embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Модель для генерации\n",
    "model_name = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 2. Загрузка реального датасета новостей\n",
    "def create_knowledge_base():\n",
    "    \"\"\"Загружаем реальный датасет новостей на русском языке\"\"\"\n",
    "    print(\"Загружаем датасет\")\n",
    "    \n",
    "    try:\n",
    "        # Загружаем датасет\n",
    "        dataset = load_dataset(\"zloelias/lenta-ru\", split=\"train[:1000]\")  # Берем первые 1000 новостей\n",
    "        \n",
    "        documents = []\n",
    "        for item in dataset:\n",
    "            # Извлекаем текст новости\n",
    "            text = item.get('text', '')\n",
    "            title = item.get('title', '')\n",
    "            \n",
    "            # Создаем документ из заголовка и начала текста\n",
    "            if title and text:\n",
    "                document = f\"{title}. {text[:300]}...\"  # Ограничиваем длину\n",
    "                documents.append(document)\n",
    "            elif text:\n",
    "                documents.append(text[:400] + \"...\")\n",
    "            elif title:\n",
    "                documents.append(title)\n",
    "        \n",
    "        print(f\"Загружено {len(documents)} новостей из датасета\")\n",
    "        return documents\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка загрузки датасета: {e}\")\n",
    "        # Резервный вариант - возвращаем несколько примеров\n",
    "        return [\n",
    "            \"Россия и Китай усиливают экономическое сотрудничество в условиях санкций.\",\n",
    "            \"Центробанк России сохранил ключевую ставку на прежнем уровне.\",\n",
    "            \"В Москве открылась новая ветка метро, соединяющая центр с спальными районами.\",\n",
    "            \"Ученые разработали новую вакцину от гриппа с повышенной эффективностью.\",\n",
    "            \"Цены на нефть выросли на мировых рынках из-за сокращения добычи.\",\n",
    "        ]\n",
    "\n",
    "# Примеры вопросов для тестирования (адаптированные под новостную тематику)\n",
    "test_questions = [\n",
    "    \"Какие последние новости о экономике России?\",\n",
    "    \"Что происходит с курсом рубля?\",\n",
    "    \"Какие новые проекты в метро Москвы?\",\n",
    "    \"Какие научные разработки последнего времени?\",\n",
    "    \"Как изменились цены на нефть?\",\n",
    "    \"Какие международные отношения у России?\",\n",
    "    \"Что нового в банковской системе?\",\n",
    "    \"Какие события в культурной жизни?\",\n",
    "    \"Какие спортивные новости?\",\n",
    "    \"Что происходит в сфере технологий?\",\n",
    "    \"Какие изменения в образовании?\",\n",
    "    \"Какие медицинские новости?\",\n",
    "    \"Какие политические события?\",\n",
    "    \"Что нового в строительстве?\",\n",
    "    \"Какие экологические проблемы обсуждаются?\"\n",
    "]\n",
    "\n",
    "def run_test_questions():\n",
    "    \"\"\"Функция для тестирования системы на подготовленных вопросах\"\"\"\n",
    "    documents = create_knowledge_base()\n",
    "    index, documents = create_vector_db(documents)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"ТЕСТИРОВАНИЕ RAG СИСТЕМЫ С РЕАЛЬНЫМИ НОВОСТЯМИ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\n{i}. Вопрос: {question}\")\n",
    "        try:\n",
    "            answer, context_docs = rag_pipeline(question, index, documents)\n",
    "            print(f\"🤖 Ответ: {answer}\")\n",
    "            print(f\"📊 Использовано документов: {len(context_docs)}\")\n",
    "            print(\"📰 Источники:\")\n",
    "            for j, doc in enumerate(context_docs, 1):\n",
    "                print(f\"   {j}. {doc['text'][:100]}... (сходство: {doc['score']:.3f})\")\n",
    "            print(\"-\" * 80)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Ошибка: {e}\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "# 3. Создание векторной базы данных\n",
    "def create_vector_db(documents):\n",
    "    \"\"\"Создаем векторную базу данных\"\"\"\n",
    "    print(\"Создаем векторную базу из новостей...\")\n",
    "    \n",
    "    # Получаем эмбеддинги для документов\n",
    "    embeddings = embedding_model.encode(documents)\n",
    "    \n",
    "    # Создаем FAISS индекс\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)  # Inner Product (косинусное сходство)\n",
    "    \n",
    "    # Нормализуем векторы для косинусного сходства\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    print(f\"Векторная база создана: {len(documents)} документов\")\n",
    "    return index, documents\n",
    "\n",
    "# 4. Поиск релевантных документов\n",
    "def search_similar_documents(query, index, documents, top_k=3):\n",
    "    \"\"\"Ищем наиболее релевантные документы для запроса\"\"\"\n",
    "    # Получаем эмбеддинг запроса\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    # Ищем похожие документы\n",
    "    scores, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    # Возвращаем найденные документы и их скоринги\n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        if idx < len(documents):\n",
    "            results.append({\n",
    "                'text': documents[idx],\n",
    "                'score': scores[0][i]\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 5. Создание промпта с контекстом\n",
    "def create_prompt(question, context_docs):\n",
    "    \"\"\"Создаем промпт для модели с контекстом\"\"\"\n",
    "    context = \"\\n\".join([f\"- {doc['text']}\" for doc in context_docs])\n",
    "    \n",
    "    prompt = f\"\"\"<bos><start_of_turn>user\n",
    "На основе следующих новостных материалов ответь на вопрос:\n",
    "\n",
    "{context}\n",
    "\n",
    "Вопрос: {question}\n",
    "\n",
    "Ответь информативно и точно, используя только предоставленную информацию.\n",
    "Если в материалах нет точного ответа, скажи об этом.<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# 6. Генерация ответа\n",
    "def generate_answer(prompt):\n",
    "    \"\"\"Генерируем ответ с помощью модели\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=2048, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            temperature=0.5,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    \n",
    "    # Извлекаем только ответ модели\n",
    "    if \"<start_of_turn>model\" in response:\n",
    "        response = response.split(\"<start_of_turn>model\")[-1].strip()\n",
    "    \n",
    "    # Очищаем от специальных токенов\n",
    "    response = response.replace(\"<end_of_turn>\", \"\").replace(\"<eos>\", \"\").strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# 7. Основная функция RAG\n",
    "def rag_pipeline(question, index, documents):\n",
    "    \"\"\"Полный пайплайн RAG\"\"\"\n",
    "    # Шаг 1: Поиск релевантных документов\n",
    "    relevant_docs = search_similar_documents(question, index, documents, top_k=3)\n",
    "    \n",
    "    # Шаг 2: Создание промпта\n",
    "    prompt = create_prompt(question, relevant_docs)\n",
    "    \n",
    "    # Шаг 3: Генерация ответа\n",
    "    answer = generate_answer(prompt)\n",
    "    \n",
    "    return answer, relevant_docs\n",
    "\n",
    "# 8. Запуск системы\n",
    "def main():\n",
    "    # Инициализация\n",
    "    documents = create_knowledge_base()\n",
    "    index, documents = create_vector_db(documents)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RAG СИСТЕМА НА ОСНОВЕ РЕАЛЬНЫХ НОВОСТЕЙ\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Загружено {len(documents)} новостных документов\")\n",
    "    print(\"Система готова! Введите ваш вопрос (или 'выход' для завершения):\")\n",
    "    \n",
    "    # Сначала запускаем тестовые вопросы\n",
    "    run_test_questions()\n",
    "    \n",
    "    # Интерактивный цикл\n",
    "    while True:\n",
    "        question = input(\"\\n🎯 Ваш вопрос: \").strip()\n",
    "        \n",
    "        if question.lower() in ['выход', 'exit', 'quit']:\n",
    "            print(\"До свидания!\")\n",
    "            break\n",
    "            \n",
    "        if not question:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            print(\"🔍 Ищем релевантные новости...\")\n",
    "            answer, context_docs = rag_pipeline(question, index, documents)\n",
    "            print(f\"\\n🤖 Ответ: {answer}\")\n",
    "            print(f\"\\n📊 Использовано источников: {len(context_docs)}\")\n",
    "            print(\"📰 Релевантные новости:\")\n",
    "            for i, doc in enumerate(context_docs, 1):\n",
    "                print(f\"   {i}. {doc['text'][:120]}... (релевантность: {doc['score']:.3f})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Ошибка: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7139f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "print(\"Загружаем модели...\")\n",
    "\n",
    "# Модель для эмбеддингов (русский язык)\n",
    "embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Модель для генерации\n",
    "model_name = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 2. Загрузка реального датасета новостей\n",
    "def create_knowledge_base():\n",
    "    \"\"\"Загружаем реальный датасет новостей на русском языке\"\"\"\n",
    "    print(\"Загружаем датасет...\")\n",
    "    \n",
    "    try:\n",
    "        # Загружаем датасет\n",
    "        dataset = load_dataset(\"zloelias/lenta-ru\", split=\"train[:1000]\")  # Берем первые 1000 новостей\n",
    "        \n",
    "        documents = []\n",
    "        for item in dataset:\n",
    "            # Извлекаем текст новости\n",
    "            text = item.get('text', '')\n",
    "            title = item.get('title', '')\n",
    "            \n",
    "            # Создаем документ из заголовка и начала текста\n",
    "            if title and text:\n",
    "                document = f\"{title}. {text[:300]}...\"  # Ограничиваем длину\n",
    "                documents.append(document)\n",
    "            elif text:\n",
    "                documents.append(text[:400] + \"...\")\n",
    "            elif title:\n",
    "                documents.append(title)\n",
    "        \n",
    "        print(f\"Загружено {len(documents)} новостей из датасета\")\n",
    "        return documents\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка загрузки датасета: {e}\")\n",
    "        # Резервный вариант - возвращаем несколько примеров\n",
    "        return [\n",
    "            \"Россия и Китай усиливают экономическое сотрудничество в условиях санкций.\",\n",
    "            \"Центробанк России сохранил ключевую ставку на прежнем уровне.\",\n",
    "            \"В Москве открылась новая ветка метро, соединяющая центр с спальными районами.\",\n",
    "            \"Ученые разработали новую вакцину от гриппа с повышенной эффективностью.\",\n",
    "            \"Цены на нефть выросли на мировых рынках из-за сокращения добычи.\",\n",
    "        ]\n",
    "\n",
    "# 3. Загрузка вопросов из pickle файла\n",
    "def load_questions_from_pickle(filename=\"input.pickle\"):\n",
    "    \"\"\"Загружаем вопросы из pickle файла\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(filename):\n",
    "            print(f\"Файл {filename} не найден. Использую тестовые вопросы.\")\n",
    "            return get_default_questions()\n",
    "        \n",
    "        with open(filename, 'rb') as f:\n",
    "            questions_data = pickle.load(f)\n",
    "        \n",
    "        # Обрабатываем разные форматы pickle файла\n",
    "        if isinstance(questions_data, list):\n",
    "            # Если это простой список вопросов\n",
    "            questions = questions_data\n",
    "        elif isinstance(questions_data, dict):\n",
    "            # Если это словарь, пытаемся извлечь вопросы\n",
    "            if 'questions' in questions_data:\n",
    "                questions = questions_data['questions']\n",
    "            elif 'queries' in questions_data:\n",
    "                questions = questions_data['queries']\n",
    "            else:\n",
    "                # Берем все строковые значения\n",
    "                questions = [v for v in questions_data.values() if isinstance(v, str)]\n",
    "        else:\n",
    "            print(f\"Неизвестный формат файла {filename}. Использую тестовые вопросы.\")\n",
    "            return get_default_questions()\n",
    "        \n",
    "        print(f\"Загружено {len(questions)} вопросов из {filename}\")\n",
    "        return questions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка загрузки вопросов из {filename}: {e}\")\n",
    "        return get_default_questions()\n",
    "\n",
    "def get_default_questions():\n",
    "    \"\"\"Возвращает вопросы по умолчанию если pickle файл недоступен\"\"\"\n",
    "    return [\n",
    "        \"Какие последние новости о экономике России?\",\n",
    "        \"Что происходит с курсом рубля?\",\n",
    "        \"Какие новые проекты в метро Москвы?\",\n",
    "        \"Какие научные разработки последнего времени?\",\n",
    "        \"Как изменились цены на нефть?\",\n",
    "        \"Какие международные отношения у России?\",\n",
    "        \"Что нового в банковской системе?\",\n",
    "        \"Какие события в культурной жизни?\",\n",
    "        \"Какие спортивные новости?\",\n",
    "        \"Что происходит в сфере технологий?\"\n",
    "    ]\n",
    "\n",
    "def run_test_questions():\n",
    "    \"\"\"Функция для тестирования системы на вопросах из pickle\"\"\"\n",
    "    # Загружаем вопросы из pickle\n",
    "    test_questions = load_questions_from_pickle(\"input.pickle\")\n",
    "    \n",
    "    # Загружаем базу знаний\n",
    "    documents = create_knowledge_base()\n",
    "    index, documents = create_vector_db(documents)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"ТЕСТИРОВАНИЕ RAG СИСТЕМЫ С РЕАЛЬНЫМИ НОВОСТЯМИ\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Будет обработано {len(test_questions)} вопросов\")\n",
    "    \n",
    "    # Сохраняем результаты\n",
    "    results = []\n",
    "    \n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\n{i}. Вопрос: {question}\")\n",
    "        try:\n",
    "            answer, context_docs = rag_pipeline(question, index, documents)\n",
    "            print(f\"🤖 Ответ: {answer}\")\n",
    "            print(f\"📊 Использовано документов: {len(context_docs)}\")\n",
    "            print(\"📰 Источники:\")\n",
    "            for j, doc in enumerate(context_docs, 1):\n",
    "                print(f\"   {j}. {doc['text'][:100]}... (сходство: {doc['score']:.3f})\")\n",
    "            \n",
    "            # Сохраняем результат\n",
    "            results.append({\n",
    "                'question': question,\n",
    "                'answer': answer,\n",
    "                'sources': [doc['text'] for doc in context_docs],\n",
    "                'scores': [doc['score'] for doc in context_docs]\n",
    "            })\n",
    "            \n",
    "            print(\"-\" * 80)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Ошибка: {e}\")\n",
    "            results.append({\n",
    "                'question': question,\n",
    "                'answer': f\"Ошибка: {e}\",\n",
    "                'sources': [],\n",
    "                'scores': []\n",
    "            })\n",
    "            print(\"-\" * 80)\n",
    "    \n",
    "    # Сохраняем результаты в файл\n",
    "    save_results(results)\n",
    "    return results\n",
    "\n",
    "def save_results(results, filename=\"rag_results.pickle\"):\n",
    "    \"\"\"Сохраняем результаты работы RAG системы\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "        print(f\"\\n💾 Результаты сохранены в {filename}\")\n",
    "        \n",
    "        # Также сохраняем в читаемом формате\n",
    "        with open(\"rag_results.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"РЕЗУЛЬТАТЫ RAG СИСТЕМЫ\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\")\n",
    "            for i, result in enumerate(results, 1):\n",
    "                f.write(f\"\\n{i}. ВОПРОС: {result['question']}\\n\")\n",
    "                f.write(f\"   ОТВЕТ: {result['answer']}\\n\")\n",
    "                f.write(f\"   ИСТОЧНИКИ: {len(result['sources'])}\\n\")\n",
    "                for j, source in enumerate(result['sources'], 1):\n",
    "                    f.write(f\"      {j}. {source[:100]}...\\n\")\n",
    "                f.write(\"-\" * 80 + \"\\n\")\n",
    "        \n",
    "        print(f\"📄 Текстовый отчет сохранен в rag_results.txt\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка сохранения результатов: {e}\")\n",
    "\n",
    "# 4. Создание векторной базы данных\n",
    "def create_vector_db(documents):\n",
    "    \"\"\"Создаем векторную базу данных\"\"\"\n",
    "    print(\"Создаем векторную базу из новостей...\")\n",
    "    \n",
    "    # Получаем эмбеддинги для документов\n",
    "    embeddings = embedding_model.encode(documents)\n",
    "    \n",
    "    # Создаем FAISS индекс\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)  # Inner Product (косинусное сходство)\n",
    "    \n",
    "    # Нормализуем векторы для косинусного сходства\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    print(f\"Векторная база создана: {len(documents)} документов\")\n",
    "    return index, documents\n",
    "\n",
    "# 5. Поиск релевантных документов\n",
    "def search_similar_documents(query, index, documents, top_k=3):\n",
    "    \"\"\"Ищем наиболее релевантные документы для запроса\"\"\"\n",
    "    # Получаем эмбеддинг запроса\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    # Ищем похожие документы\n",
    "    scores, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    # Возвращаем найденные документы и их скоринги\n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        if idx < len(documents):\n",
    "            results.append({\n",
    "                'text': documents[idx],\n",
    "                'score': scores[0][i]\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 6. Создание промпта с контекстом\n",
    "def create_prompt(question, context_docs):\n",
    "    \"\"\"Создаем промпт для модели с контекстом\"\"\"\n",
    "    context = \"\\n\".join([f\"- {doc['text']}\" for doc in context_docs])\n",
    "    \n",
    "    # Адаптируем промпт под формат Qwen\n",
    "    prompt = f\"\"\"<|im_start|>system\n",
    "Ты - помощник, который отвечает на вопросы на основе предоставленных новостных материалов.\n",
    "Отвечай точно и информативно, используя только предоставленную информацию.\n",
    "Если в материалах нет точного ответа, скажи об этом.<|im_end|>\n",
    "<|im_start|>user\n",
    "На основе следующих новостных материалов ответь на вопрос:\n",
    "\n",
    "{context}\n",
    "\n",
    "Вопрос: {question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# 7. Генерация ответа\n",
    "def generate_answer(prompt):\n",
    "    \"\"\"Генерируем ответ с помощью модели\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=2048, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            temperature=0.5,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    \n",
    "    # Извлекаем только ответ ассистента\n",
    "    if \"<|im_start|>assistant\" in response:\n",
    "        response = response.split(\"<|im_start|>assistant\")[-1].strip()\n",
    "        if \"<|im_end|>\" in response:\n",
    "            response = response.split(\"<|im_end|>\")[0].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# 8. Основная функция RAG\n",
    "def rag_pipeline(question, index, documents):\n",
    "    \"\"\"Полный пайплайн RAG\"\"\"\n",
    "    # Шаг 1: Поиск релевантных документов\n",
    "    relevant_docs = search_similar_documents(question, index, documents, top_k=3)\n",
    "    \n",
    "    # Шаг 2: Создание промпта\n",
    "    prompt = create_prompt(question, relevant_docs)\n",
    "    \n",
    "    # Шаг 3: Генерация ответа\n",
    "    answer = generate_answer(prompt)\n",
    "    \n",
    "    return answer, relevant_docs\n",
    "\n",
    "# 9. Запуск системы\n",
    "def main():\n",
    "    # Инициализация\n",
    "    documents = create_knowledge_base()\n",
    "    index, documents = create_vector_db(documents)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RAG СИСТЕМА НА ОСНОВЕ РЕАЛЬНЫХ НОВОСТЕЙ\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Загружено {len(documents)} новостных документов\")\n",
    "    \n",
    "    # Запускаем тестирование на вопросах из pickle\n",
    "    results = run_test_questions()\n",
    "    \n",
    "    # Интерактивный цикл\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ИНТЕРАКТИВНЫЙ РЕЖИМ\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Введите ваш вопрос (или 'выход' для завершения):\")\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\n🎯 Ваш вопрос: \").strip()\n",
    "        \n",
    "        if question.lower() in ['выход', 'exit', 'quit']:\n",
    "            print(\"До свидания!\")\n",
    "            break\n",
    "            \n",
    "        if not question:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            print(\"🔍 Ищем релевантные новости...\")\n",
    "            answer, context_docs = rag_pipeline(question, index, documents)\n",
    "            print(f\"\\n🤖 Ответ: {answer}\")\n",
    "            print(f\"\\n📊 Использовано источников: {len(context_docs)}\")\n",
    "            print(\"📰 Релевантные новости:\")\n",
    "            for i, doc in enumerate(context_docs, 1):\n",
    "                print(f\"   {i}. {doc['text'][:120]}... (релевантность: {doc['score']:.3f})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Ошибка: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
